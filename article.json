[
  {
    "title": "State-Dependent Conformal Perception Bounds for Neuro-Symbolic Verification of Autonomous Systems",
    "url": "http://arxiv.org/abs/2502.21308v1",
    "arxiv_id": "2502.21308v1",
    "authors": [
      "Thomas Waite",
      "Yuang Geng",
      "Trevor Turnquist",
      "Ivan Ruchkin",
      "Radoslav Ivanov"
    ],
    "published": "2025-02-28T18:51:20+00:00",
    "summary": "It remains a challenge to provide safety guarantees for autonomous systems with neural perception and control. A typical approach obtains symbolic bounds on perception error (e.g., using conformal prediction) and performs verification under these bounds. However, these bounds can lead to drastic conservatism in the resulting end-to-end safety guarantee. This paper proposes an approach to synthesize symbolic perception error bounds that serve as an optimal interface between perception performance and control verification. The key idea is to consider our error bounds to be heteroskedastic with respect to the system's state -- not time like in previous approaches. These bounds can be obtained with two gradient-free optimization algorithms. We demonstrate that our bounds lead to tighter safety guarantees than the state-of-the-art in a case study on a mountain car."
  },
  {
    "title": "Dynamically Local-Enhancement Planner for Large-Scale Autonomous Driving",
    "url": "http://arxiv.org/abs/2502.21134v1",
    "arxiv_id": "2502.21134v1",
    "authors": [
      "Nanshan Deng",
      "Weitao Zhou",
      "Bo Zhang",
      "Junze Wen",
      "Kun Jiang",
      "Zhong Cao",
      "Diange Yang"
    ],
    "published": "2025-02-28T15:17:20+00:00",
    "summary": "Current autonomous vehicles operate primarily within limited regions, but there is increasing demand for broader applications. However, as models scale, their limited capacity becomes a significant challenge for adapting to novel scenarios. It is increasingly difficult to improve models for new situations using a single monolithic model. To address this issue, we introduce the concept of dynamically enhancing a basic driving planner with local driving data, without permanently modifying the planner itself. This approach, termed the Dynamically Local-Enhancement (DLE) Planner, aims to improve the scalability of autonomous driving systems without significantly expanding the planner's size. Our approach introduces a position-varying Markov Decision Process formulation coupled with a graph neural network that extracts region-specific driving features from local observation data. The learned features describe the local behavior of the surrounding objects, which is then leveraged to enhance a basic reinforcement learning-based policy. We evaluated our approach in multiple scenarios and compared it with a one-for-all driving model. The results show that our method outperforms the baseline policy in both safety (collision rate) and average reward, while maintaining a lighter scale. This approach has the potential to benefit large-scale autonomous vehicles without the need for largely expanding on-device driving models."
  },
  {
    "title": "Rare event modeling with self-regularized normalizing flows: what can we learn from a single failure?",
    "url": "http://arxiv.org/abs/2502.21110v1",
    "arxiv_id": "2502.21110v1",
    "authors": [
      "Charles Dawson",
      "Van Tran",
      "Max Z. Li",
      "Chuchu Fan"
    ],
    "published": "2025-02-28T14:47:52+00:00",
    "summary": "Increased deployment of autonomous systems in fields like transportation and robotics have seen a corresponding increase in safety-critical failures. These failures can be difficult to model and debug due to the relative lack of data: compared to tens of thousands of examples from normal operations, we may have only seconds of data leading up to the failure. This scarcity makes it challenging to train generative models of rare failure events, as existing methods risk either overfitting to noise in the limited failure dataset or underfitting due to an overly strong prior. We address this challenge with CalNF, or calibrated normalizing flows, a self-regularized framework for posterior learning from limited data. CalNF achieves state-of-the-art performance on data-limited failure modeling and inverse problems and enables a first-of-a-kind case study into the root causes of the 2022 Southwest Airlines scheduling crisis."
  },
  {
    "title": "AuthSim: Towards Authentic and Effective Safety-critical Scenario Generation for Autonomous Driving Tests",
    "url": "http://arxiv.org/abs/2502.21100v1",
    "arxiv_id": "2502.21100v1",
    "authors": [
      "Yukuan Yang",
      "Xucheng Lu",
      "Zhili Zhang",
      "Zepeng Wu",
      "Guoqi Li",
      "Lingzhong Meng",
      "Yunzhi Xue"
    ],
    "published": "2025-02-28T14:38:35+00:00",
    "summary": "Generating adversarial safety-critical scenarios is a pivotal method for testing autonomous driving systems, as it identifies potential weaknesses and enhances system robustness and reliability. However, existing approaches predominantly emphasize unrestricted collision scenarios, prompting non-player character (NPC) vehicles to attack the ego vehicle indiscriminately. These works overlook these scenarios' authenticity, rationality, and relevance, resulting in numerous extreme, contrived, and largely unrealistic collision events involving aggressive NPC vehicles. To rectify this issue, we propose a three-layer relative safety region model, which partitions the area based on danger levels and increases the likelihood of NPC vehicles entering relative boundary regions. This model directs NPC vehicles to engage in adversarial actions within relatively safe boundary regions, thereby augmenting the scenarios' authenticity. We introduce AuthSim, a comprehensive platform for generating authentic and effective safety-critical scenarios by integrating the three-layer relative safety region model with reinforcement learning. To our knowledge, this is the first attempt to address the authenticity and effectiveness of autonomous driving system test scenarios comprehensively. Extensive experiments demonstrate that AuthSim outperforms existing methods in generating effective safety-critical scenarios. Notably, AuthSim achieves a 5.25% improvement in average cut-in distance and a 27.12% enhancement in average collision interval time, while maintaining higher efficiency in generating effective safety-critical scenarios compared to existing methods. This underscores its significant advantage in producing authentic scenarios over current methodologies."
  },
  {
    "title": "FC-Attack: Jailbreaking Large Vision-Language Models via Auto-Generated Flowcharts",
    "url": "http://arxiv.org/abs/2502.21059v1",
    "arxiv_id": "2502.21059v1",
    "authors": [
      "Ziyi Zhang",
      "Zhen Sun",
      "Zongmin Zhang",
      "Jihui Guo",
      "Xinlei He"
    ],
    "published": "2025-02-28T13:59:11+00:00",
    "summary": "Large Vision-Language Models (LVLMs) have become powerful and widely adopted in some practical applications. However, recent research has revealed their vulnerability to multimodal jailbreak attacks, whereby the model can be induced to generate harmful content, leading to safety risks. Although most LVLMs have undergone safety alignment, recent research shows that the visual modality is still vulnerable to jailbreak attacks. In our work, we discover that by using flowcharts with partially harmful information, LVLMs can be induced to provide additional harmful details. Based on this, we propose a jailbreak attack method based on auto-generated flowcharts, FC-Attack. Specifically, FC-Attack first fine-tunes a pre-trained LLM to create a step-description generator based on benign datasets. The generator is then used to produce step descriptions corresponding to a harmful query, which are transformed into flowcharts in 3 different shapes (vertical, horizontal, and S-shaped) as visual prompts. These flowcharts are then combined with a benign textual prompt to execute a jailbreak attack on LVLMs. Our evaluations using the Advbench dataset show that FC-Attack achieves over 90% attack success rates on Gemini-1.5, Llaval-Next, Qwen2-VL, and InternVL-2.5 models, outperforming existing LVLM jailbreak methods. Additionally, we investigate factors affecting the attack performance, including the number of steps and the font styles in the flowcharts. Our evaluation shows that FC-Attack can improve the jailbreak performance from 4% to 28% in Claude-3.5 by changing the font style. To mitigate the attack, we explore several defenses and find that AdaShield can largely reduce the jailbreak performance but with the cost of utility drop."
  },
  {
    "title": "Including Follower Dynamics in Beaconing for Platooning Safety",
    "url": "http://arxiv.org/abs/2502.21039v1",
    "arxiv_id": "2502.21039v1",
    "authors": [
      "Hassan Laghbi",
      "Nigel Thomas"
    ],
    "published": "2025-02-28T13:30:46+00:00",
    "summary": "In this paper, we propose procedures to address platoon follower dynamics within adaptive beaconing. We implement them in a known adaptive beaconing scheme which is Jerk Beaconing (JB) to improve its safety. We evaluate our proposed approach in terms of safety, string stability and the channel busy ratio (CBR) overhead. The results reveal that our proposal significantly enhances safety without imposing substantial CBR overhead and maintains the string stability of the PATH CACC controller under normal conditions."
  },
  {
    "title": "A RISC-V Multicore and GPU SoC Platform with a Qualifiable Software Stack for Safety Critical Systems",
    "url": "http://arxiv.org/abs/2502.21027v1",
    "arxiv_id": "2502.21027v1",
    "authors": [
      "Marc Sol\u00e9 i Bonet",
      "Jannis Wolf",
      "Leonidas Kosmidis"
    ],
    "published": "2025-02-28T13:16:00+00:00",
    "summary": "In the context of the Horizon Europe project, METASAT, a hardware platform was developed as a prototype of future space systems. The platform is based on a multiprocessor NOEL-V, an established space-grade processor, which is integrated with the SPARROW AI accelerator and connected to a GPU, Vortex. Both processing systems follow the RISC-V specification. This is a novel hardware architecture for the space domain as the use of massive parallel processing units, such as GPUs, is starting to be considered for upcoming space missions due to the increased performance required to future space-related workloads, in particular, related to AI. However, such solutions are only currently adopted for New Space, since their limitations come not only from the hardware, but also from the software, which needs to be qualified before being deployed on an institutional mission. For this reason, the METASAT platform is one of the first endeavors towards enabling the use of high performance hardware in a qualifiable environment for safety critical systems. The software stack is based on baremetal, RTEMS and the XtratuM hypervisor, providing different options for applications of various degrees of criticality."
  },
  {
    "title": "Beware of Your Po! Measuring and Mitigating AI Safety Risks in Role-Play Fine-Tuning of LLMs",
    "url": "http://arxiv.org/abs/2502.20968v1",
    "arxiv_id": "2502.20968v1",
    "authors": [
      "Weixiang Zhao",
      "Yulin Hu",
      "Yang Deng",
      "Jiahe Guo",
      "Xingyu Sui",
      "Xinyang Han",
      "An Zhang",
      "Yanyan Zhao",
      "Bing Qin",
      "Tat-Seng Chua",
      "Ting Liu"
    ],
    "published": "2025-02-28T11:31:27+00:00",
    "summary": "Role-playing enables large language models (LLMs) to engage users in immersive and personalized interactions, but it also introduces significant safety risks. Existing role-play fine-tuning techniques improve role adaptability but may degrade safety performance, particularly for villainous characters. In this work, we conduct the first comprehensive assessment of role-play fine-tuning risks by training 95 role-specific LLMs using RoleBench. Our experiments reveal that role-play fine-tuning leads to a noticeable decline in safety performance, with safety risks varying based on character traits. To tackle this challenge, we propose Safety-Aware Role-Play Fine-Tuning (SaRFT), a novel method designed to balance role-playing capabilities and safety. Extensive experiments on LLaMA-3-8B-Instruct, Gemma-2-9B-it, and Qwen2.5-7B-Instruct demonstrate that SaRFT consistently outperforms state-of-the-art baselines under both LoRA and full-parameter fine-tuning settings. Our findings highlight the necessity of role-adaptive safety measures and provide insights into mitigating role-specific safety risks in role-playing LLMs."
  },
  {
    "title": "Efficient Jailbreaking of Large Models by Freeze Training: Lower Layers Exhibit Greater Sensitivity to Harmful Content",
    "url": "http://arxiv.org/abs/2502.20952v1",
    "arxiv_id": "2502.20952v1",
    "authors": [
      "Hongyuan Shen",
      "Min Zheng",
      "Jincheng Wang",
      "Yang Zhao"
    ],
    "published": "2025-02-28T11:07:41+00:00",
    "summary": "With the widespread application of Large Language Models across various domains, their security issues have increasingly garnered significant attention from both academic and industrial communities. This study conducts sampling and normalization of the parameters of the LLM to generate visual representations and heatmaps of parameter distributions, revealing notable discrepancies in parameter distributions among certain layers within the hidden layers. Further analysis involves calculating statistical metrics for each layer, followed by the computation of a Comprehensive Sensitivity Score based on these metrics, which identifies the lower layers as being particularly sensitive to the generation of harmful content. Based on this finding, we employ a Freeze training strategy, selectively performing Supervised Fine-Tuning only on the lower layers. Experimental results demonstrate that this method significantly reduces training duration and GPU memory consumption while maintaining a high jailbreak success rate and a high harm score, outperforming the results achieved by applying the LoRA method for SFT across all layers. Additionally, the method has been successfully extended to other open-source large models, validating its generality and effectiveness across different model architectures. Furthermore, we compare our method with ohter jailbreak method, demonstrating the superior performance of our approach. By innovatively proposing a method to statistically analyze and compare large model parameters layer by layer, this study provides new insights into the interpretability of large models. These discoveries emphasize the necessity of continuous research and the implementation of adaptive security measures in the rapidly evolving field of LLMs to prevent potential jailbreak attack risks, thereby promoting the development of more robust and secure LLMs."
  },
  {
    "title": "ProBench: Benchmarking Large Language Models in Competitive Programming",
    "url": "http://arxiv.org/abs/2502.20868v1",
    "arxiv_id": "2502.20868v1",
    "authors": [
      "Lei Yang",
      "Renren Jin",
      "Ling Shi",
      "Jianxiang Peng",
      "Yue Chen",
      "Deyi Xiong"
    ],
    "published": "2025-02-28T09:12:42+00:00",
    "summary": "With reasoning language models such as OpenAI-o3 and DeepSeek-R1 emerging, large language models (LLMs) have entered a new phase of development. However, existing benchmarks for coding evaluation are gradually inadequate to assess the capability of advanced LLMs in code reasoning. To bridge the gap for high-level code reasoning assessment, we propose ProBench to benchmark LLMs in competitive programming, drawing inspiration from the International Collegiate Programming Contest. ProBench collects a comprehensive set of competitive programming problems from Codeforces, Luogu, and Nowcoder platforms during the period from July to December 2024, obtaining real test results through online submissions to ensure the fairness and accuracy of the evaluation. We establish a unified problem attribute system, including difficulty grading and algorithm tagging. With carefully collected and annotated data in ProBench, we systematically assess 9 latest LLMs in competitive programming across multiple dimensions, including thought chain analysis, error type diagnosis, and reasoning depth evaluation. Experimental results show that QwQ-32B-Preview achieves the best score of 20.93 followed by DeepSeek-V3 with a score of 16.38, suggesting that models trained with specialized reasoning tasks significantly outperform general-purpose models (even larger than reasoning-oriented models) in programming. Further analysis also reveals key areas for programming capability enhancement, e.g., algorithm adaptability and reasoning sufficiency, providing important insights for the future development of reasoning models."
  },
  {
    "title": "Multimodal Learning for Just-In-Time Software Defect Prediction in Autonomous Driving Systems",
    "url": "http://arxiv.org/abs/2502.20806v1",
    "arxiv_id": "2502.20806v1",
    "authors": [
      "Faisal Mohammad",
      "Duksan Ryu"
    ],
    "published": "2025-02-28T07:45:10+00:00",
    "summary": "In recent years, the rise of autonomous driving technologies has highlighted the critical importance of reliable software for ensuring safety and performance. This paper proposes a novel approach for just-in-time software defect prediction (JIT-SDP) in autonomous driving software systems using multimodal learning. The proposed model leverages the multimodal transformers in which the pre-trained transformers and a combining module deal with the multiple data modalities of the software system datasets such as code features, change metrics, and contextual information. The key point for adapting multimodal learning is to utilize the attention mechanism between the different data modalities such as text, numerical, and categorical. In the combining module, the output of a transformer model on text data and tabular features containing categorical and numerical data are combined to produce the predictions using the fully connected layers. Experiments conducted on three open-source autonomous driving system software projects collected from the GitHub repository (Apollo, Carla, and Donkeycar) demonstrate that the proposed approach significantly outperforms state-of-the-art deep learning and machine learning models regarding evaluation metrics. Our findings highlight the potential of multimodal learning to enhance the reliability and safety of autonomous driving software through improved defect prediction."
  },
  {
    "title": "Characteristics Analysis of Autonomous Vehicle Pre-crash Scenarios",
    "url": "http://arxiv.org/abs/2502.20789v1",
    "arxiv_id": "2502.20789v1",
    "authors": [
      "Yixuan Li",
      "Xuesong Wang",
      "Tianyi Wang",
      "Qian Liu"
    ],
    "published": "2025-02-28T07:10:53+00:00",
    "summary": "To date, hundreds of crashes have occurred in open road testing of automated vehicles (AVs), highlighting the need for improving AV reliability and safety. Pre-crash scenario typology classifies crashes based on vehicle dynamics and kinematics features. Building on this, characteristics analysis can identify similar features under comparable crashes, offering a more effective reflection of general crash patterns and providing more targeted recommendations for enhancing AV performance. However, current studies primarily concentrated on crashes among conventional human-driven vehicles, leaving a gap in research dedicated to in-depth AV crash analyses. In this paper, we analyzed the latest California AV collision reports and used the newly revised pre-crash scenario typology to identify pre-crash scenarios. We proposed a set of mapping rules for automatically extracting these AV pre-crash scenarios, successfully identifying 24 types with a 98.1% accuracy rate, and obtaining two key scenarios of AV crashes (i.e., rear-end scenarios and intersection scenarios) through detailed analysis. Association analyses of rear-end scenarios showed that the significant environmental influencing factors were traffic control type, location type, light, etc. For intersection scenarios prone to severe crashes with detailed descriptions, we employed causal analyses to obtain the significant causal factors: habitual violations and expectations of certain behavior. Optimization recommendations were then formulated, addressing both governmental oversight and AV manufacturers' potential improvements. The findings of this paper could guide government authorities to develop related regulations, help manufacturers design AV test scenarios, and identify potential shortcomings in control algorithms specific to various real-world scenarios, thereby optimizing AV systems effectively."
  },
  {
    "title": "The Rise of Darkness: Safety-Utility Trade-Offs in Role-Playing Dialogue Agents",
    "url": "http://arxiv.org/abs/2502.20757v1",
    "arxiv_id": "2502.20757v1",
    "authors": [
      "Yihong Tang",
      "Kehai Chen",
      "Xuefeng Bai",
      "Zhengyu Niu",
      "Bo Wang",
      "Jie Liu",
      "Min Zhang"
    ],
    "published": "2025-02-28T06:18:50+00:00",
    "summary": "Large Language Models (LLMs) have made remarkable advances in role-playing dialogue agents, demonstrating their utility in character simulations. However, it remains challenging for these agents to balance character portrayal utility with content safety because this essential character simulation often comes with the risk of generating unsafe content. To address this issue, we first conduct a systematic exploration of the safety-utility trade-off across multiple LLMs. Our analysis reveals that risk scenarios created by villain characters and user queries (referred to as risk coupling) contribute to this trade-off. Building on this, we propose a novel Adaptive Dynamic Multi-Preference (ADMP) method, which dynamically adjusts safety-utility preferences based on the degree of risk coupling and guides the model to generate responses biased toward utility or safety. We further introduce Coupling Margin Sampling (CMS) into coupling detection to enhance the model's ability to handle high-risk scenarios. Experimental results demonstrate that our approach improves safety metrics while maintaining utility."
  },
  {
    "title": "Computationally Efficient Safe Control of Linear Systems under Severe Sensor Attacks",
    "url": "http://arxiv.org/abs/2502.20718v1",
    "arxiv_id": "2502.20718v1",
    "authors": [
      "Xiao Tan",
      "Pio Ong",
      "Paulo Tabuada",
      "Aaron D. Ames"
    ],
    "published": "2025-02-28T05:05:48+00:00",
    "summary": "Cyber-physical systems are prone to sensor attacks that can compromise safety. A common approach to synthesizing controllers robust to sensor attacks is secure state reconstruction (SSR) -- but this is computationally expensive, hindering real-time control. In this paper, we take a safety-critical perspective on mitigating severe sensor attacks, leading to a computationally efficient solution. Namely, we design feedback controllers that ensure system safety by directly computing control actions from past input-output data. Instead of fully solving the SSR problem, we use conservative bounds on a control barrier function (CBF) condition, which we obtain by extending the recent eigendecomposition-based SSR approach to severe sensor attack settings. Additionally, we present an extended approach that solves a smaller-scale subproblem of the SSR problem, taking on some computational burden to mitigate the conservatism in the main approach. Numerical comparisons confirm that the traditional SSR approaches suffer from combinatorial issues, while our approach achieves safety guarantees with greater computational efficiency."
  },
  {
    "title": "From Safety Standards to Safe Operation with Mobile Robotic Systems Deployment",
    "url": "http://arxiv.org/abs/2502.20693v1",
    "arxiv_id": "2502.20693v1",
    "authors": [
      "Bruno Belzile",
      "Tatiana Wanang-Siyapdjie",
      "Sina Karimi",
      "Rafael Gomes Braga",
      "Ivanka Iordanova",
      "David St-Onge"
    ],
    "published": "2025-02-28T03:52:10+00:00",
    "summary": "Mobile robotic systems are increasingly used in various work environments to support productivity. However, deploying robots in workplaces crowded by human workers and interacting with them results in safety challenges and concerns, namely robot-worker collisions and worker distractions in hazardous environments. Moreover, the literature on risk assessment as well as the standard specific to mobile platforms is rather limited. In this context, this paper first conducts a review of the relevant standards and methodologies and then proposes a risk assessment for the safe deployment of mobile robots on construction sites. The approach extends relevant existing safety standards to encompass uncovered scenarios. Safety recommendations are made based on the framework, after its validation by field experts."
  },
  {
    "title": "Delayed-Decision Motion Planning in the Presence of Multiple Predictions",
    "url": "http://arxiv.org/abs/2502.20636v1",
    "arxiv_id": "2502.20636v1",
    "authors": [
      "David Isele",
      "Alexandre Miranda Anon",
      "Faizan M. Tariq",
      "Goro Yeh",
      "Avinash Singh",
      "Sangjae Bae"
    ],
    "published": "2025-02-28T01:36:33+00:00",
    "summary": "Reliable automated driving technology is challenged by various sources of uncertainties, in particular, behavioral uncertainties of traffic agents. It is common for traffic agents to have intentions that are unknown to others, leaving an automated driving car to reason over multiple possible behaviors. This paper formalizes a behavior planning scheme in the presence of multiple possible futures with corresponding probabilities. We present a maximum entropy formulation and show how, under certain assumptions, this allows delayed decision-making to improve safety. The general formulation is then turned into a model predictive control formulation, which is solved as a quadratic program or a set of quadratic programs. We discuss implementation details for improving computation and verify operation in simulation and on a mobile robot."
  },
  {
    "title": "SafeText: Safe Text-to-image Models via Aligning the Text Encoder",
    "url": "http://arxiv.org/abs/2502.20623v1",
    "arxiv_id": "2502.20623v1",
    "authors": [
      "Yuepeng Hu",
      "Zhengyuan Jiang",
      "Neil Zhenqiang Gong"
    ],
    "published": "2025-02-28T01:02:57+00:00",
    "summary": "Text-to-image models can generate harmful images when presented with unsafe prompts, posing significant safety and societal risks. Alignment methods aim to modify these models to ensure they generate only non-harmful images, even when exposed to unsafe prompts. A typical text-to-image model comprises two main components: 1) a text encoder and 2) a diffusion module. Existing alignment methods mainly focus on modifying the diffusion module to prevent harmful image generation. However, this often significantly impacts the model's behavior for safe prompts, causing substantial quality degradation of generated images. In this work, we propose SafeText, a novel alignment method that fine-tunes the text encoder rather than the diffusion module. By adjusting the text encoder, SafeText significantly alters the embedding vectors for unsafe prompts, while minimally affecting those for safe prompts. As a result, the diffusion module generates non-harmful images for unsafe prompts while preserving the quality of images for safe prompts. We evaluate SafeText on multiple datasets of safe and unsafe prompts, including those generated through jailbreak attacks. Our results show that SafeText effectively prevents harmful image generation with minor impact on the images for safe prompts, and SafeText outperforms six existing alignment methods. We will publish our code and data after paper acceptance."
  },
  {
    "title": "HazardNet: A Small-Scale Vision Language Model for Real-Time Traffic Safety Detection at Edge Devices",
    "url": "http://arxiv.org/abs/2502.20572v1",
    "arxiv_id": "2502.20572v1",
    "authors": [
      "Mohammad Abu Tami",
      "Mohammed Elhenawy",
      "Huthaifa I. Ashqar"
    ],
    "published": "2025-02-27T22:21:45+00:00",
    "summary": "Traffic safety remains a vital concern in contemporary urban settings, intensified by the increase of vehicles and the complicated nature of road networks. Traditional safety-critical event detection systems predominantly rely on sensor-based approaches and conventional machine learning algorithms, necessitating extensive data collection and complex training processes to adhere to traffic safety regulations. This paper introduces HazardNet, a small-scale Vision Language Model designed to enhance traffic safety by leveraging the reasoning capabilities of advanced language and vision models. We built HazardNet by fine-tuning the pre-trained Qwen2-VL-2B model, chosen for its superior performance among open-source alternatives and its compact size of two billion parameters. This helps to facilitate deployment on edge devices with efficient inference throughput. In addition, we present HazardQA, a novel Vision Question Answering (VQA) dataset constructed specifically for training HazardNet on real-world scenarios involving safety-critical events. Our experimental results show that the fine-tuned HazardNet outperformed the base model up to an 89% improvement in F1-Score and has comparable results with improvement in some cases reach up to 6% when compared to larger models, such as GPT-4o. These advancements underscore the potential of HazardNet in providing real-time, reliable traffic safety event detection, thereby contributing to reduced accidents and improved traffic management in urban environments. Both HazardNet model and the HazardQA dataset are available at https://huggingface.co/Tami3/HazardNet and https://huggingface.co/datasets/Tami3/HazardQA, respectively."
  },
  {
    "title": "SoS1: O1 and R1-Like Reasoning LLMs are Sum-of-Square Solvers",
    "url": "http://arxiv.org/abs/2502.20545v1",
    "arxiv_id": "2502.20545v1",
    "authors": [
      "Kechen Li",
      "Wenqi Zhu",
      "Coralia Cartis",
      "Tianbo Ji",
      "Shiwei Liu"
    ],
    "published": "2025-02-27T21:41:43+00:00",
    "summary": "Large Language Models (LLMs) have achieved human-level proficiency across diverse tasks, but their ability to perform rigorous mathematical problem solving remains an open challenge. In this work, we investigate a fundamental yet computationally intractable problem: determining whether a given multivariate polynomial is nonnegative. This problem, closely related to Hilbert's Seventeenth Problem, plays a crucial role in global polynomial optimization and has applications in various fields. First, we introduce SoS-1K, a meticulously curated dataset of approximately 1,000 polynomials, along with expert-designed reasoning instructions based on five progressively challenging criteria. Evaluating multiple state-of-the-art LLMs, we find that without structured guidance, all models perform only slightly above the random guess baseline 50%. However, high-quality reasoning instructions significantly improve accuracy, boosting performance up to 81%. Furthermore, our 7B model, SoS-7B, fine-tuned on SoS-1K for just 4 hours, outperforms the 671B DeepSeek-V3 and GPT-4o-mini in accuracy while only requiring 1.8% and 5% of the computation time needed for letters, respectively. Our findings highlight the potential of LLMs to push the boundaries of mathematical reasoning and tackle NP-hard problems."
  },
  {
    "title": "Automated Profile-Guided Replacement of Data Structures to Reduce Memory Allocation",
    "url": "http://arxiv.org/abs/2502.20536v1",
    "arxiv_id": "2502.20536v1",
    "authors": [
      "Lukas Makor",
      "Sebastian Kloibhofer",
      "Peter Hofer",
      "David Leopoldseder",
      "Hanspeter M\u00f6ssenb\u00f6ck"
    ],
    "published": "2025-02-27T21:38:48+00:00",
    "summary": "Data structures are a cornerstone of most modern programming languages. Whether they are provided via separate libraries, built into the language specification, or as part of the language's standard library -- data structures such as lists, maps, sets, or arrays provide programmers with a large repertoire of tools to deal with data. Moreover, each kind of data structure typically comes with a variety of implementations that focus on scalability, memory efficiency, performance, thread-safety, or similar aspects. Choosing the *right* data structure for a particular use case can be difficult or even impossible if the data structure is part of a framework over which the user has no control. It typically requires in-depth knowledge about the program and, in particular, about the usage of the data structure in question. However, it is usually not feasible for developers to obtain such information about programs in advance. Hence, it makes sense to look for a more automated way for optimizing data structures. We present an approach to automatically replace data structures in Java applications. We use profiling to determine allocation-site-specific metrics about data structures and their usages, and then automatically replace their allocations with customized versions, focusing on memory efficiency. Our approach is integrated into GraalVM Native Image, an Ahead-of-Time compiler for Java applications. By analyzing the generated data structure profiles, we show how standard benchmarks and microservice-based applications use data structures and demonstrate the impact of customized data structures on the memory usage of applications. We conducted an evaluation on standard and microservice-based benchmarks, in which the memory usage was reduced by up to 13.85 % in benchmarks that make heavy use of data structures. While others are only slightly affected, we could still reduce the average memory usage by 1.63 % in standard benchmarks and by 2.94 % in microservice-based benchmarks. We argue that our work demonstrates that choosing appropriate data structures can reduce the memory usage of applications. While acknowledge that our approach does not provide benefits for all kinds of workloads, our work nevertheless shows how automated profiling and replacement can be used to optimize data structures in general. Hence, we argue that our work could pave the way for future optimizations of data structures."
  },
  {
    "title": "APIKS: A Modular ROS2 Framework for Rapid Prototyping and Validation of Automated Driving Systems",
    "url": "http://arxiv.org/abs/2502.20507v1",
    "arxiv_id": "2502.20507v1",
    "authors": [
      "Jo\u00e3o-Vitor Zacchi",
      "Edoardo Clementi",
      "N\u00faria Mata"
    ],
    "published": "2025-02-27T20:29:31+00:00",
    "summary": "Automated driving technologies promise substantial improvements in transportation safety, efficiency, and accessibility. However, ensuring the reliability and safety of Autonomous Vehicles in complex, real-world environments remains a significant challenge, particularly during the early stages of software development. Existing software development environments and simulation platforms often either focus narrowly on specific functions or are too complex, hindering the rapid prototyping of small proofs of concept. To address this challenge, we have developed the APIKS automotive platform, a modular framework based on ROS2. APIKS is designed for the efficient testing and validation of autonomous vehicle software within software-defined vehicles. It offers a simplified, standards-based architecture designed specifically for small-scale proofs of concept. This enables rapid prototyping without the overhead associated with comprehensive platforms. We demonstrate the capabilities of APIKS through an exemplary use case involving a Construction Zone Assist system, illustrating its effectiveness in facilitating the development and testing of autonomous vehicle functionalities."
  },
  {
    "title": "Equivariant Reinforcement Learning Frameworks for Quadrotor Low-Level Control",
    "url": "http://arxiv.org/abs/2502.20500v1",
    "arxiv_id": "2502.20500v1",
    "authors": [
      "Beomyeol Yu",
      "Taeyoung Lee"
    ],
    "published": "2025-02-27T20:16:19+00:00",
    "summary": "Improving sampling efficiency and generalization capability is critical for the successful data-driven control of quadrotor unmanned aerial vehicles (UAVs) that are inherently unstable. While various reinforcement learning (RL) approaches have been applied to autonomous quadrotor flight, they often require extensive training data, posing multiple challenges and safety risks in practice. To address these issues, we propose data-efficient, equivariant monolithic and modular RL frameworks for quadrotor low-level control. Specifically, by identifying the rotational and reflectional symmetries in quadrotor dynamics and encoding these symmetries into equivariant network models, we remove redundancies of learning in the state-action space. This approach enables the optimal control action learned in one configuration to automatically generalize into other configurations via symmetry, thereby enhancing data efficiency. Experimental results demonstrate that our equivariant approaches significantly outperform their non-equivariant counterparts in terms of learning efficiency and flight performance."
  },
  {
    "title": "EgoNormia: Benchmarking Physical Social Norm Understanding",
    "url": "http://arxiv.org/abs/2502.20490v1",
    "arxiv_id": "2502.20490v1",
    "authors": [
      "MohammadHossein Rezaei",
      "Yicheng Fu",
      "Phil Cuvin",
      "Caleb Ziems",
      "Yanzhe Zhang",
      "Hao Zhu",
      "Diyi Yang"
    ],
    "published": "2025-02-27T19:54:16+00:00",
    "summary": "Human activity is moderated by norms. When performing actions in the real world, humans not only follow norms, but also consider the trade-off between different norms However, machines are often trained without explicit supervision on norm understanding and reasoning, especially when the norms are grounded in a physical and social context. To improve and evaluate the normative reasoning capability of vision-language models (VLMs), we present EgoNormia $\\|\\epsilon\\|$, consisting of 1,853 ego-centric videos of human interactions, each of which has two related questions evaluating both the prediction and justification of normative actions. The normative actions encompass seven categories: safety, privacy, proxemics, politeness, cooperation, coordination/proactivity, and communication/legibility. To compile this dataset at scale, we propose a novel pipeline leveraging video sampling, automatic answer generation, filtering, and human validation. Our work demonstrates that current state-of-the-art vision-language models lack robust norm understanding, scoring a maximum of 45% on EgoNormia (versus a human bench of 92%). Our analysis of performance in each dimension highlights the significant risks of safety, privacy, and the lack of collaboration and communication capability when applied to real-world agents. We additionally show that through a retrieval-based generation method, it is possible to use EgoNomia to enhance normative reasoning in VLMs."
  },
  {
    "title": "Bounded First-Class Universe Levels in Dependent Type Theory",
    "url": "http://arxiv.org/abs/2502.20485v1",
    "arxiv_id": "2502.20485v1",
    "authors": [
      "Jonathan Chan",
      "Stephanie Weirich"
    ],
    "published": "2025-02-27T19:52:46+00:00",
    "summary": "In dependent type theory, being able to refer to a type universe as a term itself increases its expressive power, but requires mechanisms in place to prevent Girard's paradox from introducing logical inconsistency in the presence of type-in-type. The simplest mechanism is a hierarchy of universes indexed by a sequence of levels, typically the naturals. To improve reusability of definitions, they can be made level polymorphic, abstracting over level variables and adding a notion of level expressions. For even more expressive power, level expressions can be made first-class as terms themselves, and level polymorphism is subsumed by dependent functions quantifying over levels. Furthermore, bounded level polymorphism provides more expressivity by being able to explicitly state constraints on level variables. While semantics for first-class levels with constraints are known, syntax and typing rules have not been explicitly written down. Yet pinning down a well-behaved syntax is not trivial; there exist prior type theories with bounded level polymorphism that fail to satisfy subject reduction. In this work, we design an explicit syntax for a type theory with bounded first-class levels, parametrized over arbitrary well-founded sets of levels. We prove the metatheoretic properties of subject reduction, type safety, consistency, and canonicity, entirely mechanized from syntax to semantics in Lean."
  },
  {
    "title": "Searching for additional structure and redshift evolution in the observed binary black hole population with a parametric time-dependent mass distribution",
    "url": "http://arxiv.org/abs/2502.20445v1",
    "arxiv_id": "2502.20445v1",
    "authors": [
      "Vasco Gennari",
      "Simone Mastrogiovanni",
      "Nicola Tamanini",
      "Sylvain Marsat",
      "Gr\u00e9goire Pierra"
    ],
    "published": "2025-02-27T19:00:02+00:00",
    "summary": "The population of the observed gravitational wave events encodes unique information on the formation and evolution of stellar-mass black holes, from the underlying astrophysical processes to the large-scale dynamics of the Universe. We use the ICAROGW analysis infrastructure to perform hierarchical Bayesian inference on the gravitational wave signals from the LIGO-Virgo-KAGRA third observing run, O3. Searching for additional structure and redshift evolution in the primary mass distribution, we explore the dependence of the mass spectrum reconstruction on different parametrizations and prior choices. For the stationary case, we find strong evidence (Bayes factor $B \\simeq 180$) that the results obtained using a power-law model with a peak (Powerlaw-Gaussian)--the model preferred so far in the literature--are sensitive to prior bounds, affecting the resolvability of the $\\sim 35 M_{\\odot}$ peak. This behaviour is reproduced by simulated data, indicating a bimodal structure in the likelihood. Models with three mass features simultaneously capture a sharp $\\sim 10M_{\\odot}$ peak, a $\\sim 35 M_{\\odot}$ overdensity, and support for a $\\sim 20 M_{\\odot}$ overdensity preceded by a dip. Among these, a model with three power-law peaks (Powerlaw-Powerlaw-Powerlaw) is equally favored, in terms of evidence, over the Powerlaw-Gaussian model with wide priors. We find no statistical support for redshift evolution in the current data and provide constraints on the parameters governing this evolution, showing consistency with stationarity. We highlight possible limitations of the hierarchical Bayesian inference framework in reconstructing evolving features outside the detector horizon. Our work lays the foundations for a robust characterization of time-dependent population distributions, with significant implications for black hole astrophysics and gravitational wave cosmology."
  },
  {
    "title": "Large Language Model Strategic Reasoning Evaluation through Behavioral Game Theory",
    "url": "http://arxiv.org/abs/2502.20432v1",
    "arxiv_id": "2502.20432v1",
    "authors": [
      "Jingru Jia",
      "Zehua Yuan",
      "Junhao Pan",
      "Paul E. McNamara",
      "Deming Chen"
    ],
    "published": "2025-02-27T18:58:31+00:00",
    "summary": "Strategic decision-making involves interactive reasoning where agents adapt their choices in response to others, yet existing evaluations of large language models (LLMs) often emphasize Nash Equilibrium (NE) approximation, overlooking the mechanisms driving their strategic choices. To bridge this gap, we introduce an evaluation framework grounded in behavioral game theory, disentangling reasoning capability from contextual effects. Testing 22 state-of-the-art LLMs, we find that GPT-o3-mini, GPT-o1, and DeepSeek-R1 dominate most games yet also demonstrate that the model scale alone does not determine performance. In terms of prompting enhancement, Chain-of-Thought (CoT) prompting is not universally effective, as it increases strategic reasoning only for models at certain levels while providing limited gains elsewhere. Additionally, we investigate the impact of encoded demographic features on the models, observing that certain assignments impact the decision-making pattern. For instance, GPT-4o shows stronger strategic reasoning with female traits than males, while Gemma assigns higher reasoning levels to heterosexual identities compared to other sexual orientations, indicating inherent biases. These findings underscore the need for ethical standards and contextual alignment to balance improved reasoning with fairness."
  },
  {
    "title": "Why Are Web AI Agents More Vulnerable Than Standalone LLMs? A Security Analysis",
    "url": "http://arxiv.org/abs/2502.20383v1",
    "arxiv_id": "2502.20383v1",
    "authors": [
      "Jeffrey Yang Fan Chiang",
      "Seungjae Lee",
      "Jia-Bin Huang",
      "Furong Huang",
      "Yizheng Chen"
    ],
    "published": "2025-02-27T18:56:26+00:00",
    "summary": "Recent advancements in Web AI agents have demonstrated remarkable capabilities in addressing complex web navigation tasks. However, emerging research shows that these agents exhibit greater vulnerability compared to standalone Large Language Models (LLMs), despite both being built upon the same safety-aligned models. This discrepancy is particularly concerning given the greater flexibility of Web AI Agent compared to standalone LLMs, which may expose them to a wider range of adversarial user inputs. To build a scaffold that addresses these concerns, this study investigates the underlying factors that contribute to the increased vulnerability of Web AI agents. Notably, this disparity stems from the multifaceted differences between Web AI agents and standalone LLMs, as well as the complex signals - nuances that simple evaluation metrics, such as success rate, often fail to capture. To tackle these challenges, we propose a component-level analysis and a more granular, systematic evaluation framework. Through this fine-grained investigation, we identify three critical factors that amplify the vulnerability of Web AI agents; (1) embedding user goals into the system prompt, (2) multi-step action generation, and (3) observational capabilities. Our findings highlights the pressing need to enhance security and robustness in AI agent design and provide actionable insights for targeted defense strategies."
  },
  {
    "title": "Trajectory-to-Action Pipeline (TAP): Automated Scenario Description Extraction for Autonomous Vehicle Behavior Comparison",
    "url": "http://arxiv.org/abs/2502.20353v1",
    "arxiv_id": "2502.20353v1",
    "authors": [
      "Aron Harder",
      "Madhur Behl"
    ],
    "published": "2025-02-27T18:27:05+00:00",
    "summary": "Scenario Description Languages (SDLs) provide structured, interpretable embeddings that represent traffic scenarios encountered by autonomous vehicles (AVs), supporting key tasks such as scenario similarity searches and edge case detection for safety analysis. This paper introduces the Trajectory-to-Action Pipeline (TAP), a scalable and automated method for extracting SDL labels from large trajectory datasets. TAP applies a rules-based cross-entropy optimization approach to learn parameters directly from data, enhancing generalization across diverse driving contexts. Using the Waymo Open Motion Dataset (WOMD), TAP achieves 30% greater precision than Average Displacement Error (ADE) and 24% over Dynamic Time Warping (DTW) in identifying behaviorally similar trajectories. Additionally, TAP enables automated detection of unique driving behaviors, streamlining safety evaluation processes for AV testing. This work provides a foundation for scalable scenario-based AV behavior analysis, with potential extensions for integrating multi-agent contexts."
  },
  {
    "title": "Safety Representations for Safer Policy Learning",
    "url": "http://arxiv.org/abs/2502.20341v1",
    "arxiv_id": "2502.20341v1",
    "authors": [
      "Kaustubh Mani",
      "Vincent Mai",
      "Charlie Gauthier",
      "Annie Chen",
      "Samer Nashed",
      "Liam Paull"
    ],
    "published": "2025-02-27T18:10:33+00:00",
    "summary": "Reinforcement learning algorithms typically necessitate extensive exploration of the state space to find optimal policies. However, in safety-critical applications, the risks associated with such exploration can lead to catastrophic consequences. Existing safe exploration methods attempt to mitigate this by imposing constraints, which often result in overly conservative behaviours and inefficient learning. Heavy penalties for early constraint violations can trap agents in local optima, deterring exploration of risky yet high-reward regions of the state space. To address this, we introduce a method that explicitly learns state-conditioned safety representations. By augmenting the state features with these safety representations, our approach naturally encourages safer exploration without being excessively cautious, resulting in more efficient and safer policy learning in safety-critical scenarios. Empirical evaluations across diverse environments show that our method significantly improves task performance while reducing constraint violations during training, underscoring its effectiveness in balancing exploration with safety."
  },
  {
    "title": "On Adversarial Attacks In Acoustic Drone Localization",
    "url": "http://arxiv.org/abs/2502.20325v1",
    "arxiv_id": "2502.20325v1",
    "authors": [
      "Tamir Shor",
      "Chaim Baskin",
      "Alex Bronstein"
    ],
    "published": "2025-02-27T17:50:17+00:00",
    "summary": "Multi-rotor aerial autonomous vehicles (MAVs, more widely known as \"drones\") have been generating increased interest in recent years due to their growing applicability in a vast and diverse range of fields (e.g., agriculture, commercial delivery, search and rescue). The sensitivity of visual-based methods to lighting conditions and occlusions had prompted growing study of navigation reliant on other modalities, such as acoustic sensing. A major concern in using drones in scale for tasks in non-controlled environments is the potential threat of adversarial attacks over their navigational systems, exposing users to mission-critical failures, security breaches, and compromised safety outcomes that can endanger operators and bystanders. While previous work shows impressive progress in acoustic-based drone localization, prior research in adversarial attacks over drone navigation only addresses visual sensing-based systems. In this work, we aim to compensate for this gap by supplying a comprehensive analysis of the effect of PGD adversarial attacks over acoustic drone localization. We furthermore develop an algorithm for adversarial perturbation recovery, capable of markedly diminishing the affect of such attacks in our setting. The code for reproducing all experiments will be released upon publication."
  },
  {
    "title": "Adapting Automatic Speech Recognition for Accented Air Traffic Control Communications",
    "url": "http://arxiv.org/abs/2502.20311v1",
    "arxiv_id": "2502.20311v1",
    "authors": [
      "Marcus Yu Zhe Wee",
      "Justin Juin Hng Wong",
      "Lynus Lim",
      "Joe Yu Wei Tan",
      "Prannaya Gupta",
      "Dillion Lim",
      "En Hao Tew",
      "Aloysius Keng Siew Han",
      "Yong Zhi Lim"
    ],
    "published": "2025-02-27T17:35:59+00:00",
    "summary": "Effective communication in Air Traffic Control (ATC) is critical to maintaining aviation safety, yet the challenges posed by accented English remain largely unaddressed in Automatic Speech Recognition (ASR) systems. Existing models struggle with transcription accuracy for Southeast Asian-accented (SEA-accented) speech, particularly in noisy ATC environments. This study presents the development of ASR models fine-tuned specifically for Southeast Asian accents using a newly created dataset. Our research achieves significant improvements, achieving a Word Error Rate (WER) of 0.0982 or 9.82% on SEA-accented ATC speech. Additionally, the paper highlights the importance of region-specific datasets and accent-focused training, offering a pathway for deploying ASR systems in resource-constrained military operations. The findings emphasize the need for noise-robust training techniques and region-specific datasets to improve transcription accuracy for non-Western accents in ATC communications."
  },
  {
    "title": "Interpreting AI for Fusion: an application to Plasma Profile Analysis for Tearing Mode Stability",
    "url": "http://arxiv.org/abs/2502.20294v1",
    "arxiv_id": "2502.20294v1",
    "authors": [
      "Hiro J Farre-Kaga",
      "Andrew Rothstein",
      "Rohit Sonker",
      "SangKyeun Kim",
      "Ricardo Shousha",
      "Minseok Kim",
      "Keith Erickson",
      "Jeff Schneider",
      "Egemen Kolemen"
    ],
    "published": "2025-02-27T17:19:14+00:00",
    "summary": "AI models have demonstrated strong predictive capabilities for various tokamak instabilities--including tearing modes (TM), ELMs, and disruptive events--but their opaque nature raises concerns about safety and trustworthiness when applied to fusion power plants. Here, we present a physics-based interpretation framework using a TM prediction model as a first demonstration that is validated through a dedicated DIII-D TM avoidance experiment. By applying Shapley analysis, we identify how profiles such as rotation, temperature, and density contribute to the model's prediction of TM stability. Our analysis shows that in our experimental scenario, peaked rotation profiles are lightly stabilizing, but core electron temperature and density profile shape play the primary role in TM stability. This work offers a generalizable ML-based event prediction methodology, from training to physics-driven interpretability, bridging the gap between physics understanding and opaque ML models."
  },
  {
    "title": "QPM: Discrete Optimization for Globally Interpretable Image Classification",
    "url": "http://arxiv.org/abs/2502.20130v1",
    "arxiv_id": "2502.20130v1",
    "authors": [
      "Thomas Norrenbrock",
      "Timo Kaiser",
      "Sovan Biswas",
      "Ramesh Manuvinakurike",
      "Bodo Rosenhahn"
    ],
    "published": "2025-02-27T14:25:36+00:00",
    "summary": "Understanding the classifications of deep neural networks, e.g. used in safety-critical situations, is becoming increasingly important. While recent models can locally explain a single decision, to provide a faithful global explanation about an accurate model's general behavior is a more challenging open task. Towards that goal, we introduce the Quadratic Programming Enhanced Model (QPM), which learns globally interpretable class representations. QPM represents every class with a binary assignment of very few, typically 5, features, that are also assigned to other classes, ensuring easily comparable contrastive class representations. This compact binary assignment is found using discrete optimization based on predefined similarity measures and interpretability constraints. The resulting optimal assignment is used to fine-tune the diverse features, so that each of them becomes the shared general concept between the assigned classes. Extensive evaluations show that QPM delivers unprecedented global interpretability across small and large-scale datasets while setting the state of the art for the accuracy of interpretable models."
  },
  {
    "title": "Minds on the Move: Decoding Trajectory Prediction in Autonomous Driving with Cognitive Insights",
    "url": "http://arxiv.org/abs/2502.20084v1",
    "arxiv_id": "2502.20084v1",
    "authors": [
      "Haicheng Liao",
      "Chengyue Wang",
      "Kaiqun Zhu",
      "Yilong Ren",
      "Bolin Gao",
      "Shengbo Eben Li",
      "Chengzhong Xu",
      "Zhenning Li"
    ],
    "published": "2025-02-27T13:43:17+00:00",
    "summary": "In mixed autonomous driving environments, accurately predicting the future trajectories of surrounding vehicles is crucial for the safe operation of autonomous vehicles (AVs). In driving scenarios, a vehicle's trajectory is determined by the decision-making process of human drivers. However, existing models primarily focus on the inherent statistical patterns in the data, often neglecting the critical aspect of understanding the decision-making processes of human drivers. This oversight results in models that fail to capture the true intentions of human drivers, leading to suboptimal performance in long-term trajectory prediction. To address this limitation, we introduce a Cognitive-Informed Transformer (CITF) that incorporates a cognitive concept, Perceived Safety, to interpret drivers' decision-making mechanisms. Perceived Safety encapsulates the varying risk tolerances across drivers with different driving behaviors. Specifically, we develop a Perceived Safety-aware Module that includes a Quantitative Safety Assessment for measuring the subject risk levels within scenarios, and Driver Behavior Profiling for characterizing driver behaviors. Furthermore, we present a novel module, Leanformer, designed to capture social interactions among vehicles. CITF demonstrates significant performance improvements on three well-established datasets. In terms of long-term prediction, it surpasses existing benchmarks by 12.0% on the NGSIM, 28.2% on the HighD, and 20.8% on the MoCAD dataset. Additionally, its robustness in scenarios with limited or missing data is evident, surpassing most state-of-the-art (SOTA) baselines, and paving the way for real-world applications."
  },
  {
    "title": "Deterministic or probabilistic? The psychology of LLMs as random number generators",
    "url": "http://arxiv.org/abs/2502.19965v1",
    "arxiv_id": "2502.19965v1",
    "authors": [
      "Javier Coronado-Bl\u00e1zquez"
    ],
    "published": "2025-02-27T10:45:27+00:00",
    "summary": "Large Language Models (LLMs) have transformed text generation through inherently probabilistic context-aware mechanisms, mimicking human natural language. In this paper, we systematically investigate the performance of various LLMs when generating random numbers, considering diverse configurations such as different model architectures, numerical ranges, temperature, and prompt languages. Our results reveal that, despite their stochastic transformers-based architecture, these models often exhibit deterministic responses when prompted for random numerical outputs. In particular, we find significant differences when changing the model, as well as the prompt language, attributing this phenomenon to biases deeply embedded within the training data. Models such as DeepSeek-R1 can shed some light on the internal reasoning process of LLMs, despite arriving to similar results. These biases induce predictable patterns that undermine genuine randomness, as LLMs are nothing but reproducing our own human cognitive biases."
  },
  {
    "title": "Beyond the Tip of Efficiency: Uncovering the Submerged Threats of Jailbreak Attacks in Small Language Models",
    "url": "http://arxiv.org/abs/2502.19883v1",
    "arxiv_id": "2502.19883v1",
    "authors": [
      "Sibo Yi",
      "Tianshuo Cong",
      "Xinlei He",
      "Qi Li",
      "Jiaxing Song"
    ],
    "published": "2025-02-27T08:44:04+00:00",
    "summary": "Small language models (SLMs) have become increasingly prominent in the deployment on edge devices due to their high efficiency and low computational cost. While researchers continue to advance the capabilities of SLMs through innovative training strategies and model compression techniques, the security risks of SLMs have received considerably less attention compared to large language models (LLMs).To fill this gap, we provide a comprehensive empirical study to evaluate the security performance of 13 state-of-the-art SLMs under various jailbreak attacks. Our experiments demonstrate that most SLMs are quite susceptible to existing jailbreak attacks, while some of them are even vulnerable to direct harmful prompts.To address the safety concerns, we evaluate several representative defense methods and demonstrate their effectiveness in enhancing the security of SLMs. We further analyze the potential security degradation caused by different SLM techniques including architecture compression, quantization, knowledge distillation, and so on. We expect that our research can highlight the security challenges of SLMs and provide valuable insights to future work in developing more robust and secure SLMs."
  },
  {
    "title": "Behind the Tip of Efficiency: Uncovering the Submerged Threats of Jailbreak Attacks in Small Language Models",
    "url": "http://arxiv.org/abs/2502.19883v2",
    "arxiv_id": "2502.19883v2",
    "authors": [
      "Sibo Yi",
      "Tianshuo Cong",
      "Xinlei He",
      "Qi Li",
      "Jiaxing Song"
    ],
    "published": "2025-02-27T08:44:04+00:00",
    "summary": "Small language models (SLMs) have become increasingly prominent in the deployment on edge devices due to their high efficiency and low computational cost. While researchers continue to advance the capabilities of SLMs through innovative training strategies and model compression techniques, the security risks of SLMs have received considerably less attention compared to large language models (LLMs).To fill this gap, we provide a comprehensive empirical study to evaluate the security performance of 13 state-of-the-art SLMs under various jailbreak attacks. Our experiments demonstrate that most SLMs are quite susceptible to existing jailbreak attacks, while some of them are even vulnerable to direct harmful prompts.To address the safety concerns, we evaluate several representative defense methods and demonstrate their effectiveness in enhancing the security of SLMs. We further analyze the potential security degradation caused by different SLM techniques including architecture compression, quantization, knowledge distillation, and so on. We expect that our research can highlight the security challenges of SLMs and provide valuable insights to future work in developing more robust and secure SLMs."
  },
  {
    "title": "Nonlinear dynamics in pulse-modulated feedback drug dosing",
    "url": "http://arxiv.org/abs/2502.19878v1",
    "arxiv_id": "2502.19878v1",
    "authors": [
      "Alexander Medvedev",
      "Anton V. Proskurnikov",
      "Zhanybai T. Zhusubaliyev"
    ],
    "published": "2025-02-27T08:36:38+00:00",
    "summary": "Pulse-modulated feedback is utilized in drug dosing to mimic sustained over a longer period of time manual discrete dose administration, the latter is in contrast with continuous drug infusion. The intermittent mode of dosing calls for a hybrid (continuous-discrete) modeling of the closed-loop system, where the pharmacokinetics and pharmacodynamics of the drug are captured by differential equations whereas the control law is described by difference equations. Hybrid dynamics are highly nonlinear which complicates formal design of pulse-modulated feedback. This paper demonstrates complex nonlinear dynamical phenomena arising in a simple control system of dosing a neuromuscular blockade agent in anesthesia. Along with the nominal periodic regimen, undesirable nonlinear behaviors, i.e. periodic solutions of high multiplicity, multistability, as well as deterministic chaos, are shown to exist. It is concluded that design of feedback drug dosing algorithms based on a hybrid paradigm has to be informed by a thorough bifurcation analysis in order to secure patient safety."
  },
  {
    "title": "Empowering Social Service with AI: Insights from a Participatory Design Study with Practitioners",
    "url": "http://arxiv.org/abs/2502.19822v1",
    "arxiv_id": "2502.19822v1",
    "authors": [
      "Yugin Tan",
      "Kai Xin Soh",
      "Renwen Zhang",
      "Jungup Lee",
      "Han Meng",
      "Biswadeep Sen",
      "Yi-Chieh Lee"
    ],
    "published": "2025-02-27T06:50:05+00:00",
    "summary": "In social service, administrative burdens and decision-making challenges often hinder practitioners from performing effective casework. Generative AI (GenAI) offers significant potential to streamline these tasks, yet exacerbates concerns about overreliance, algorithmic bias, and loss of identity within the profession. We explore these issues through a two-stage participatory design study. We conducted formative co-design workshops (\\textit{n=27}) to create a prototype GenAI tool, followed by contextual inquiry sessions with practitioners (\\textit{n=24}) using the tool with real case data. We reveal opportunities for AI integration in documentation, assessment, and worker supervision, while highlighting risks related to GenAI limitations, skill retention, and client safety. Drawing comparisons with GenAI tools in other fields, we discuss design and usage guidelines for such tools in social service practice."
  },
  {
    "title": "Foot-In-The-Door: A Multi-turn Jailbreak for LLMs",
    "url": "http://arxiv.org/abs/2502.19820v1",
    "arxiv_id": "2502.19820v1",
    "authors": [
      "Zixuan Weng",
      "Xiaolong Jin",
      "Jinyuan Jia",
      "Xiangyu Zhang"
    ],
    "published": "2025-02-27T06:49:16+00:00",
    "summary": "Ensuring AI safety is crucial as large language models become increasingly integrated into real-world applications. A key challenge is jailbreak, where adversarial prompts bypass built-in safeguards to elicit harmful disallowed outputs. Inspired by psychological foot-in-the-door principles, we introduce FITD,a novel multi-turn jailbreak method that leverages the phenomenon where minor initial commitments lower resistance to more significant or more unethical transgressions.Our approach progressively escalates the malicious intent of user queries through intermediate bridge prompts and aligns the model's response by itself to induce toxic responses. Extensive experimental results on two jailbreak benchmarks demonstrate that FITD achieves an average attack success rate of 94% across seven widely used models, outperforming existing state-of-the-art methods. Additionally, we provide an in-depth analysis of LLM self-corruption, highlighting vulnerabilities in current alignment strategies and emphasizing the risks inherent in multi-turn interactions.The code is available at https://github.com/Jinxiaolong1129/Foot-in-the-door-Jailbreak ."
  },
  {
    "title": "Foot-In-The-Door: A Multi-turn Jailbreak for LLMs",
    "url": "http://arxiv.org/abs/2502.19820v2",
    "arxiv_id": "2502.19820v2",
    "authors": [
      "Zixuan Weng",
      "Xiaolong Jin",
      "Jinyuan Jia",
      "Xiangyu Zhang"
    ],
    "published": "2025-02-27T06:49:16+00:00",
    "summary": "Ensuring AI safety is crucial as large language models become increasingly integrated into real-world applications. A key challenge is jailbreak, where adversarial prompts bypass built-in safeguards to elicit harmful disallowed outputs. Inspired by psychological foot-in-the-door principles, we introduce FITD,a novel multi-turn jailbreak method that leverages the phenomenon where minor initial commitments lower resistance to more significant or more unethical transgressions. Our approach progressively escalates the malicious intent of user queries through intermediate bridge prompts and aligns the model's response by itself to induce toxic responses. Extensive experimental results on two jailbreak benchmarks demonstrate that FITD achieves an average attack success rate of 94% across seven widely used models, outperforming existing state-of-the-art methods. Additionally, we provide an in-depth analysis of LLM self-corruption, highlighting vulnerabilities in current alignment strategies and emphasizing the risks inherent in multi-turn interactions. The code is available at https://github.com/Jinxiaolong1129/Foot-in-the-door-Jailbreak."
  },
  {
    "title": "Automatic Linear Resource Bound Analysis for Rust via Prophecy Potentials",
    "url": "http://arxiv.org/abs/2502.19810v1",
    "arxiv_id": "2502.19810v1",
    "authors": [
      "Qihao Lian",
      "Di Wang"
    ],
    "published": "2025-02-27T06:35:40+00:00",
    "summary": "Rust has become a popular system programming language that strikes a balance between memory safety and performance. Rust's type system ensures the safety of low-level memory controls; however, a well-typed Rust program is not guaranteed to enjoy high performance. This article studies static analysis for resource consumption of Rust programs, aiming at understanding the performance of Rust programs. Although there have been tons of studies on static resource analysis, exploiting Rust's memory safety -- especially the borrow mechanisms and their properties -- to aid resource-bound analysis, remains unexplored. This article presents RaRust, a type-based linear resource-bound analysis for well-typed Rust programs. RaRust follows the methodology of automatic amortized resource analysis (AARA) to build a resource-aware type system. To support Rust's borrow mechanisms, including shared and mutable borrows, RaRust introduces shared and novel prophecy potentials to reason about borrows compositionally. To prove the soundness of RaRust, this article proposes Resource-Aware Borrow Calculus (RABC) as a variant of recently proposed Low-Level Borrow Calculus (LLBC). The experimental evaluation of a prototype implementation of RaRust demonstrates that RaRust is capable of inferring symbolic linear resource bounds for Rust programs featuring shared and mutable borrows, reborrows, heap-allocated data structures, loops, and recursion."
  },
  {
    "title": "R1-T1: Fully Incentivizing Translation Capability in LLMs via Reasoning Learning",
    "url": "http://arxiv.org/abs/2502.19735v1",
    "arxiv_id": "2502.19735v1",
    "authors": [
      "Minggui He",
      "Yilun Liu",
      "Shimin Tao",
      "Yuanchang Luo",
      "Hongyong Zeng",
      "Chang Su",
      "Li Zhang",
      "Hongxia Ma",
      "Daimeng Wei",
      "Weibin Meng",
      "Hao Yang",
      "Boxing Chen",
      "Osamu Yoshie"
    ],
    "published": "2025-02-27T03:57:00+00:00",
    "summary": "Despite recent breakthroughs in reasoning-enhanced large language models (LLMs) like DeepSeek-R1, incorporating inference-time reasoning into machine translation (MT), where human translators naturally employ structured, multi-layered reasoning chain-of-thoughts (CoTs), is yet underexplored. Existing methods either design a fixed CoT tailored for a specific MT sub-task (e.g., literature translation), or rely on synthesizing CoTs unaligned with humans and supervised fine-tuning (SFT) prone to catastrophic forgetting, limiting their adaptability to diverse translation scenarios. This paper introduces R1-Translator (R1-T1), a novel framework to achieve inference-time reasoning for general MT via reinforcement learning (RL) with human-aligned CoTs comprising six common patterns. Our approach pioneers three innovations: (1) extending reasoning-based translation beyond MT sub-tasks to six languages and diverse tasks (e.g., legal/medical domain adaptation, idiom resolution); (2) formalizing six expert-curated CoT templates that mirror hybrid human strategies like context-aware paraphrasing and back translation; and (3) enabling self-evolving CoT discovery and anti-forgetting adaptation through RL with KL-constrained rewards. Experimental results indicate a steady translation performance improvement in 21 languages and 80 translation directions on Flores-101 test set, especially on the 15 languages unseen from training, with its general multilingual abilities preserved compared with plain SFT."
  },
  {
    "title": "R1-T1: Fully Incentivizing Translation Capability in LLMs via Reasoning Learning",
    "url": "http://arxiv.org/abs/2502.19735v2",
    "arxiv_id": "2502.19735v2",
    "authors": [
      "Minggui He",
      "Yilun Liu",
      "Shimin Tao",
      "Yuanchang Luo",
      "Hongyong Zeng",
      "Chang Su",
      "Li Zhang",
      "Hongxia Ma",
      "Daimeng Wei",
      "Weibin Meng",
      "Hao Yang",
      "Boxing Chen",
      "Osamu Yoshie"
    ],
    "published": "2025-02-27T03:57:00+00:00",
    "summary": "Despite recent breakthroughs in reasoning-enhanced large language models (LLMs) like DeepSeek-R1, incorporating inference-time reasoning into machine translation (MT), where human translators naturally employ structured, multi-layered reasoning chain-of-thoughts (CoTs), is yet underexplored. Existing methods either design a fixed CoT tailored for a specific MT sub-task (e.g., literature translation), or rely on synthesizing CoTs unaligned with humans, limiting their adaptability to diverse translation scenarios. This paper introduces R1-Translator (R1-T1), a novel framework to achieve inference-time reasoning for general MT via reinforcement learning (RL) with human-aligned CoTs comprising six common patterns. Our approach pioneers three innovations: (1) extending reasoning-based translation beyond MT sub-tasks to six languages and diverse tasks (e.g., legal/medical domain adaptation, idiom resolution); (2) formalizing six expert-curated CoT templates that mirror hybrid human strategies like context-aware paraphrasing and back translation; and (3) enabling self-evolving CoT discovery through RL. Experimental results indicate a steady translation performance improvement in 11 languages and 40 translation directions on Flores-101 test set, especially on the languages unseen from training."
  },
  {
    "title": "Unveiling Security Weaknesses in Autonomous Driving Systems: An In-Depth Empirical Study",
    "url": "http://arxiv.org/abs/2502.19687v1",
    "arxiv_id": "2502.19687v1",
    "authors": [
      "Wenyuan Cheng",
      "Zengyang Li",
      "Peng Liang",
      "Ran Mo",
      "Hui Liu"
    ],
    "published": "2025-02-27T01:57:53+00:00",
    "summary": "The advent of Autonomous Driving Systems (ADS) has marked a significant shift towards intelligent transportation, with implications for public safety and traffic efficiency. While these systems integrate a variety of technologies and offer numerous benefits, their security is paramount, as vulnerabilities can have severe consequences for safety and trust. This study aims to systematically investigate potential security weaknesses in the codebases of prominent open-source ADS projects using CodeQL, a static code analysis tool. The goal is to identify common vulnerabilities, their distribution and persistence across versions to enhance the security of ADS. We selected three representative open-source ADS projects, Autoware, AirSim, and Apollo, based on their high GitHub star counts and Level 4 autonomous driving capabilities. Using CodeQL, we analyzed multiple versions of these projects to identify vulnerabilities, focusing on CWE categories such as CWE-190 (Integer Overflow or Wraparound) and CWE-20 (Improper Input Validation). We also tracked the lifecycle of these vulnerabilities across software versions. This approach allows us to systematically analyze vulnerabilities in projects, which has not been extensively explored in previous ADS research. Our analysis revealed that specific CWE categories, particularly CWE-190 (59.6%) and CWE-20 (16.1%), were prevalent across the selected ADS projects. These vulnerabilities often persisted for over six months, spanning multiple version iterations. The empirical assessment showed a direct link between the severity of these vulnerabilities and their tangible effects on ADS performance. These security issues among ADS still remain to be resolved. Our findings highlight the need for integrating static code analysis into ADS development to detect and mitigate common vulnerabilities."
  },
  {
    "title": "MICINet: Multi-Level Inter-Class Confusing Information Removal for Reliable Multimodal Classification",
    "url": "http://arxiv.org/abs/2502.19674v1",
    "arxiv_id": "2502.19674v1",
    "authors": [
      "Tong Zhang",
      "Shu Shen",
      "C. L. Philip Chen"
    ],
    "published": "2025-02-27T01:33:28+00:00",
    "summary": "Reliable multimodal learning in the presence of noisy data is a widely concerned issue, especially in safety-critical applications. Many reliable multimodal methods delve into addressing modality-specific or cross-modality noise. However, they fail to handle the coexistence of both types of noise efficiently. Moreover, the lack of comprehensive consideration for noise at both global and individual levels limits their reliability. To address these issues, a reliable multimodal classification method dubbed Multi-Level Inter-Class Confusing Information Removal Network (MICINet) is proposed. MICINet achieves the reliable removal of both types of noise by unifying them into the concept of Inter-class Confusing Information (\\textit{ICI}) and eliminating it at both global and individual levels. Specifically, MICINet first reliably learns the global \\textit{ICI} distribution through the proposed \\textbf{\\textit{Global \\textbf{ICI} Learning Module}}. Then, it introduces the \\textbf{\\textit{Global-guided Sample ICI Learning module}} to efficiently remove global-level \\textit{ICI} from sample features utilizing the learned global \\textit{ICI} distribution. Subsequently, the \\textbf{\\textit{Sample-adaptive Cross-modality Information Compensation module}} is designed to remove individual-level \\textit{ICI} from each sample reliably. This is achieved through interpretable cross-modality information compensation based on the complementary relationship between discriminative features and \\textit{ICI} and the perception of the relative quality of modalities introduced by the relative discriminative power. Experiments on four datasets demonstrate that MICINet outperforms other state-of-the-art reliable multimodal classification methods under various noise conditions."
  },
  {
    "title": "Med-RLVR: Emerging Medical Reasoning from a 3B base model via reinforcement Learning",
    "url": "http://arxiv.org/abs/2502.19655v1",
    "arxiv_id": "2502.19655v1",
    "authors": [
      "Sheng Zhang",
      "Qianchu Liu",
      "Guanghui Qin",
      "Tristan Naumann",
      "Hoifung Poon"
    ],
    "published": "2025-02-27T00:54:38+00:00",
    "summary": "Reinforcement learning from verifiable rewards (RLVR) has recently gained attention for its ability to elicit self-evolved reasoning capabilitie from base language models without explicit reasoning supervisions, as demonstrated by DeepSeek-R1. While prior work on RLVR has primarily focused on mathematical and coding domains, its applicability to other tasks and domains remains unexplored. In this work, we investigate whether medical reasoning can emerge from RLVR. We introduce Med-RLVR as an initial study of RLVR in the medical domain leveraging medical multiple-choice question answering (MCQA) data as verifiable labels. Our results demonstrate that RLVR is not only effective for math and coding but also extends successfully to medical question answering. Notably, Med-RLVR achieves performance comparable to traditional supervised fine-tuning (SFT) on in-distribution tasks while significantly improving out-of-distribution generalization, with an 8-point accuracy gain. Further analysis of training dynamics reveals that, with no explicit reasoning supervision, reasoning emerges from the 3B-parameter base model. These findings underscore the potential of RLVR in domains beyond math and coding, opening new avenues for its application in knowledge-intensive fields such as medicine."
  },
  {
    "title": "3D Nephrographic Image Synthesis in CT Urography with the Diffusion Model and Swin Transformer",
    "url": "http://arxiv.org/abs/2502.19623v1",
    "arxiv_id": "2502.19623v1",
    "authors": [
      "Hongkun Yu",
      "Syed Jamal Safdar Gardezi",
      "E. Jason Abel",
      "Daniel Shapiro",
      "Meghan G. Lubner",
      "Joshua Warner",
      "Matthew Smith",
      "Giuseppe Toia",
      "Lu Mao",
      "Pallavi Tiwari",
      "Andrew L. Wentland"
    ],
    "published": "2025-02-26T23:22:31+00:00",
    "summary": "Purpose: This study aims to develop and validate a method for synthesizing 3D nephrographic phase images in CT urography (CTU) examinations using a diffusion model integrated with a Swin Transformer-based deep learning approach. Materials and Methods: This retrospective study was approved by the local Institutional Review Board. A dataset comprising 327 patients who underwent three-phase CTU (mean $\\pm$ SD age, 63 $\\pm$ 15 years; 174 males, 153 females) was curated for deep learning model development. The three phases for each patient were aligned with an affine registration algorithm. A custom deep learning model coined dsSNICT (diffusion model with a Swin transformer for synthetic nephrographic phase images in CT) was developed and implemented to synthesize the nephrographic images. Performance was assessed using Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index (SSIM), Mean Absolute Error (MAE), and Fr\\'{e}chet Video Distance (FVD). Qualitative evaluation by two fellowship-trained abdominal radiologists was performed. Results: The synthetic nephrographic images generated by our proposed approach achieved high PSNR (26.3 $\\pm$ 4.4 dB), SSIM (0.84 $\\pm$ 0.069), MAE (12.74 $\\pm$ 5.22 HU), and FVD (1323). Two radiologists provided average scores of 3.5 for real images and 3.4 for synthetic images (P-value = 0.5) on a Likert scale of 1-5, indicating that our synthetic images closely resemble real images. Conclusion: The proposed approach effectively synthesizes high-quality 3D nephrographic phase images. This model can be used to reduce radiation dose in CTU by 33.3\\% without compromising image quality, which thereby enhances the safety and diagnostic utility of CT urography."
  },
  {
    "title": "No, of course I can! Refusal Mechanisms Can Be Exploited Using Harmless Fine-Tuning Data",
    "url": "http://arxiv.org/abs/2502.19537v1",
    "arxiv_id": "2502.19537v1",
    "authors": [
      "Joshua Kazdan",
      "Lisa Yu",
      "Rylan Schaeffer",
      "Chris Cundy",
      "Sanmi Koyejo",
      "Dvijotham Krishnamurthy"
    ],
    "published": "2025-02-26T20:20:01+00:00",
    "summary": "Leading language model (LM) providers like OpenAI and Google offer fine-tuning APIs that allow customers to adapt LMs for specific use cases. To prevent misuse, these LM providers implement filtering mechanisms to block harmful fine-tuning data. Consequently, adversaries seeking to produce unsafe LMs via these APIs must craft adversarial training data that are not identifiably harmful. We make three contributions in this context: 1. We show that many existing attacks that use harmless data to create unsafe LMs rely on eliminating model refusals in the first few tokens of their responses. 2. We show that such prior attacks can be blocked by a simple defense that pre-fills the first few tokens from an aligned model before letting the fine-tuned model fill in the rest. 3. We describe a new data-poisoning attack, ``No, Of course I Can Execute'' (NOICE), which exploits an LM's formulaic refusal mechanism to elicit harmful responses. By training an LM to refuse benign requests on the basis of safety before fulfilling those requests regardless, we are able to jailbreak several open-source models and a closed-source model (GPT-4o). We show an attack success rate (ASR) of 57% against GPT-4o; our attack earned a Bug Bounty from OpenAI. Against open-source models protected by simple defenses, we improve ASRs by an average of 3.25 times compared to the best performing previous attacks that use only harmless data. NOICE demonstrates the exploitability of repetitive refusal mechanisms and broadens understanding of the threats closed-source models face from harmless data."
  },
  {
    "title": "Can Language Models Falsify? Evaluating Algorithmic Reasoning with Counterexample Creation",
    "url": "http://arxiv.org/abs/2502.19414v1",
    "arxiv_id": "2502.19414v1",
    "authors": [
      "Shiven Sinha",
      "Shashwat Goel",
      "Ponnurangam Kumaraguru",
      "Jonas Geiping",
      "Matthias Bethge",
      "Ameya Prabhu"
    ],
    "published": "2025-02-26T18:58:13+00:00",
    "summary": "There is growing excitement about the potential of Language Models (LMs) to accelerate scientific discovery. Falsifying hypotheses is key to scientific progress, as it allows claims to be iteratively refined over time. This process requires significant researcher effort, reasoning, and ingenuity. Yet current benchmarks for LMs predominantly assess their ability to generate solutions rather than challenge them. We advocate for developing benchmarks that evaluate this inverse capability - creating counterexamples for subtly incorrect solutions. To demonstrate this approach, we start with the domain of algorithmic problem solving, where counterexamples can be evaluated automatically using code execution. Specifically, we introduce REFUTE, a dynamically updating benchmark that includes recent problems and incorrect submissions from programming competitions, where human experts successfully identified counterexamples. Our analysis finds that the best reasoning agents, even OpenAI o3-mini (high) with code execution feedback, can create counterexamples for only <9% of incorrect solutions in REFUTE, even though ratings indicate its ability to solve up to 48% of these problems from scratch. We hope our work spurs progress in evaluating and enhancing LMs' ability to falsify incorrect solutions - a capability that is crucial for both accelerating research and making models self-improve through reliable reflective reasoning."
  },
  {
    "title": "ARENA: Adaptive Risk-aware and Energy-efficient NAvigation for Multi-Objective 3D Infrastructure Inspection with a UAV",
    "url": "http://arxiv.org/abs/2502.19401v1",
    "arxiv_id": "2502.19401v1",
    "authors": [
      "David-Alexandre Poissant",
      "Alexis Lussier Desbiens",
      "Fran\u00e7ois Ferland",
      "Louis Petit"
    ],
    "published": "2025-02-26T18:50:49+00:00",
    "summary": "Autonomous robotic inspection missions require balancing multiple conflicting objectives while navigating near costly obstacles. Current multi-objective path planning (MOPP) methods struggle to adapt to evolving risks like localization errors, weather, battery state, and communication issues. This letter presents an Adaptive Risk-aware and Energy-efficient NAvigation (ARENA) MOPP approach for UAVs in complex 3D environments. Our method enables online trajectory adaptation by optimizing safety, time, and energy using 4D NURBS representation and a genetic-based algorithm to generate the Pareto front. A novel risk-aware voting algorithm ensures adaptivity. Simulations and real-world tests demonstrate the planner's ability to produce diverse, optimized trajectories covering 95% or more of the range defined by single-objective benchmarks and its ability to estimate power consumption with a mean error representing 14% of the full power range. The ARENA framework enhances UAV autonomy and reliability in critical, evolving 3D missions."
  },
  {
    "title": "TheoremExplainAgent: Towards Multimodal Explanations for LLM Theorem Understanding",
    "url": "http://arxiv.org/abs/2502.19400v1",
    "arxiv_id": "2502.19400v1",
    "authors": [
      "Max Ku",
      "Thomas Chong",
      "Jonathan Leung",
      "Krish Shah",
      "Alvin Yu",
      "Wenhu Chen"
    ],
    "published": "2025-02-26T18:50:09+00:00",
    "summary": "Understanding domain-specific theorems often requires more than just text-based reasoning; effective communication through structured visual explanations is crucial for deeper comprehension. While large language models (LLMs) demonstrate strong performance in text-based theorem reasoning, their ability to generate coherent and pedagogically meaningful visual explanations remains an open challenge. In this work, we introduce TheoremExplainAgent, an agentic approach for generating long-form theorem explanation videos (over 5 minutes) using Manim animations. To systematically evaluate multimodal theorem explanations, we propose TheoremExplainBench, a benchmark covering 240 theorems across multiple STEM disciplines, along with 5 automated evaluation metrics. Our results reveal that agentic planning is essential for generating detailed long-form videos, and the o3-mini agent achieves a success rate of 93.8% and an overall score of 0.77. However, our quantitative and qualitative studies show that most of the videos produced exhibit minor issues with visual element layout. Furthermore, multimodal explanations expose deeper reasoning flaws that text-based explanations fail to reveal, highlighting the importance of multimodal explanations."
  },
  {
    "title": "Can Large Language Models Detect Errors in Long Chain-of-Thought Reasoning?",
    "url": "http://arxiv.org/abs/2502.19361v1",
    "arxiv_id": "2502.19361v1",
    "authors": [
      "Yancheng He",
      "Shilong Li",
      "Jiaheng Liu",
      "Weixun Wang",
      "Xingyuan Bu",
      "Ge Zhang",
      "Zhongyuan Peng",
      "Zhaoxiang Zhang",
      "Wenbo Su",
      "Bo Zheng"
    ],
    "published": "2025-02-26T17:59:27+00:00",
    "summary": "Recently, o1-like models have drawn significant attention, where these models produce the long Chain-of-Thought (CoT) reasoning steps to improve the reasoning abilities of existing Large Language Models (LLMs). In this paper, to understand the qualities of these long CoTs and measure the critique abilities of existing LLMs on these long CoTs, we introduce the DeltaBench, including the generated long CoTs from different o1-like models (e.g., QwQ, DeepSeek-R1) for different reasoning tasks (e.g., Math, Code, General Reasoning), to measure the ability to detect errors in long CoT reasoning. Based on DeltaBench, we first perform fine-grained analysis of the generated long CoTs to discover the effectiveness and efficiency of different o1-like models. Then, we conduct extensive evaluations of existing process reward models (PRMs) and critic models to detect the errors of each annotated process, which aims to investigate the boundaries and limitations of existing PRMs and critic models. Finally, we hope that DeltaBench could guide developers to better understand the long CoT reasoning abilities of their models."
  },
  {
    "title": "Can Large Language Models Detect Errors in Long Chain-of-Thought Reasoning?",
    "url": "http://arxiv.org/abs/2502.19361v2",
    "arxiv_id": "2502.19361v2",
    "authors": [
      "Yancheng He",
      "Shilong Li",
      "Jiaheng Liu",
      "Weixun Wang",
      "Xingyuan Bu",
      "Ge Zhang",
      "Zhongyuan Peng",
      "Zhaoxiang Zhang",
      "Zhicheng Zheng",
      "Wenbo Su",
      "Bo Zheng"
    ],
    "published": "2025-02-26T17:59:27+00:00",
    "summary": "Recently, o1-like models have drawn significant attention, where these models produce the long Chain-of-Thought (CoT) reasoning steps to improve the reasoning abilities of existing Large Language Models (LLMs). In this paper, to understand the qualities of these long CoTs and measure the critique abilities of existing LLMs on these long CoTs, we introduce the DeltaBench, including the generated long CoTs from different o1-like models (e.g., QwQ, DeepSeek-R1) for different reasoning tasks (e.g., Math, Code, General Reasoning), to measure the ability to detect errors in long CoT reasoning. Based on DeltaBench, we first perform fine-grained analysis of the generated long CoTs to discover the effectiveness and efficiency of different o1-like models. Then, we conduct extensive evaluations of existing process reward models (PRMs) and critic models to detect the errors of each annotated process, which aims to investigate the boundaries and limitations of existing PRMs and critic models. Finally, we hope that DeltaBench could guide developers to better understand the long CoT reasoning abilities of their models."
  },
  {
    "title": "Anomaly Detection in Complex Dynamical Systems: A Systematic Framework Using Embedding Theory and Physics-Inspired Consistency",
    "url": "http://arxiv.org/abs/2502.19307v1",
    "arxiv_id": "2502.19307v1",
    "authors": [
      "Michael Somma",
      "Thomas Gallien",
      "Branka Stojanovic"
    ],
    "published": "2025-02-26T17:06:13+00:00",
    "summary": "Anomaly detection in complex dynamical systems is essential for ensuring reliability, safety, and efficiency in industrial and cyber-physical infrastructures. Predictive maintenance helps prevent costly failures, while cybersecurity monitoring has become critical as digitized systems face growing threats. Many of these systems exhibit oscillatory behaviors and bounded motion, requiring anomaly detection methods that capture structured temporal dependencies while adhering to physical consistency principles. In this work, we propose a system-theoretic approach to anomaly detection, grounded in classical embedding theory and physics-inspired consistency principles. We build upon the Fractal Whitney Embedding Prevalence Theorem, extending traditional embedding techniques to complex system dynamics. Additionally, we introduce state-derivative pairs as an embedding strategy to capture system evolution. To enforce temporal coherence, we develop a Temporal Differential Consistency Autoencoder (TDC-AE), incorporating a TDC-Loss that aligns the approximated derivatives of latent variables with their dynamic representations. We evaluate our method on the C-MAPSS dataset, a benchmark for turbofan aeroengine degradation. TDC-AE outperforms LSTMs and Transformers while achieving a 200x reduction in MAC operations, making it particularly suited for lightweight edge computing. Our findings support the hypothesis that anomalies disrupt stable system dynamics, providing a robust, interpretable signal for anomaly detection."
  },
  {
    "title": "Utility-Based Dose Optimization Approaches for Multiple-Dose Randomized Trial Designs Accounting for Multiple Endpoints",
    "url": "http://arxiv.org/abs/2502.19216v1",
    "arxiv_id": "2502.19216v1",
    "authors": [
      "Gina DAngelo",
      "Guannan Chen",
      "Di Ran"
    ],
    "published": "2025-02-26T15:18:17+00:00",
    "summary": "The initiation of dose optimization has driven a paradigm shift in oncology clinical trials to determine the optimal biological dose (OBD). Early-phase trials with randomized doses can facilitate additional investigation of the identified OBD in targeted populations by incorporating safety, efficacy, and biomarker data. To support dose comparison in such settings, we propose to extend the utility score-based approach (U-MET) and introduce the clinical utility index-based approach (CUI-MET) to account for multiple endpoints and doses. The utility-based dose optimization approach for multiple-dose randomized trial designs accounting for multiple endpoints and doses (U-MET-m) extends the U-MET, using a utility score to account for multiple endpoints jointly (e.g., toxicity-efficacy trade-off), while the CUI-MET uses a utility index to do this marginally. U-MET-m and CUI-MET use Bayesian inference within a hypothesis framework to compare utility metrics across doses to identify the OBD. Here we describe simulation studies and present an example to compare the U-MET-m design, CUI-MET, and empirical design. The U-MET-m design and CUI-MET were shown to have satisfactory operating characteristics for selecting the OBD. Based on these findings, we recommend using the U-MET-m and CUI-MET designs as the primary dose comparison approach or as supportive evidence to select the OBD."
  },
  {
    "title": "When Personalization Meets Reality: A Multi-Faceted Analysis of Personalized Preference Learning",
    "url": "http://arxiv.org/abs/2502.19158v1",
    "arxiv_id": "2502.19158v1",
    "authors": [
      "Yijiang River Dong",
      "Tiancheng Hu",
      "Yinhong Liu",
      "Ahmet \u00dcst\u00fcn",
      "Nigel Collier"
    ],
    "published": "2025-02-26T14:14:58+00:00",
    "summary": "While Reinforcement Learning from Human Feedback (RLHF) is widely used to align Large Language Models (LLMs) with human preferences, it typically assumes homogeneous preferences across users, overlooking diverse human values and minority viewpoints. Although personalized preference learning addresses this by tailoring separate preferences for individual users, the field lacks standardized methods to assess its effectiveness. We present a multi-faceted evaluation framework that measures not only performance but also fairness, unintended effects, and adaptability across varying levels of preference divergence. Through extensive experiments comparing eight personalization methods across three preference datasets, we demonstrate that performance differences between methods could reach 36% when users strongly disagree, and personalization can introduce up to 20% safety misalignment. These findings highlight the critical need for holistic evaluation approaches to advance the development of more effective and inclusive preference learning systems."
  },
  {
    "title": "Formal Verification of PLCs as a Service: A CERN-GSI Safety-Critical Case Study (extended version)",
    "url": "http://arxiv.org/abs/2502.19150v1",
    "arxiv_id": "2502.19150v1",
    "authors": [
      "Ignacio D. Lopez-Miguel",
      "Borja Fern\u00e1ndez Adiego",
      "Matias Salinas",
      "Christine Betz"
    ],
    "published": "2025-02-26T14:08:58+00:00",
    "summary": "The increased technological complexity and demand for software reliability require organizations to formally design and verify their safety-critical programs to minimize systematic failures. Formal methods are recommended by functional safety standards (e.g., by IEC 61511 for the process industry and by the generic IEC 61508) and play a crucial role. Their structured approach reduces ambiguity in system requirements, facilitating early error detection. This paper introduces a formal verification service for PLC (programmable logic controller) programs compliant with functional safety standards, providing external expertise to organizations while eliminating the need for extensive internal training. It offers a cost-effective solution to meet the rising demands for formal verification processes. The approach is extended to include modeling time-dependent, know-how-protected components, enabling formal verification of real safety-critical applications. A case study shows the application of PLC formal verification as a service provided by CERN in a safety-critical installation at the GSI particle accelerator facility."
  },
  {
    "title": "Multi-Agent Security Tax: Trading Off Security and Collaboration Capabilities in Multi-Agent Systems",
    "url": "http://arxiv.org/abs/2502.19145v1",
    "arxiv_id": "2502.19145v1",
    "authors": [
      "Pierre Peigne-Lefebvre",
      "Mikolaj Kniejski",
      "Filip Sondej",
      "Matthieu David",
      "Jason Hoelscher-Obermaier",
      "Christian Schroeder de Witt",
      "Esben Kran"
    ],
    "published": "2025-02-26T14:00:35+00:00",
    "summary": "As AI agents are increasingly adopted to collaborate on complex objectives, ensuring the security of autonomous multi-agent systems becomes crucial. We develop simulations of agents collaborating on shared objectives to study these security risks and security trade-offs. We focus on scenarios where an attacker compromises one agent, using it to steer the entire system toward misaligned outcomes by corrupting other agents. In this context, we observe infectious malicious prompts - the multi-hop spreading of malicious instructions. To mitigate this risk, we evaluated several strategies: two \"vaccination\" approaches that insert false memories of safely handling malicious input into the agents' memory stream, and two versions of a generic safety instruction strategy. While these defenses reduce the spread and fulfillment of malicious instructions in our experiments, they tend to decrease collaboration capability in the agent network. Our findings illustrate potential trade-off between security and collaborative efficiency in multi-agent systems, providing insights for designing more secure yet effective AI collaborations."
  },
  {
    "title": "A 106K Multi-Topic Multilingual Conversational User Dataset with Emoticons",
    "url": "http://arxiv.org/abs/2502.19108v1",
    "arxiv_id": "2502.19108v1",
    "authors": [
      "Heng Er Metilda Chee",
      "Jiayin Wang",
      "Zhiqiang Guo",
      "Weizhi Ma",
      "Qinglang Guo",
      "Min Zhang"
    ],
    "published": "2025-02-26T12:50:58+00:00",
    "summary": "Instant messaging has become a predominant form of communication, with texts and emoticons enabling users to express emotions and ideas efficiently. Emoticons, in particular, have gained significant traction as a medium for conveying sentiments and information, leading to the growing importance of emoticon retrieval and recommendation systems. However, one of the key challenges in this area has been the absence of datasets that capture both the temporal dynamics and user-specific interactions with emoticons, limiting the progress of personalized user modeling and recommendation approaches. To address this, we introduce the emoticon dataset, a comprehensive resource that includes time-based data along with anonymous user identifiers across different conversations. As the largest publicly accessible emoticon dataset to date, it comprises 22K unique users, 370K emoticons, and 8.3M messages. The data was collected from a widely-used messaging platform across 67 conversations and 720 hours of crawling. Strict privacy and safety checks were applied to ensure the integrity of both text and image data. Spanning across 10 distinct domains, the emoticon dataset provides rich insights into temporal, multilingual, and cross-domain behaviors, which were previously unavailable in other emoticon-based datasets. Our in-depth experiments, both quantitative and qualitative, demonstrate the dataset's potential in modeling user behavior and personalized recommendation systems, opening up new possibilities for research in personalized retrieval and conversational AI. The dataset is freely accessible."
  },
  {
    "title": "Nexus: A Lightweight and Scalable Multi-Agent Framework for Complex Tasks Automation",
    "url": "http://arxiv.org/abs/2502.19091v1",
    "arxiv_id": "2502.19091v1",
    "authors": [
      "Humza Sami",
      "Mubashir ul Islam",
      "Samy Charas",
      "Asav Gandhi",
      "Pierre-Emmanuel Gaillardon",
      "Valerio Tenace"
    ],
    "published": "2025-02-26T12:37:47+00:00",
    "summary": "Recent advancements in Large Language Models (LLMs) have substantially evolved Multi-Agent Systems (MASs) capabilities, enabling systems that not only automate tasks but also leverage near-human reasoning capabilities. To achieve this, LLM-based MASs need to be built around two critical principles: (i) a robust architecture that fully exploits LLM potential for specific tasks -- or related task sets -- and ($ii$) an effective methodology for equipping LLMs with the necessary capabilities to perform tasks and manage information efficiently. It goes without saying that a priori architectural designs can limit the scalability and domain adaptability of a given MAS.   To address these challenges, in this paper we introduce Nexus: a lightweight Python framework designed to easily build and manage LLM-based MASs. Nexus introduces the following innovations: (i) a flexible multi-supervisor hierarchy, (ii) a simplified workflow design, and (iii) easy installation and open-source flexibility: Nexus can be installed via pip and is distributed under a permissive open-source license, allowing users to freely modify and extend its capabilities.   Experimental results demonstrate that architectures built with Nexus exhibit state-of-the-art performance across diverse domains. In coding tasks, Nexus-driven MASs achieve a 99% pass rate on HumanEval and a flawless 100% on VerilogEval-Human, outperforming cutting-edge reasoning language models such as o3-mini and DeepSeek-R1. Moreover, these architectures display robust proficiency in complex reasoning and mathematical problem solving, achieving correct solutions for all randomly selected problems from the MATH dataset. In the realm of multi-objective optimization, Nexus-based architectures successfully address challenging timing closure tasks on designs from the VTR benchmark suite, while guaranteeing, on average, a power saving of nearly 30%."
  },
  {
    "title": "MathClean: A Benchmark for Synthetic Mathematical Data Cleaning",
    "url": "http://arxiv.org/abs/2502.19058v1",
    "arxiv_id": "2502.19058v1",
    "authors": [
      "Hao Liang",
      "Meiyi Qiang",
      "Yuying Li",
      "Zefeng He",
      "Yongzhen Guo",
      "Zhengzhou Zhu",
      "Wentao Zhang",
      "Bin Cui"
    ],
    "published": "2025-02-26T11:17:50+00:00",
    "summary": "With the rapid development of large language models (LLMs), the quality of training data has become crucial. Among the various types of training data, mathematical data plays a key role in enabling LLMs to acquire strong reasoning abilities. While high-quality open-source data is important, it is often insufficient for pre-training, necessitating the addition of synthetic math problems. However, synthetic math questions and answers can introduce inaccuracies, which may degrade both the training data and web data. Therefore, an effective method for cleaning synthetic math data is essential. In this paper, we propose the MathClean benchmark to evaluate the effectiveness of math data cleaning models. The MathClean benchmark consists of 2,000 correct questions and 2,000 erroneous questions with additional 2,000 correct and erroneous answers sourced from augmented data based on GSM8K and MATH. Moreover, we also annotate error types for each question or answer, since it can assess whether models can correctly identify the error categories for future improvements. Finally, we present comprehensive evaluations using state-of-the-art (SOTA) models. Our results demonstrate that even strong models like GPT-o1 and DeepSeek-R1 perform poorly on this benchmark, highlighting the utility of MathClean. Our code and data is available at https://github.com/YuYingLi0/MathClean."
  },
  {
    "title": "An Improved 3D Skeletons UP-Fall Dataset: Enhancing Data Quality for Efficient Impact Fall Detection",
    "url": "http://arxiv.org/abs/2502.19048v1",
    "arxiv_id": "2502.19048v1",
    "authors": [
      "Tresor Y. Koffi",
      "Youssef Mourchid",
      "Mohammed Hindawi",
      "Yohan Dupuis"
    ],
    "published": "2025-02-26T11:02:44+00:00",
    "summary": "Detecting impact where an individual makes contact with the ground within a fall event is crucial in fall detection systems, particularly for elderly care where prompt intervention can prevent serious injuries. The UP-Fall dataset, a key resource in fall detection research, has proven valuable but suffers from limitations in data accuracy and comprehensiveness. These limitations cause confusion in distinguishing between non-impact events, such as sliding, and real falls with impact, where the person actually hits the ground. This confusion compromises the effectiveness of current fall detection systems. This study presents enhancements to the UP-Fall dataset aiming at improving it for impact fall detection by incorporating 3D skeleton data. Our preprocessing techniques ensure high data accuracy and comprehensiveness, enabling a more reliable impact fall detection. Extensive experiments were conducted using various machine learning and deep learning algorithms to benchmark the improved 3D skeletons dataset. The results demonstrate substantial improvements in the performance of fall detection models trained on the enhanced dataset. This contribution aims to enhance the safety and well-being of the elderly population at risk. To support further research and development of building more reliable impact fall detection systems, we have made the improved 3D skeletons UP-Fall dataset publicly available at this link https://zenodo.org/records/12773013."
  },
  {
    "title": "Beyond Surface-Level Patterns: An Essence-Driven Defense Framework Against Jailbreak Attacks in LLMs",
    "url": "http://arxiv.org/abs/2502.19041v1",
    "arxiv_id": "2502.19041v1",
    "authors": [
      "Shiyu Xiang",
      "Ansen Zhang",
      "Yanfei Cao",
      "Yang Fan",
      "Ronghao Chen"
    ],
    "published": "2025-02-26T10:53:58+00:00",
    "summary": "Although Aligned Large Language Models (LLMs) are trained to refuse harmful requests, they remain vulnerable to jailbreak attacks. Unfortunately, existing methods often focus on surface-level patterns, overlooking the deeper attack essences. As a result, defenses fail when attack prompts change, even though the underlying \"attack essence\" remains the same. To address this issue, we introduce EDDF, an \\textbf{E}ssence-\\textbf{D}riven \\textbf{D}efense \\textbf{F}ramework Against Jailbreak Attacks in LLMs. EDDF is a plug-and-play input-filtering method and operates in two stages: 1) offline essence database construction, and 2) online adversarial query detection. The key idea behind EDDF is to extract the \"attack essence\" from a diverse set of known attack instances and store it in an offline vector database. Experimental results demonstrate that EDDF significantly outperforms existing methods by reducing the Attack Success Rate by at least 20\\%, underscoring its superior robustness against jailbreak attacks."
  },
  {
    "title": "Polarization Angle Scanning for Wide-band Millimeter-wave Direct Detection",
    "url": "http://arxiv.org/abs/2502.18981v1",
    "arxiv_id": "2502.18981v1",
    "authors": [
      "Heyao Wang",
      "Ziran Zhao",
      "Lingbo Qiao",
      "Dalu Guo"
    ],
    "published": "2025-02-26T09:43:09+00:00",
    "summary": "Millimeter-wave (MMW) technology has been widely utilized in human security screening applications due to its superior penetration capabilities through clothing and safety for human exposure. However, existing methods largely rely on fixed polarization modes, neglecting the potential insights from variations in target echoes with respect to incident polarization. This study provides a theoretical analysis of the cross-polarization echo power as a function of the incident polarization angle under linear polarization conditions. Additionally, based on the transmission characteristics of multi-layer medium, we extended the depth spectrum model employed in direct detection to accommodate scenarios involving multi-layered structures. Building on this foundation, by obtaining multiple depth spectrums through polarization angle scanning, we propose the Polarization Angle-Depth Matrix to characterize target across both the polarization angle and depth dimensions in direct detection. Simulations and experimental validations confirm its accuracy and practical value in detecting concealed weapons in human security screening scenarios."
  },
  {
    "title": "JailBench: A Comprehensive Chinese Security Assessment Benchmark for Large Language Models",
    "url": "http://arxiv.org/abs/2502.18935v1",
    "arxiv_id": "2502.18935v1",
    "authors": [
      "Shuyi Liu",
      "Simiao Cui",
      "Haoran Bu",
      "Yuming Shang",
      "Xi Zhang"
    ],
    "published": "2025-02-26T08:36:42+00:00",
    "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across various applications, highlighting the urgent need for comprehensive safety evaluations. In particular, the enhanced Chinese language proficiency of LLMs, combined with the unique characteristics and complexity of Chinese expressions, has driven the emergence of Chinese-specific benchmarks for safety assessment. However, these benchmarks generally fall short in effectively exposing LLM safety vulnerabilities. To address the gap, we introduce JailBench, the first comprehensive Chinese benchmark for evaluating deep-seated vulnerabilities in LLMs, featuring a refined hierarchical safety taxonomy tailored to the Chinese context. To improve generation efficiency, we employ a novel Automatic Jailbreak Prompt Engineer (AJPE) framework for JailBench construction, which incorporates jailbreak techniques to enhance assessing effectiveness and leverages LLMs to automatically scale up the dataset through context-learning. The proposed JailBench is extensively evaluated over 13 mainstream LLMs and achieves the highest attack success rate against ChatGPT compared to existing Chinese benchmarks, underscoring its efficacy in identifying latent vulnerabilities in LLMs, as well as illustrating the substantial room for improvement in the security and trustworthiness of LLMs within the Chinese context. Our benchmark is publicly available at https://github.com/STAIR-BUPT/JailBench."
  },
  {
    "title": "Adaptive Shielding via Parametric Safety Proofs",
    "url": "http://arxiv.org/abs/2502.18879v1",
    "arxiv_id": "2502.18879v1",
    "authors": [
      "Yao Feng",
      "Jun Zhu",
      "Andr\u00e9 Platzer",
      "Jonathan Laurent"
    ],
    "published": "2025-02-26T06:50:48+00:00",
    "summary": "A major challenge to deploying cyber-physical systems with learning-enabled controllers is to ensure their safety, especially in the face of changing environments that necessitate runtime knowledge acquisition. Model-checking and automated reasoning have been successfully used for shielding, i.e., to monitor untrusted controllers and override potentially unsafe decisions, but only at the cost of hard tradeoffs in terms of expressivity, safety, adaptivity, precision and runtime efficiency. We propose a programming-language framework that allows experts to statically specify adaptive shields for learning-enabled agents, which enforce a safe control envelope that gets more permissive as knowledge is gathered at runtime. A shield specification provides a safety model that is parametric in the current agent's knowledge. In addition, a nondeterministic inference strategy can be specified using a dedicated domain-specific language, enforcing that such knowledge parameters are inferred at runtime in a statistically-sound way. By leveraging language design and theorem proving, our proposed framework empowers experts to design adaptive shields with an unprecedented level of modeling flexibility, while providing rigorous, end-to-end probabilistic safety guarantees."
  },
  {
    "title": "Investigating Generalization of One-shot LLM Steering Vectors",
    "url": "http://arxiv.org/abs/2502.18862v1",
    "arxiv_id": "2502.18862v1",
    "authors": [
      "Jacob Dunefsky",
      "Arman Cohan"
    ],
    "published": "2025-02-26T06:13:01+00:00",
    "summary": "Steering vectors have emerged as a promising approach for interpreting and controlling LLMs, but current methods typically require large contrastive datasets that are often impractical to construct and may capture spurious correlations. We propose directly optimizing steering vectors through gradient descent on a single training example, and systematically investigate how these vectors generalize. We consider several steering optimization techniques, including multiple novel ones, and find that the resulting vectors effectively mediate safety-relevant behaviors in multiple models. Indeed, in experiments on an alignment-faking model, we are able to optimize one-shot steering vectors that induce harmful behavior on benign examples and whose negations suppress harmful behavior on malign examples. And in experiments on refusal suppression, we demonstrate that one-shot optimized steering vectors can transfer across inputs, yielding a Harmbench attack success rate of 96.9%. Furthermore, to quantitatively assess steering effectiveness in instruction-tuned models, we develop a novel evaluation framework using sequence probabilities from the corresponding base model. With this framework, we analyze how steering vectors modulate an instruction-tuned LLM's ability to recover from outputting false information, and find that this ability derives from the base model. Overall, our findings suggest that optimizing steering vectors on a single example can mediate misaligned behavior in LLMs, and provide a path toward better understanding the relationship between LLM behavior and activation space structure."
  },
  {
    "title": "Safe and usable kernel extensions with Rax",
    "url": "http://arxiv.org/abs/2502.18832v1",
    "arxiv_id": "2502.18832v1",
    "authors": [
      "Jinghao Jia",
      "Ruowen Qin",
      "Milo Craun",
      "Egor Lukiyanov",
      "Ayush Bansal",
      "Michael V. Le",
      "Hubertus Franke",
      "Hani Jamjoom",
      "Tianyin Xu",
      "Dan Williams"
    ],
    "published": "2025-02-26T05:16:06+00:00",
    "summary": "Safe kernel extensions have gained significant traction, evolving from simple packet filters to large, complex programs that customize storage, networking, and scheduling. Existing kernel extension mechanisms like eBPF rely on in-kernel verifiers to ensure safety of kernel extensions by static verification using symbolic execution. We identify significant usability issues -- safe extensions being rejected by the verifier -- due to the language-verifier gap, a mismatch between developers' expectation of program safety provided by a contract with the programming language, and the verifier's expectation.   We present Rax, a new kernel extension framework that closes the language-verifier gap and improves the usability of kernel extensions in terms of programming experience and maintainability. Rax builds upon language-based safety to provide safety properties desired by kernel extensions, along with a lightweight extralingual runtime for properties that are unsuitable for static analysis, including safe exception handling, stack safety, and termination. With Rax, kernel extensions are written in safe Rust and interact with the kernel via a safe interface provided by Rax's kernel crate. No separate static verification is needed. Rax addresses usability issues of eBPF kernel extensions without compromising performance."
  },
  {
    "title": "Simulating Safe Bite Transfer in Robot-Assisted Feeding with a Soft Head and Articulated Jaw",
    "url": "http://arxiv.org/abs/2502.18749v1",
    "arxiv_id": "2502.18749v1",
    "authors": [
      "Yi Heng San",
      "Vasanthamaran Ravichandram",
      "J-Anne Yow",
      "Sherwin Stephen Chan",
      "Yifan Wang",
      "Wei Tech Ang"
    ],
    "published": "2025-02-26T01:52:04+00:00",
    "summary": "Ensuring safe and comfortable bite transfer during robot-assisted feeding is challenging due to the close physical human-robot interaction required. This paper presents a novel approach to modeling physical human-robot interaction in a physics-based simulator (MuJoCo) using soft-body dynamics. We integrate a flexible head model with a rigid skeleton while accounting for internal dynamics, enabling the flexible model to be actuated by the skeleton. Incorporating realistic soft-skin contact dynamics in simulation allows for systematically evaluating bite transfer parameters, such as insertion depth and entry angle, and their impact on user safety and comfort. Our findings suggest that a straight-in-straight-out strategy minimizes forces and enhances user comfort in robot-assisted feeding, assuming a static head. This simulation-based approach offers a safer and more controlled alternative to real-world experimentation. Supplementary videos can be found at: https://tinyurl.com/224yh2kx."
  },
  {
    "title": "AI Mismatches: Identifying Potential Algorithmic Harms Before AI Development",
    "url": "http://arxiv.org/abs/2502.18682v1",
    "arxiv_id": "2502.18682v1",
    "authors": [
      "Devansh Saxena",
      "Ji-Youn Jung",
      "Jodi Forlizzi",
      "Kenneth Holstein",
      "John Zimmerman"
    ],
    "published": "2025-02-25T22:43:00+00:00",
    "summary": "AI systems are often introduced with high expectations, yet many fail to deliver, resulting in unintended harm and missed opportunities for benefit. We frequently observe significant \"AI Mismatches\", where the system's actual performance falls short of what is needed to ensure safety and co-create value. These mismatches are particularly difficult to address once development is underway, highlighting the need for early-stage intervention. Navigating complex, multi-dimensional risk factors that contribute to AI Mismatches is a persistent challenge. To address it, we propose an AI Mismatch approach to anticipate and mitigate risks early on, focusing on the gap between realistic model performance and required task performance. Through an analysis of 774 AI cases, we extracted a set of critical factors, which informed the development of seven matrices that map the relationships between these factors and highlight high-risk areas. Through case studies, we demonstrate how our approach can help reduce risks in AI development."
  },
  {
    "title": "Provably Efficient RL for Linear MDPs under Instantaneous Safety Constraints in Non-Convex Feature Spaces",
    "url": "http://arxiv.org/abs/2502.18655v1",
    "arxiv_id": "2502.18655v1",
    "authors": [
      "Amirhossein Roknilamouki",
      "Arnob Ghosh",
      "Ming Shi",
      "Fatemeh Nourzad",
      "Eylem Ekici",
      "Ness B. Shroff"
    ],
    "published": "2025-02-25T21:32:55+00:00",
    "summary": "In Reinforcement Learning (RL), tasks with instantaneous hard constraints present significant challenges, particularly when the decision space is non-convex or non-star-convex. This issue is especially relevant in domains like autonomous vehicles and robotics, where constraints such as collision avoidance often take a non-convex form. In this paper, we establish a regret bound of $\\tilde{\\mathcal{O}}\\bigl(\\bigl(1 + \\tfrac{1}{\\tau}\\bigr) \\sqrt{\\log(\\tfrac{1}{\\tau}) d^3 H^4 K} \\bigr)$, applicable to both star-convex and non-star-convex cases, where $d$ is the feature dimension, $H$ the episode length, $K$ the number of episodes, and $\\tau$ the safety threshold. Moreover, the violation of safety constraints is zero with high probability throughout the learning process. A key technical challenge in these settings is bounding the covering number of the value-function class, which is essential for achieving value-aware uniform concentration in model-free function approximation. For the star-convex setting, we develop a novel technique called Objective Constraint-Decomposition (OCD) to properly bound the covering number. This result also resolves an error in a previous work on constrained RL. In non-star-convex scenarios, where the covering number can become infinitely large, we propose a two-phase algorithm, Non-Convex Safe Least Squares Value Iteration (NCS-LSVI), which first reduces uncertainty about the safe set by playing a known safe policy. After that, it carefully balances exploration and exploitation to achieve the regret bound. Finally, numerical simulations on an autonomous driving scenario demonstrate the effectiveness of NCS-LSVI."
  },
  {
    "title": "Faster, Cheaper, Better: Multi-Objective Hyperparameter Optimization for LLM and RAG Systems",
    "url": "http://arxiv.org/abs/2502.18635v1",
    "arxiv_id": "2502.18635v1",
    "authors": [
      "Matthew Barker",
      "Andrew Bell",
      "Evan Thomas",
      "James Carr",
      "Thomas Andrews",
      "Umang Bhatt"
    ],
    "published": "2025-02-25T20:52:06+00:00",
    "summary": "While Retrieval Augmented Generation (RAG) has emerged as a popular technique for improving Large Language Model (LLM) systems, it introduces a large number of choices, parameters and hyperparameters that must be made or tuned. This includes the LLM, embedding, and ranker models themselves, as well as hyperparameters governing individual RAG components. Yet, collectively optimizing the entire configuration in a RAG or LLM system remains under-explored - especially in multi-objective settings - due to intractably large solution spaces, noisy objective evaluations, and the high cost of evaluations. In this work, we introduce the first approach for multi-objective parameter optimization of cost, latency, safety and alignment over entire LLM and RAG systems. We find that Bayesian optimization methods significantly outperform baseline approaches, obtaining a superior Pareto front on two new RAG benchmark tasks. We conclude our work with important considerations for practitioners who are designing multi-objective RAG systems, highlighting nuances such as how optimal configurations may not generalize across tasks and objectives."
  },
  {
    "title": "Autonomous Vision-Guided Resection of Central Airway Obstruction",
    "url": "http://arxiv.org/abs/2502.18586v1",
    "arxiv_id": "2502.18586v1",
    "authors": [
      "M. E. Smith",
      "N. Yilmaz",
      "T. Watts",
      "P. M. Scheikl",
      "J. Ge",
      "A. Deguet",
      "A. Kuntz",
      "A. Krieger"
    ],
    "published": "2025-02-25T19:11:11+00:00",
    "summary": "Existing tracheal tumor resection methods often lack the precision required for effective airway clearance, and robotic advancements offer new potential for autonomous resection. We present a vision-guided, autonomous approach for palliative resection of tracheal tumors. This system models the tracheal surface with a fifth-degree polynomial to plan tool trajectories, while a custom Faster R-CNN segmentation pipeline identifies the trachea and tumor boundaries. The electrocautery tool angle is optimized using handheld surgical demonstrations, and trajectories are planned to maintain a 1 mm safety clearance from the tracheal surface. We validated the workflow successfully in five consecutive experiments on ex-vivo animal tissue models, successfully clearing the airway obstruction without trachea perforation in all cases (with more than 90% volumetric tumor removal). These results support the feasibility of an autonomous resection platform, paving the way for future developments in minimally-invasive autonomous resection."
  },
  {
    "title": "Evaluating the Effectiveness of Small Language Models in Detecting Refactoring Bugs",
    "url": "http://arxiv.org/abs/2502.18454v1",
    "arxiv_id": "2502.18454v1",
    "authors": [
      "Rohit Gheyi",
      "Marcio Ribeiro",
      "Jonhnanthan Oliveira"
    ],
    "published": "2025-02-25T18:52:28+00:00",
    "summary": "Popular IDEs frequently contain bugs in their refactoring implementations. Ensuring that a transformation preserves a program's behavior is a complex task. Traditional detection methods rely on predefined preconditions for each refactoring type, limiting their scalability and adaptability to new transformations. These methods often require extensive static and dynamic analyses, which are computationally expensive, time-consuming, and may still fail to detect certain refactoring bugs. This study evaluates the effectiveness of Small Language Models (SLMs) in detecting two types of refactoring bugs in Java and Python: (i) transformations that introduce errors or behavioral changes (Type I) and (ii) transformations unnecessarily blocked by IDEs despite being valid (Type II). We assess whether Llama 3.2 3B, Mistral 7B, Gemma 2 9B, DeepSeek-R1 14B, Phi-4 14B, o1-mini, and o3-mini-high can accurately detect 100 refactoring bugs reported in widely used Java and Python IDEs, such as Eclipse and NetBeans. The study covers 16 refactoring types and employs zero-shot prompting on consumer-grade hardware to evaluate the models' ability to reason about refactoring correctness without explicit prior training. The proprietary o3-mini-high model achieved the highest detection rate, identifying 84.3% of Type I bugs. The open-source Phi-4 14B performed comparably well, demonstrating strong effectiveness across both bug types. However, o3-mini-high struggled with Type II bugs, correctly identifying and applying valid but blocked transformations in only 40% of cases. The findings highlight the potential of SLMs for efficiently detecting refactoring bugs, particularly in verifying behavioral changes. Additionally, SLMs offer a more adaptable solution capable of generalizing across different refactoring types and programming languages, addressing key limitations of traditional approaches."
  },
  {
    "title": "Application of Attention Mechanism with Bidirectional Long Short-Term Memory (BiLSTM) and CNN for Human Conflict Detection using Computer Vision",
    "url": "http://arxiv.org/abs/2502.18555v1",
    "arxiv_id": "2502.18555v1",
    "authors": [
      "Erick da Silva Farias",
      "Eduardo Palhares Junior"
    ],
    "published": "2025-02-25T18:48:34+00:00",
    "summary": "The automatic detection of human conflicts through videos is a crucial area in computer vision, with significant applications in monitoring and public safety policies. However, the scarcity of public datasets and the complexity of human interactions make this task challenging. This study investigates the integration of advanced deep learning techniques, including Attention Mechanism, Convolutional Neural Networks (CNNs), and Bidirectional Long ShortTerm Memory (BiLSTM), to improve the detection of violent behaviors in videos. The research explores how the use of the attention mechanism can help focus on the most relevant parts of the video, enhancing the accuracy and robustness of the model. The experiments indicate that the combination of CNNs with BiLSTM and the attention mechanism provides a promising solution for conflict monitoring, offering insights into the effectiveness of different strategies. This work opens new possibilities for the development of automated surveillance systems that can operate more efficiently in real-time detection of violent events."
  },
  {
    "title": "SWE-RL: Advancing LLM Reasoning via Reinforcement Learning on Open Software Evolution",
    "url": "http://arxiv.org/abs/2502.18449v1",
    "arxiv_id": "2502.18449v1",
    "authors": [
      "Yuxiang Wei",
      "Olivier Duchenne",
      "Jade Copet",
      "Quentin Carbonneaux",
      "Lingming Zhang",
      "Daniel Fried",
      "Gabriel Synnaeve",
      "Rishabh Singh",
      "Sida I. Wang"
    ],
    "published": "2025-02-25T18:45:04+00:00",
    "summary": "The recent DeepSeek-R1 release has demonstrated the immense potential of reinforcement learning (RL) in enhancing the general reasoning capabilities of large language models (LLMs). While DeepSeek-R1 and other follow-up work primarily focus on applying RL to competitive coding and math problems, this paper introduces SWE-RL, the first approach to scale RL-based LLM reasoning for real-world software engineering. Leveraging a lightweight rule-based reward (e.g., the similarity score between ground-truth and LLM-generated solutions), SWE-RL enables LLMs to autonomously recover a developer's reasoning processes and solutions by learning from extensive open-source software evolution data -- the record of a software's entire lifecycle, including its code snapshots, code changes, and events such as issues and pull requests. Trained on top of Llama 3, our resulting reasoning model, Llama3-SWE-RL-70B, achieves a 41.0% solve rate on SWE-bench Verified -- a human-verified collection of real-world GitHub issues. To our knowledge, this is the best performance reported for medium-sized (<100B) LLMs to date, even comparable to leading proprietary LLMs like GPT-4o. Surprisingly, despite performing RL solely on software evolution data, Llama3-SWE-RL has even emerged with generalized reasoning skills. For example, it shows improved results on five out-of-domain tasks, namely, function coding, library use, code reasoning, mathematics, and general language understanding, whereas a supervised-finetuning baseline even leads to performance degradation on average. Overall, SWE-RL opens up a new direction to improve the reasoning capabilities of LLMs through reinforcement learning on massive software engineering data."
  },
  {
    "title": "Rank1: Test-Time Compute for Reranking in Information Retrieval",
    "url": "http://arxiv.org/abs/2502.18418v1",
    "arxiv_id": "2502.18418v1",
    "authors": [
      "Orion Weller",
      "Kathryn Ricci",
      "Eugene Yang",
      "Andrew Yates",
      "Dawn Lawrie",
      "Benjamin Van Durme"
    ],
    "published": "2025-02-25T18:14:06+00:00",
    "summary": "We introduce Rank1, the first reranking model trained to take advantage of test-time compute. Rank1 demonstrates the applicability within retrieval of using a reasoning language model (i.e. OpenAI's o1, Deepseek's R1, etc.) for distillation in order to rapidly improve the performance of a smaller model. We gather and open-source a dataset of more than 600,000 examples of R1 reasoning traces from queries and passages in MS MARCO. Models trained on this dataset show: (1) state-of-the-art performance on advanced reasoning and instruction following datasets; (2) work remarkably well out of distribution due to the ability to respond to user-input prompts; and (3) have explainable reasoning chains that can be given to users or RAG-based systems. Further, we demonstrate that quantized versions of these models retain strong performance while using less compute/memory. Overall, Rank1 shows that test-time compute allows for a fundamentally new type of explainable and performant reranker model for search."
  },
  {
    "title": "What is the Alignment Objective of GRPO?",
    "url": "http://arxiv.org/abs/2502.18548v1",
    "arxiv_id": "2502.18548v1",
    "authors": [
      "Milan Vojnovic",
      "Se-Young Yun"
    ],
    "published": "2025-02-25T15:56:56+00:00",
    "summary": "In this note, we examine the aggregation of preferences achieved by the Group Policy Optimisation (GRPO) algorithm, a reinforcement learning method used to train advanced artificial intelligence models such as DeepSeek-R1-Zero and DeepSeekMath. The GRPO algorithm trains a policy using a reward preference model, which is computed by sampling a set of outputs for a given context, observing the corresponding rewards, and applying shift-and-scale normalisation to these reward values. Additionally, it incorporates a penalty function to discourage deviations from a reference policy.   We present a framework that enables us to characterise the stationary policies of the GRPO algorithm. This analysis reveals that the aggregation of preferences differs fundamentally from standard logarithmic pooling, which is implemented by other approaches such as RLHF. The precise form of preference aggregation arises from the way the reward preference model is defined and from the penalty function, which we show to essentially correspond to the reverse Kullback-Leibler (KL) divergence between the aggregation policy and the reference policy.   Interestingly, we demonstrate that for groups of size two, the reward preference model corresponds to pairwise comparison preferences, similar to those in other alignment methods based on pairwise comparison feedback. We provide explicit characterisations of the aggregate preference for binary questions, for groups of size two, and in the limit of large group size. This provides insights into the dependence of the aggregate preference on parameters such as the regularisation constant and the confidence margin of question answers.   Finally, we discuss the aggregation of preferences obtained by modifying the GRPO algorithm to use direct KL divergence as the penalty or to use rewards without scale normalisation."
  },
  {
    "title": "What is the Alignment Objective of GRPO?",
    "url": "http://arxiv.org/abs/2502.18548v2",
    "arxiv_id": "2502.18548v2",
    "authors": [
      "Milan Vojnovic",
      "Se-Young Yun"
    ],
    "published": "2025-02-25T15:56:56+00:00",
    "summary": "In this note, we examine the aggregation of preferences achieved by the Group Policy Optimisation (GRPO) algorithm, a reinforcement learning method used to train advanced artificial intelligence models such as DeepSeek-R1-Zero and DeepSeekMath. The GRPO algorithm trains a policy using a reward preference model, which is computed by sampling a set of outputs for a given context, observing the corresponding rewards, and applying shift-and-scale normalisation to these reward values. Additionally, it incorporates a penalty function to discourage deviations from a reference policy.   We present a framework that enables us to characterise the stationary policies of the GRPO algorithm. This analysis reveals that the aggregation of preferences differs fundamentally from standard logarithmic pooling, which is implemented by other approaches such as RLHF. The precise form of preference aggregation arises from the way the reward preference model is defined and from the penalty function, which we show to essentially correspond to the reverse Kullback-Leibler (KL) divergence between the aggregation policy and the reference policy.   Interestingly, we demonstrate that for groups of size two, the reward preference model corresponds to pairwise comparison preferences, similar to those in other alignment methods based on pairwise comparison feedback. We provide explicit characterisations of the aggregate preference for binary questions, for groups of size two, and in the limit of large group size. This provides insights into the dependence of the aggregate preference on parameters such as the regularisation constant and the confidence margin of question answers.   Finally, we discuss the aggregation of preferences obtained by modifying the GRPO algorithm to use direct KL divergence as the penalty or to use rewards without scale normalisation."
  },
  {
    "title": "You Shall Not Pass: Warning Drivers of Unsafe Overtaking Maneuvers on Country Roads by Predicting Safe Sight Distance",
    "url": "http://arxiv.org/abs/2502.18163v1",
    "arxiv_id": "2502.18163v1",
    "authors": [
      "Adrian Bauske",
      "Arthur Fleig"
    ],
    "published": "2025-02-25T12:49:05+00:00",
    "summary": "Overtaking on country roads with possible opposing traffic is a dangerous maneuver and many proposed assistant systems assume car-to-car communication and sensors currently unavailable in cars. To overcome this limitation, we develop an assistant that uses simple in-car sensors to predict the required sight distance for safe overtaking. Our models predict this from vehicle speeds, accelerations, and 3D map data. In a user study with a Virtual Reality driving simulator (N=25), we compare two UI variants (monitoring-focused vs scheduling-focused). The results reveal that both UIs enable more patient driving and thus increase overall driving safety. While the monitoring-focused UI achieves higher System Usability Score and distracts drivers less, the preferred UI depends on personal preference. Driving data shows predictions were off at times. We investigate and discuss this in a comparison of our models to actual driving behavior and identify crucial model parameters and assumptions that significantly improve model predictions."
  },
  {
    "title": "Towards Thinking-Optimal Scaling of Test-Time Compute for LLM Reasoning",
    "url": "http://arxiv.org/abs/2502.18080v1",
    "arxiv_id": "2502.18080v1",
    "authors": [
      "Wenkai Yang",
      "Shuming Ma",
      "Yankai Lin",
      "Furu Wei"
    ],
    "published": "2025-02-25T10:48:05+00:00",
    "summary": "Recent studies have shown that making a model spend more time thinking through longer Chain of Thoughts (CoTs) enables it to gain significant improvements in complex reasoning tasks. While current researches continue to explore the benefits of increasing test-time compute by extending the CoT lengths of Large Language Models (LLMs), we are concerned about a potential issue hidden behind the current pursuit of test-time scaling: Would excessively scaling the CoT length actually bring adverse effects to a model's reasoning performance? Our explorations on mathematical reasoning tasks reveal an unexpected finding that scaling with longer CoTs can indeed impair the reasoning performance of LLMs in certain domains. Moreover, we discover that there exists an optimal scaled length distribution that differs across different domains. Based on these insights, we propose a Thinking-Optimal Scaling strategy. Our method first uses a small set of seed data with varying response length distributions to teach the model to adopt different reasoning efforts for deep thinking. Then, the model selects its shortest correct response under different reasoning efforts on additional problems for self-improvement. Our self-improved models built upon Qwen2.5-32B-Instruct outperform other distillation-based 32B o1-like models across various math benchmarks, and achieve performance on par with QwQ-32B-Preview."
  },
  {
    "title": "Exploring the Effects of Traditional Chinese Medicine Scents on Mitigating Driving Fatigue",
    "url": "http://arxiv.org/abs/2502.18013v1",
    "arxiv_id": "2502.18013v1",
    "authors": [
      "Nengyue Su",
      "Liang Luo",
      "Yu Gu",
      "Fuji Ren"
    ],
    "published": "2025-02-25T09:20:45+00:00",
    "summary": "The rise of autonomous driving technology has led to concerns about inactivity-induced fatigue. This paper explores Traditional Chinese Medicine (TCM) scents for mitigating. Two human-involved studies have been conducted in a high-fidelity driving simulator. Study 1 maps six prevalent TCM scents onto the arousal/valence circumplex to select proper candidates, i.e., argy wormwood (with the highest arousal) and tangerine peel (with the highest valence). Study 2 tests both scents in an auto-driving course. Statistics show both scents can improve driver alertness and reaction-time, but should be used in different ways: argy wormwood is suitable for short-term use due to its higher intensity but poor acceptance, while tangerine peel is ideal for long-term use due to its higher likeness. These findings provide insights for in-car fatigue mitigation to enhance driver safety and well-being. However, issues such as scent longevity as for aromatherapy and automatic fatigue prediction remain unresolved."
  },
  {
    "title": "InVDriver: Intra-Instance Aware Vectorized Query-Based Autonomous Driving Transformer",
    "url": "http://arxiv.org/abs/2502.17949v1",
    "arxiv_id": "2502.17949v1",
    "authors": [
      "Bo Zhang",
      "Heye Huang",
      "Chunyang Liu",
      "Yaqin Zhang",
      "Zhenhua Xu"
    ],
    "published": "2025-02-25T08:20:16+00:00",
    "summary": "End-to-end autonomous driving with its holistic optimization capabilities, has gained increasing traction in academia and industry. Vectorized representations, which preserve instance-level topological information while reducing computational overhead, have emerged as a promising paradigm. While existing vectorized query-based frameworks often overlook the inherent spatial correlations among intra-instance points, resulting in geometrically inconsistent outputs (e.g., fragmented HD map elements or oscillatory trajectories). To address these limitations, we propose InVDriver, a novel vectorized query-based system that systematically models intra-instance spatial dependencies through masked self-attention layers, thereby enhancing planning accuracy and trajectory smoothness. Across all core modules, i.e., perception, prediction, and planning, InVDriver incorporates masked self-attention mechanisms that restrict attention to intra-instance point interactions, enabling coordinated refinement of structural elements while suppressing irrelevant inter-instance noise. Experimental results on the nuScenes benchmark demonstrate that InVDriver achieves state-of-the-art performance, surpassing prior methods in both accuracy and safety, while maintaining high computational efficiency. Our work validates that explicit modeling of intra-instance geometric coherence is critical for advancing vectorized autonomous driving systems, bridging the gap between theoretical advantages of end-to-end frameworks and practical deployment requirements."
  },
  {
    "title": "DeepSeek-R1 Outperforms Gemini 2.0 Pro, OpenAI o1, and o3-mini in Bilingual Complex Ophthalmology Reasoning",
    "url": "http://arxiv.org/abs/2502.17947v1",
    "arxiv_id": "2502.17947v1",
    "authors": [
      "Pusheng Xu",
      "Yue Wu",
      "Kai Jin",
      "Xiaolan Chen",
      "Mingguang He",
      "Danli Shi"
    ],
    "published": "2025-02-25T08:08:53+00:00",
    "summary": "Purpose: To evaluate the accuracy and reasoning ability of DeepSeek-R1 and three other recently released large language models (LLMs) in bilingual complex ophthalmology cases. Methods: A total of 130 multiple-choice questions (MCQs) related to diagnosis (n = 39) and management (n = 91) were collected from the Chinese ophthalmology senior professional title examination and categorized into six topics. These MCQs were translated into English using DeepSeek-R1. The responses of DeepSeek-R1, Gemini 2.0 Pro, OpenAI o1 and o3-mini were generated under default configurations between February 15 and February 20, 2025. Accuracy was calculated as the proportion of correctly answered questions, with omissions and extra answers considered incorrect. Reasoning ability was evaluated through analyzing reasoning logic and the causes of reasoning error. Results: DeepSeek-R1 demonstrated the highest overall accuracy, achieving 0.862 in Chinese MCQs and 0.808 in English MCQs. Gemini 2.0 Pro, OpenAI o1, and OpenAI o3-mini attained accuracies of 0.715, 0.685, and 0.692 in Chinese MCQs (all P<0.001 compared with DeepSeek-R1), and 0.746 (P=0.115), 0.723 (P=0.027), and 0.577 (P<0.001) in English MCQs, respectively. DeepSeek-R1 achieved the highest accuracy across five topics in both Chinese and English MCQs. It also excelled in management questions conducted in Chinese (all P<0.05). Reasoning ability analysis showed that the four LLMs shared similar reasoning logic. Ignoring key positive history, ignoring key positive signs, misinterpretation medical data, and too aggressive were the most common causes of reasoning errors. Conclusion: DeepSeek-R1 demonstrated superior performance in bilingual complex ophthalmology reasoning tasks than three other state-of-the-art LLMs. While its clinical applicability remains challenging, it shows promise for supporting diagnosis and clinical decision-making."
  },
  {
    "title": "FetchBot: Object Fetching in Cluttered Shelves via Zero-Shot Sim2Real",
    "url": "http://arxiv.org/abs/2502.17894v1",
    "arxiv_id": "2502.17894v1",
    "authors": [
      "Weiheng Liu",
      "Yuxuan Wan",
      "Jilong Wang",
      "Yuxuan Kuang",
      "Xuesong Shi",
      "Haoran Li",
      "Dongbin Zhao",
      "Zhizheng Zhang",
      "He Wang"
    ],
    "published": "2025-02-25T06:32:42+00:00",
    "summary": "Object fetching from cluttered shelves is an important capability for robots to assist humans in real-world scenarios. Achieving this task demands robotic behaviors that prioritize safety by minimizing disturbances to surrounding objects, an essential but highly challenging requirement due to restricted motion space, limited fields of view, and complex object dynamics. In this paper, we introduce FetchBot, a sim-to-real framework designed to enable zero-shot generalizable and safety-aware object fetching from cluttered shelves in real-world settings. To address data scarcity, we propose an efficient voxel-based method for generating diverse simulated cluttered shelf scenes at scale and train a dynamics-aware reinforcement learning (RL) policy to generate object fetching trajectories within these scenes. This RL policy, which leverages oracle information, is subsequently distilled into a vision-based policy for real-world deployment. Considering that sim-to-real discrepancies stem from texture variations mostly while from geometric dimensions rarely, we propose to adopt depth information estimated by full-fledged depth foundation models as the input for the vision-based policy to mitigate sim-to-real gap. To tackle the challenge of limited views, we design a novel architecture for learning multi-view representations, allowing for comprehensive encoding of cluttered shelf scenes. This enables FetchBot to effectively minimize collisions while fetching objects from varying positions and depths, ensuring robust and safety-aware operation. Both simulation and real-robot experiments demonstrate FetchBot's superior generalization ability, particularly in handling a broad range of real-world scenarios, includ"
  },
  {
    "title": "LR${}^{2}$Bench: Evaluating Long-chain Reflective Reasoning Capabilities of Large Language Models via Constraint Satisfaction Problems",
    "url": "http://arxiv.org/abs/2502.17848v1",
    "arxiv_id": "2502.17848v1",
    "authors": [
      "Jianghao Chen",
      "Zhenlin Wei",
      "Zhenjiang Ren",
      "Ziyong Li",
      "Jiajun Zhang"
    ],
    "published": "2025-02-25T04:51:17+00:00",
    "summary": "Recent progress in o1-like models has significantly enhanced the reasoning abilities of Large Language Models (LLMs), empowering them to tackle increasingly complex tasks through reflection capabilities, such as making assumptions, backtracking, and self-refinement. However, effectively evaluating such reflection capabilities remains challenging due to the lack of appropriate benchmarks. To bridge this gap, we introduce LR${}^{2}$Bench, a novel benchmark designed to evaluate the Long-chain Reflective Reasoning capabilities of LLMs. LR${}^{2}$Bench comprises 850 samples across six Constraint Satisfaction Problems (CSPs) where reflective reasoning is crucial for deriving solutions that meet all given constraints. Each type of task focuses on distinct constraint patterns, such as knowledge-based, logical, and spatial constraints, providing a comprehensive evaluation of diverse problem-solving scenarios. We conduct extensive evaluation on both conventional models and o1-like models. Our experimental results reveal that even the most advanced reasoning-specific models, such as DeepSeek-R1 and OpenAI o1-preview, struggle with tasks in LR${}^{2}$Bench, achieving an average Exact Match score of only 20.0% and 23.6%, respectively. These findings underscore the significant room for improvement in the reflective reasoning capabilities of current LLMs. The leaderboard of our benchmark is available at https://huggingface.co/spaces/UltraRonin/LR2Bench"
  },
  {
    "title": "Impact of Object Weight in Handovers: Inspiring Robotic Grip Release and Motion from Human Handovers",
    "url": "http://arxiv.org/abs/2502.17834v1",
    "arxiv_id": "2502.17834v1",
    "authors": [
      "Parag Khanna",
      "M\u00e5rten Bj\u00f6rkman",
      "Christian Smith"
    ],
    "published": "2025-02-25T04:29:11+00:00",
    "summary": "This work explores the effect of object weight on human motion and grip release during handovers to enhance the naturalness, safety, and efficiency of robot-human interactions. We introduce adaptive robotic strategies based on the analysis of human handover behavior with varying object weights. The key contributions of this work includes the development of an adaptive grip-release strategy for robots, a detailed analysis of how object weight influences human motion to guide robotic motion adaptations, and the creation of handover-datasets incorporating various object weights, including the YCB handover dataset. By aligning robotic grip release and motion with human behavior, this work aims to improve robot-human handovers for different weighted objects. We also evaluate these human-inspired adaptive robotic strategies in robot-to-human handovers to assess their effectiveness and performance and demonstrate that they outperform the baseline approaches in terms of naturalness, efficiency, and user perception."
  },
  {
    "title": "MM-PoisonRAG: Disrupting Multimodal RAG with Local and Global Poisoning Attacks",
    "url": "http://arxiv.org/abs/2502.17832v1",
    "arxiv_id": "2502.17832v1",
    "authors": [
      "Hyeonjeong Ha",
      "Qiusi Zhan",
      "Jeonghwan Kim",
      "Dimitrios Bralios",
      "Saikrishna Sanniboina",
      "Nanyun Peng",
      "Kai-wei Chang",
      "Daniel Kang",
      "Heng Ji"
    ],
    "published": "2025-02-25T04:23:59+00:00",
    "summary": "Multimodal large language models (MLLMs) equipped with Retrieval Augmented Generation (RAG) leverage both their rich parametric knowledge and the dynamic, external knowledge to excel in tasks such as Question Answering. While RAG enhances MLLMs by grounding responses in query-relevant external knowledge, this reliance poses a critical yet underexplored safety risk: knowledge poisoning attacks, where misinformation or irrelevant knowledge is intentionally injected into external knowledge bases to manipulate model outputs to be incorrect and even harmful. To expose such vulnerabilities in multimodal RAG, we propose MM-PoisonRAG, a novel knowledge poisoning attack framework with two attack strategies: Localized Poisoning Attack (LPA), which injects query-specific misinformation in both text and images for targeted manipulation, and Globalized Poisoning Attack (GPA) to provide false guidance during MLLM generation to elicit nonsensical responses across all queries. We evaluate our attacks across multiple tasks, models, and access settings, demonstrating that LPA successfully manipulates the MLLM to generate attacker-controlled answers, with a success rate of up to 56% on MultiModalQA. Moreover, GPA completely disrupts model generation to 0% accuracy with just a single irrelevant knowledge injection. Our results highlight the urgent need for robust defenses against knowledge poisoning to safeguard multimodal RAG frameworks."
  },
  {
    "title": "Safe Multi-Agent Navigation guided by Goal-Conditioned Safe Reinforcement Learning",
    "url": "http://arxiv.org/abs/2502.17813v1",
    "arxiv_id": "2502.17813v1",
    "authors": [
      "Meng Feng",
      "Viraj Parimi",
      "Brian Williams"
    ],
    "published": "2025-02-25T03:38:52+00:00",
    "summary": "Safe navigation is essential for autonomous systems operating in hazardous environments. Traditional planning methods excel at long-horizon tasks but rely on a predefined graph with fixed distance metrics. In contrast, safe Reinforcement Learning (RL) can learn complex behaviors without relying on manual heuristics but fails to solve long-horizon tasks, particularly in goal-conditioned and multi-agent scenarios.   In this paper, we introduce a novel method that integrates the strengths of both planning and safe RL. Our method leverages goal-conditioned RL and safe RL to learn a goal-conditioned policy for navigation while concurrently estimating cumulative distance and safety levels using learned value functions via an automated self-training algorithm. By constructing a graph with states from the replay buffer, our method prunes unsafe edges and generates a waypoint-based plan that the agent follows until reaching its goal, effectively balancing faster and safer routes over extended distances.   Utilizing this unified high-level graph and a shared low-level goal-conditioned safe RL policy, we extend this approach to address the multi-agent safe navigation problem. In particular, we leverage Conflict-Based Search (CBS) to create waypoint-based plans for multiple agents allowing for their safe navigation over extended horizons. This integration enhances the scalability of goal-conditioned safe RL in multi-agent scenarios, enabling efficient coordination among agents.   Extensive benchmarking against state-of-the-art baselines demonstrates the effectiveness of our method in achieving distance goals safely for multiple agents in complex and hazardous environments. Our code will be released to support future research."
  },
  {
    "title": "DocPuzzle: A Process-Aware Benchmark for Evaluating Realistic Long-Context Reasoning Capabilities",
    "url": "http://arxiv.org/abs/2502.17807v1",
    "arxiv_id": "2502.17807v1",
    "authors": [
      "Tianyi Zhuang",
      "Chuqiao Kuang",
      "Xiaoguang Li",
      "Yihua Teng",
      "Jihao Wu",
      "Yasheng Wang",
      "Lifeng Shang"
    ],
    "published": "2025-02-25T03:29:53+00:00",
    "summary": "We present DocPuzzle, a rigorously constructed benchmark for evaluating long-context reasoning capabilities in large language models (LLMs). This benchmark comprises 100 expert-level QA problems requiring multi-step reasoning over long real-world documents. To ensure the task quality and complexity, we implement a human-AI collaborative annotation-validation pipeline. DocPuzzle introduces an innovative evaluation framework that mitigates guessing bias through checklist-guided process analysis, establishing new standards for assessing reasoning capacities in LLMs. Our evaluation results show that: 1)Advanced slow-thinking reasoning models like o1-preview(69.7%) and DeepSeek-R1(66.3%) significantly outperform best general instruct models like Claude 3.5 Sonnet(57.7%); 2)Distilled reasoning models like DeepSeek-R1-Distill-Qwen-32B(41.3%) falls far behind the teacher model, suggesting challenges to maintain the generalization of reasoning capabilities relying solely on distillation."
  },
  {
    "title": "Exploring the Potential of Large Language Models for Estimating the Reading Comprehension Question Difficulty",
    "url": "http://arxiv.org/abs/2502.17785v1",
    "arxiv_id": "2502.17785v1",
    "authors": [
      "Yoshee Jain",
      "John Hollander",
      "Amber He",
      "Sunny Tang",
      "Liang Zhang",
      "John Sabatini"
    ],
    "published": "2025-02-25T02:28:48+00:00",
    "summary": "Reading comprehension is a key for individual success, yet the assessment of question difficulty remains challenging due to the extensive human annotation and large-scale testing required by traditional methods such as linguistic analysis and Item Response Theory (IRT). While these robust approaches provide valuable insights, their scalability is limited. There is potential for Large Language Models (LLMs) to automate question difficulty estimation; however, this area remains underexplored. Our study investigates the effectiveness of LLMs, specifically OpenAI's GPT-4o and o1, in estimating the difficulty of reading comprehension questions using the Study Aid and Reading Assessment (SARA) dataset. We evaluated both the accuracy of the models in answering comprehension questions and their ability to classify difficulty levels as defined by IRT. The results indicate that, while the models yield difficulty estimates that align meaningfully with derived IRT parameters, there are notable differences in their sensitivity to extreme item characteristics. These findings suggest that LLMs can serve as the scalable method for automated difficulty assessment, particularly in dynamic interactions between learners and Adaptive Instructional Systems (AIS), bridging the gap between traditional psychometric techniques and modern AIS for reading comprehension and paving the way for more adaptive and personalized educational assessments."
  },
  {
    "title": "GPUArmor: A Hardware-Software Co-design for Efficient and Scalable Memory Safety on GPUs",
    "url": "http://arxiv.org/abs/2502.17780v1",
    "arxiv_id": "2502.17780v1",
    "authors": [
      "Mohamed Tarek Ibn Ziad",
      "Sana Damani",
      "Mark Stephenson",
      "Stephen W. Keckler",
      "Aamer Jaleel"
    ],
    "published": "2025-02-25T02:15:06+00:00",
    "summary": "Memory safety errors continue to pose a significant threat to current computing systems, and graphics processing units (GPUs) are no exception. A prominent class of memory safety algorithms is allocation-based solutions. The key idea is to maintain each allocation's metadata (base address and size) in a disjoint table and retrieve it at runtime to verify memory accesses. While several previous solutions have adopted allocation-based algorithms (e.g., cuCatch and GPUShield), they typically suffer from high memory overheads or scalability problems. In this work, we examine the key characteristics of real-world GPU workloads and observe several differences between GPU and CPU applications regarding memory access patterns, memory footprint, number of live allocations, and active allocation working set. Our observations motivate GPUArmor, a hardware-software co-design framework for memory safety on GPUs. We show that a simple compiler analysis combined with lightweight hardware support using a small Memory Lookaside Buffer (MLB) can help prevent spatial and temporal memory violations on modern GPU workloads with 2.3% average run time overheads. More importantly, GPUArmor achieves speed-of-light performance with negligible storage requirements. This result benefits both base and bounds solutions and memory tagging techniques, which we showcase with GPUArmor-HWOnly, a variation of GPUArmor that does not require recompilation, and achieves 2.2% slowdowns while significantly reducing storage overheads beyond traditional memory tagging approaches."
  },
  {
    "title": "GPUArmor: A Hardware-Software Co-design for Efficient and Scalable Memory Safety on GPUs",
    "url": "http://arxiv.org/abs/2502.17780v2",
    "arxiv_id": "2502.17780v2",
    "authors": [
      "Mohamed Tarek Ibn Ziad",
      "Sana Damani",
      "Mark Stephenson",
      "Stephen W. Keckler",
      "Aamer Jaleel"
    ],
    "published": "2025-02-25T02:15:06+00:00",
    "summary": "Memory safety errors continue to pose a significant threat to current computing systems, and graphics processing units (GPUs) are no exception. A prominent class of memory safety algorithms is allocation-based solutions. The key idea is to maintain each allocation's metadata (base address and size) in a disjoint table and retrieve it at runtime to verify memory accesses. While several previous solutions have adopted allocation-based algorithms (e.g., cuCatch and GPUShield), they typically suffer from high memory overheads or scalability problems. In this work, we examine the key characteristics of real-world GPU workloads and observe several differences between GPU and CPU applications regarding memory access patterns, memory footprint, number of live allocations, and active allocation working set. Our observations motivate GPUArmor, a hardware-software co-design framework for memory safety on GPUs. We show that a simple compiler analysis combined with lightweight hardware support using a small Memory Lookaside Buffer (MLB) can help prevent spatial and temporal memory violations on modern GPU workloads with 2.3% average run time overheads. More importantly, GPUArmor achieves speed-of-light performance with negligible storage requirements. This result benefits both base and bounds solutions and memory tagging techniques, which we showcase with GPUArmor-HWOnly, a variation of GPUArmor that does not require recompilation, and achieves 2.2% slowdowns while significantly reducing storage overheads beyond traditional memory tagging approaches."
  },
  {
    "title": "Design of a Breakaway Utensil Attachment for Enhanced Safety in Robot-Assisted Feeding",
    "url": "http://arxiv.org/abs/2502.17774v1",
    "arxiv_id": "2502.17774v1",
    "authors": [
      "Hau Wen Chang",
      "J-Anne Yow",
      "Lek Syn Lim",
      "Wei Tech Ang"
    ],
    "published": "2025-02-25T02:09:32+00:00",
    "summary": "Robot-assisted feeding systems enhance the independence of individuals with motor impairments and alleviate caregiver burden. While existing systems predominantly rely on software-based safety features to mitigate risks during unforeseen collisions, this study explores the use of a mechanical fail-safe to improve safety. We designed a breakaway utensil attachment that decouples forces exerted by the robot on the user when excessive forces occur. Finite element analysis (FEA) simulations were performed to predict failure points under various loading conditions, followed by experimental validation using 3D-printed attachments with variations in slot depth and wall loops. To facilitate testing, a drop test rig was developed and validated. Our results demonstrated a consistent failure point at the slot of the attachment, with a slot depth of 1 mm and three wall loops achieving failure at the target force of 65 N. Additionally, the parameters can be tailored to customize the breakaway force based on user-specific factors, such as comfort and pain tolerance. CAD files and utensil assembly instructions can be found here: https://tinyurl.com/rfa-utensil-attachment"
  },
  {
    "title": "DeepSeek vs. ChatGPT: A Comparative Study for Scientific Computing and Scientific Machine Learning Tasks",
    "url": "http://arxiv.org/abs/2502.17764v1",
    "arxiv_id": "2502.17764v1",
    "authors": [
      "Qile Jiang",
      "Zhiwei Gao",
      "George Em Karniadakis"
    ],
    "published": "2025-02-25T01:49:50+00:00",
    "summary": "Large Language Models (LLMs) have emerged as powerful tools for tackling a wide range of problems, including those in scientific computing, particularly in solving partial differential equations (PDEs). However, different models exhibit distinct strengths and preferences, resulting in varying levels of performance. In this paper, we compare the capabilities of the most advanced LLMs--ChatGPT and DeepSeek--along with their reasoning-optimized versions in addressing computational challenges. Specifically, we evaluate their proficiency in solving traditional numerical problems in scientific computing as well as leveraging scientific machine learning techniques for PDE-based problems. We designed all our experiments so that a non-trivial decision is required, e.g. defining the proper space of input functions for neural operator learning. Our findings reveal that the latest model, ChatGPT o3-mini-high, usually delivers the most accurate results while also responding significantly faster than its reasoning counterpart, DeepSeek R1. This enhanced speed and accuracy make ChatGPT o3-mini-high a more practical and efficient choice for diverse computational tasks at this juncture."
  },
  {
    "title": "Heterogeneous Decision Making in Mixed Traffic: Uncertainty-aware Planning and Bounded Rationality",
    "url": "http://arxiv.org/abs/2502.18529v1",
    "arxiv_id": "2502.18529v1",
    "authors": [
      "Hang Wang",
      "Qiaoyi Fang",
      "Junshan Zhang"
    ],
    "published": "2025-02-25T00:32:33+00:00",
    "summary": "The past few years have witnessed a rapid growth of the deployment of automated vehicles (AVs). Clearly, AVs and human-driven vehicles (HVs) will co-exist for many years, and AVs will have to operate around HVs, pedestrians, cyclists, and more, calling for fundamental breakthroughs in AI designed for mixed traffic to achieve mixed autonomy. Thus motivated, we study heterogeneous decision making by AVs and HVs in a mixed traffic environment, aiming to capture the interactions between human and machine decision-making and develop an AI foundation that enables vehicles to operate safely and efficiently. There are a number of challenges to achieve mixed autonomy, including 1) humans drivers make driving decisions with bounded rationality, and it remains open to develop accurate models for HVs' decision making; and 2) uncertainty-aware planning plays a critical role for AVs to take safety maneuvers in response to the human behavior. In this paper, we introduce a formulation of AV-HV interaction, where the HV makes decisions with bounded rationality and the AV employs uncertainty-aware planning based on the prediction on HV's future actions. We conduct a comprehensive analysis on AV and HV's learning regret to answer the questions: 1) {How does the learning performance depend on HV's bounded rationality and AV's planning}; 2) {How do different decision making strategies impact the overall learning performance}? Our findings reveal some intriguing phenomena, such as Goodhart's Law in AV's learning performance and compounding effects in HV's decision making process. By examining the dynamics of the regrets, we gain insights into the interplay between human and machine decision making."
  },
  {
    "title": "Mind the Gesture: Evaluating AI Sensitivity to Culturally Offensive Non-Verbal Gestures",
    "url": "http://arxiv.org/abs/2502.17710v1",
    "arxiv_id": "2502.17710v1",
    "authors": [
      "Akhila Yerukola",
      "Saadia Gabriel",
      "Nanyun Peng",
      "Maarten Sap"
    ],
    "published": "2025-02-24T23:10:08+00:00",
    "summary": "Gestures are an integral part of non-verbal communication, with meanings that vary across cultures, and misinterpretations that can have serious social and diplomatic consequences. As AI systems become more integrated into global applications, ensuring they do not inadvertently perpetuate cultural offenses is critical. To this end, we introduce Multi-Cultural Set of Inappropriate Gestures and Nonverbal Signs (MC-SIGNS), a dataset of 288 gesture-country pairs annotated for offensiveness, cultural significance, and contextual factors across 25 gestures and 85 countries. Through systematic evaluation using MC-SIGNS, we uncover critical limitations: text-to-image (T2I) systems exhibit strong US-centric biases, performing better at detecting offensive gestures in US contexts than in non-US ones; large language models (LLMs) tend to over-flag gestures as offensive; and vision-language models (VLMs) default to US-based interpretations when responding to universal concepts like wishing someone luck, frequently suggesting culturally inappropriate gestures. These findings highlight the urgent need for culturally-aware AI safety mechanisms to ensure equitable global deployment of AI technologies."
  },
  {
    "title": "Data-Driven Input-Output Control Barrier Functions",
    "url": "http://arxiv.org/abs/2502.17688v1",
    "arxiv_id": "2502.17688v1",
    "authors": [
      "MMohammad Bajelani",
      "Klaske van Heusden"
    ],
    "published": "2025-02-24T22:16:27+00:00",
    "summary": "Control Barrier Functions (CBFs) offer a framework for ensuring set invariance and designing constrained control laws. However, crafting a valid CBF relies on system-specific assumptions and the availability of an accurate system model, underscoring the need for systematic data-driven synthesis methods. This paper introduces a data-driven approach to synthesizing a CBF for discrete-time LTI systems using only input-output measurements. The method begins by computing the maximal control invariant set using an input-output data-driven representation, eliminating the need for precise knowledge of the system's order and explicit state estimation. The proposed CBF is then systematically derived from this set, which can accommodate multiple input-output constraints. Furthermore, the proposed CBF is leveraged to develop a minimally invasive safety filter that ensures recursive feasibility with an adaptive decay rate. To improve clarity, we assume a noise-free dataset, though data-driven control techniques can be used to robustify the approach. Finally, the effectiveness of the proposed method is demonstrated on an unknown time-delay system."
  },
  {
    "title": "Architecting Digital Twins for Intelligent Transportation Systems",
    "url": "http://arxiv.org/abs/2502.17646v1",
    "arxiv_id": "2502.17646v1",
    "authors": [
      "Hiya Bhatt",
      "Sahil",
      "Karthik Vaidhyanathan",
      "Rahul Biju",
      "Deepak Gangadharan",
      "Ramona Trestian",
      "Purav Shah"
    ],
    "published": "2025-02-24T20:51:09+00:00",
    "summary": "Modern transportation systems face growing challenges in managing traffic flow, ensuring safety, and maintaining operational efficiency amid dynamic traffic patterns. Addressing these challenges requires intelligent solutions capable of real-time monitoring, predictive analytics, and adaptive control. This paper proposes an architecture for DigIT, a Digital Twin (DT) platform for Intelligent Transportation Systems (ITS), designed to overcome the limitations of existing frameworks by offering a modular and scalable solution for traffic management. Built on a Domain Concept Model (DCM), the architecture systematically models key ITS components enabling seamless integration of predictive modeling and simulations. The architecture leverages machine learning models to forecast traffic patterns based on historical and real-time data. To adapt to evolving traffic patterns, the architecture incorporates adaptive Machine Learning Operations (MLOps), automating the deployment and lifecycle management of predictive models. Evaluation results highlight the effectiveness of the architecture in delivering accurate predictions and computational efficiency."
  },
  {
    "title": "Hallucination Detection in LLMs Using Spectral Features of Attention Maps",
    "url": "http://arxiv.org/abs/2502.17598v1",
    "arxiv_id": "2502.17598v1",
    "authors": [
      "Jakub Binkowski",
      "Denis Janiak",
      "Albert Sawczyn",
      "Bogdan Gabrys",
      "Tomasz Kajdanowicz"
    ],
    "published": "2025-02-24T19:30:24+00:00",
    "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across various tasks but remain prone to hallucinations. Detecting hallucinations is essential for safety-critical applications, and recent methods leverage attention map properties to this end, though their effectiveness remains limited. In this work, we investigate the spectral features of attention maps by interpreting them as adjacency matrices of graph structures. We propose the $\\text{LapEigvals}$ method, which utilises the top-$k$ eigenvalues of the Laplacian matrix derived from the attention maps as an input to hallucination detection probes. Empirical evaluations demonstrate that our approach achieves state-of-the-art hallucination detection performance among attention-based methods. Extensive ablation studies further highlight the robustness and generalisation of $\\text{LapEigvals}$, paving the way for future advancements in the hallucination detection domain."
  },
  {
    "title": "How Do Large Language Monkeys Get Their Power (Laws)?",
    "url": "http://arxiv.org/abs/2502.17578v1",
    "arxiv_id": "2502.17578v1",
    "authors": [
      "Rylan Schaeffer",
      "Joshua Kazdan",
      "John Hughes",
      "Jordan Juravsky",
      "Sara Price",
      "Aengus Lynch",
      "Erik Jones",
      "Robert Kirk",
      "Azalia Mirhoseini",
      "Sanmi Koyejo"
    ],
    "published": "2025-02-24T19:01:47+00:00",
    "summary": "Recent research across mathematical problem solving, proof assistant programming and multimodal jailbreaking documents a striking finding: when (multimodal) language model tackle a suite of tasks with multiple attempts per task -- succeeding if any attempt is correct -- then the negative log of the average success rate scales a power law in the number of attempts. In this work, we identify an apparent puzzle: a simple mathematical calculation predicts that on each problem, the failure rate should fall exponentially with the number of attempts. We confirm this prediction empirically, raising a question: from where does aggregate polynomial scaling emerge? We then answer this question by demonstrating per-problem exponential scaling can be made consistent with aggregate polynomial scaling if the distribution of single-attempt success probabilities is heavy tailed such that a small fraction of tasks with extremely low success probabilities collectively warp the aggregate success trend into a power law - even as each problem scales exponentially on its own. We further demonstrate that this distributional perspective explains previously observed deviations from power law scaling, and provides a simple method for forecasting the power law exponent with an order of magnitude lower relative error, or equivalently, ${\\sim}2-4$ orders of magnitude less inference compute. Overall, our work contributes to a better understanding of how neural language model performance improves with scaling inference compute and the development of scaling-predictable evaluations of (multimodal) language models."
  },
  {
    "title": "LongSpec: Long-Context Speculative Decoding with Efficient Drafting and Verification",
    "url": "http://arxiv.org/abs/2502.17421v1",
    "arxiv_id": "2502.17421v1",
    "authors": [
      "Penghui Yang",
      "Cunxiao Du",
      "Fengzhuo Zhang",
      "Haonan Wang",
      "Tianyu Pang",
      "Chao Du",
      "Bo An"
    ],
    "published": "2025-02-24T18:53:31+00:00",
    "summary": "Speculative decoding has become a promising technique to mitigate the high inference latency of autoregressive decoding in Large Language Models (LLMs). Despite its promise, the effective application of speculative decoding in LLMs still confronts three key challenges: the increasing memory demands of the draft model, the distribution shift between the short-training corpora and long-context inference, and inefficiencies in attention implementation. In this work, we enhance the performance of speculative decoding in long-context settings by addressing these challenges. First, we propose a memory-efficient draft model with a constant-sized Key-Value (KV) cache. Second, we introduce novel position indices for short-training data, enabling seamless adaptation from short-context training to long-context inference. Finally, we present an innovative attention aggregation method that combines fast implementations for prefix computation with standard attention for tree mask handling, effectively resolving the latency and memory inefficiencies of tree decoding. Our approach achieves strong results on various long-context tasks, including repository-level code completion, long-context summarization, and o1-like long reasoning tasks, demonstrating significant improvements in latency reduction. The code is available at https://github.com/sail-sg/LongSpec."
  },
  {
    "title": "The Geometry of Refusal in Large Language Models: Concept Cones and Representational Independence",
    "url": "http://arxiv.org/abs/2502.17420v1",
    "arxiv_id": "2502.17420v1",
    "authors": [
      "Tom Wollschl\u00e4ger",
      "Jannes Elstner",
      "Simon Geisler",
      "Vincent Cohen-Addad",
      "Stephan G\u00fcnnemann",
      "Johannes Gasteiger"
    ],
    "published": "2025-02-24T18:52:59+00:00",
    "summary": "The safety alignment of large language models (LLMs) can be circumvented through adversarially crafted inputs, yet the mechanisms by which these attacks bypass safety barriers remain poorly understood. Prior work suggests that a single refusal direction in the model's activation space determines whether an LLM refuses a request. In this study, we propose a novel gradient-based approach to representation engineering and use it to identify refusal directions. Contrary to prior work, we uncover multiple independent directions and even multi-dimensional concept cones that mediate refusal. Moreover, we show that orthogonality alone does not imply independence under intervention, motivating the notion of representational independence that accounts for both linear and non-linear effects. Using this framework, we identify mechanistically independent refusal directions. We show that refusal mechanisms in LLMs are governed by complex spatial structures and identify functionally independent directions, confirming that multiple distinct mechanisms drive refusal behavior. Our gradient-based approach uncovers these mechanisms and can further serve as a foundation for future work on understanding LLMs."
  },
  {
    "title": "From System 1 to System 2: A Survey of Reasoning Large Language Models",
    "url": "http://arxiv.org/abs/2502.17419v1",
    "arxiv_id": "2502.17419v1",
    "authors": [
      "Zhong-Zhi Li",
      "Duzhen Zhang",
      "Ming-Liang Zhang",
      "Jiaxin Zhang",
      "Zengyan Liu",
      "Yuxuan Yao",
      "Haotian Xu",
      "Junhao Zheng",
      "Pei-Jie Wang",
      "Xiuyi Chen",
      "Yingying Zhang",
      "Fei Yin",
      "Jiahua Dong",
      "Zhijiang Guo",
      "Le Song",
      "Cheng-Lin Liu"
    ],
    "published": "2025-02-24T18:50:52+00:00",
    "summary": "Achieving human-level intelligence requires refining the transition from the fast, intuitive System 1 to the slower, more deliberate System 2 reasoning. While System 1 excels in quick, heuristic decisions, System 2 relies on logical reasoning for more accurate judgments and reduced biases. Foundational Large Language Models (LLMs) excel at fast decision-making but lack the depth for complex reasoning, as they have not yet fully embraced the step-by-step analysis characteristic of true System 2 thinking. Recently, reasoning LLMs like OpenAI's o1/o3 and DeepSeek's R1 have demonstrated expert-level performance in fields such as mathematics and coding, closely mimicking the deliberate reasoning of System 2 and showcasing human-like cognitive abilities. This survey begins with a brief overview of the progress in foundational LLMs and the early development of System 2 technologies, exploring how their combination has paved the way for reasoning LLMs. Next, we discuss how to construct reasoning LLMs, analyzing their features, the core methods enabling advanced reasoning, and the evolution of various reasoning LLMs. Additionally, we provide an overview of reasoning benchmarks, offering an in-depth comparison of the performance of representative reasoning LLMs. Finally, we explore promising directions for advancing reasoning LLMs and maintain a real-time \\href{https://github.com/zzli2022/Awesome-Slow-Reason-System}{GitHub Repository} to track the latest developments. We hope this survey will serve as a valuable resource to inspire innovation and drive progress in this rapidly evolving field."
  },
  {
    "title": "From System 1 to System 2: A Survey of Reasoning Large Language Models",
    "url": "http://arxiv.org/abs/2502.17419v2",
    "arxiv_id": "2502.17419v2",
    "authors": [
      "Zhong-Zhi Li",
      "Duzhen Zhang",
      "Ming-Liang Zhang",
      "Jiaxin Zhang",
      "Zengyan Liu",
      "Yuxuan Yao",
      "Haotian Xu",
      "Junhao Zheng",
      "Pei-Jie Wang",
      "Xiuyi Chen",
      "Yingying Zhang",
      "Fei Yin",
      "Jiahua Dong",
      "Zhijiang Guo",
      "Le Song",
      "Cheng-Lin Liu"
    ],
    "published": "2025-02-24T18:50:52+00:00",
    "summary": "Achieving human-level intelligence requires refining the transition from the fast, intuitive System 1 to the slower, more deliberate System 2 reasoning. While System 1 excels in quick, heuristic decisions, System 2 relies on logical reasoning for more accurate judgments and reduced biases. Foundational Large Language Models (LLMs) excel at fast decision-making but lack the depth for complex reasoning, as they have not yet fully embraced the step-by-step analysis characteristic of true System 2 thinking. Recently, reasoning LLMs like OpenAI's o1/o3 and DeepSeek's R1 have demonstrated expert-level performance in fields such as mathematics and coding, closely mimicking the deliberate reasoning of System 2 and showcasing human-like cognitive abilities. This survey begins with a brief overview of the progress in foundational LLMs and the early development of System 2 technologies, exploring how their combination has paved the way for reasoning LLMs. Next, we discuss how to construct reasoning LLMs, analyzing their features, the core methods enabling advanced reasoning, and the evolution of various reasoning LLMs. Additionally, we provide an overview of reasoning benchmarks, offering an in-depth comparison of the performance of representative reasoning LLMs. Finally, we explore promising directions for advancing reasoning LLMs and maintain a real-time \\href{https://github.com/zzli2022/Awesome-Slow-Reason-System}{GitHub Repository} to track the latest developments. We hope this survey will serve as a valuable resource to inspire innovation and drive progress in this rapidly evolving field."
  },
  {
    "title": "Dataset Featurization: Uncovering Natural Language Features through Unsupervised Data Reconstruction",
    "url": "http://arxiv.org/abs/2502.17541v1",
    "arxiv_id": "2502.17541v1",
    "authors": [
      "Michal Bravansky",
      "Vaclav Kubon",
      "Suhas Hariharan",
      "Robert Kirk"
    ],
    "published": "2025-02-24T18:42:33+00:00",
    "summary": "Interpreting data is central to modern research. Large language models (LLMs) show promise in providing such natural language interpretations of data, yet simple feature extraction methods such as prompting often fail to produce accurate and versatile descriptions for diverse datasets and lack control over granularity and scale. To address these limitations, we propose a domain-agnostic method for dataset featurization that provides precise control over the number of features extracted while maintaining compact and descriptive representations comparable to human expert labeling. Our method optimizes the selection of informative binary features by evaluating the ability of an LLM to reconstruct the original data using those features. We demonstrate its effectiveness in dataset modeling tasks and through two case studies: (1) Constructing a feature representation of jailbreak tactics that compactly captures both the effectiveness and diversity of a larger set of human-crafted attacks; and (2) automating the discovery of features that align with human preferences, achieving accuracy and robustness comparable to expert-crafted features. Moreover, we show that the pipeline scales effectively, improving as additional features are sampled, making it suitable for large and diverse datasets."
  },
  {
    "title": "Experimental validation of UAV search and detection system in real wilderness environment",
    "url": "http://arxiv.org/abs/2502.17372v1",
    "arxiv_id": "2502.17372v1",
    "authors": [
      "Stella Dumen\u010di\u0107",
      "Luka Lan\u010da",
      "Karlo Jakac",
      "Stefan Ivi\u0107"
    ],
    "published": "2025-02-24T17:53:54+00:00",
    "summary": "Search and rescue (SAR) missions require reliable search methods to locate survivors, especially in challenging or inaccessible environments. This is why introducing unmanned aerial vehicles (UAVs) can be of great help to enhance the efficiency of SAR missions while simultaneously increasing the safety of everyone involved in the mission. Motivated by this, we design and experiment with autonomous UAV search for humans in a Mediterranean karst environment. The UAVs are directed using Heat equation-driven area coverage (HEDAC) ergodic control method according to known probability density and detection function. The implemented sensing framework consists of a probabilistic search model, motion control system, and computer vision object detection. It enables calculation of the probability of the target being detected in the SAR mission, and this paper focuses on experimental validation of proposed probabilistic framework and UAV control. The uniform probability density to ensure the even probability of finding the targets in the desired search area is achieved by assigning suitably thought-out tasks to 78 volunteers. The detection model is based on YOLO and trained with a previously collected ortho-photo image database. The experimental search is carefully planned and conducted, while as many parameters as possible are recorded. The thorough analysis consists of the motion control system, object detection, and the search validation. The assessment of the detection and search performance provides strong indication that the designed detection model in the UAV control algorithm is aligned with real-world results."
  },
  {
    "title": "Hybrid Human-Machine Perception via Adaptive LiDAR for Advanced Driver Assistance Systems",
    "url": "http://arxiv.org/abs/2502.17309v1",
    "arxiv_id": "2502.17309v1",
    "authors": [
      "Federico Scar\u00ec",
      "Nitin Jonathan Myers",
      "Chen Quan",
      "Arkady Zgonnikov"
    ],
    "published": "2025-02-24T16:44:20+00:00",
    "summary": "Accurate environmental perception is critical for advanced driver assistance systems (ADAS). Light detection and ranging (LiDAR) systems play a crucial role in ADAS; they can reliably detect obstacles and help ensure traffic safety. Existing research on LiDAR sensing has demonstrated that adapting the LiDAR's resolution and range based on environmental characteristics can improve machine perception. However, current adaptive LiDAR approaches for ADAS have not explored the possibility of combining the perception abilities of the vehicle and the human driver, which can potentially further enhance the detection performance. In this paper, we propose a novel system that adapts LiDAR characteristics to human driver's visual perception to enhance LiDAR sensing outside human's field of view. We develop a proof-of-concept prototype of the system in the virtual environment CARLA. Our system integrates real-time data on the driver's gaze to identify regions in the environment that the driver is monitoring. This allows the system to optimize LiDAR resources by dynamically increasing the LiDAR's range and resolution in peripheral areas that the driver may not be attending to. Our simulations show that this gaze-aware LiDAR enhances detection performance compared to a baseline standalone LiDAR, particularly in challenging environmental conditions like fog. Our hybrid human-machine sensing approach potentially offers improved safety and situational awareness in real-time driving scenarios for ADAS applications."
  },
  {
    "title": "REINFORCE Adversarial Attacks on Large Language Models: An Adaptive, Distributional, and Semantic Objective",
    "url": "http://arxiv.org/abs/2502.17254v1",
    "arxiv_id": "2502.17254v1",
    "authors": [
      "Simon Geisler",
      "Tom Wollschl\u00e4ger",
      "M. H. I. Abdalla",
      "Vincent Cohen-Addad",
      "Johannes Gasteiger",
      "Stephan G\u00fcnnemann"
    ],
    "published": "2025-02-24T15:34:48+00:00",
    "summary": "To circumvent the alignment of large language models (LLMs), current optimization-based adversarial attacks usually craft adversarial prompts by maximizing the likelihood of a so-called affirmative response. An affirmative response is a manually designed start of a harmful answer to an inappropriate request. While it is often easy to craft prompts that yield a substantial likelihood for the affirmative response, the attacked model frequently does not complete the response in a harmful manner. Moreover, the affirmative objective is usually not adapted to model-specific preferences and essentially ignores the fact that LLMs output a distribution over responses. If low attack success under such an objective is taken as a measure of robustness, the true robustness might be grossly overestimated. To alleviate these flaws, we propose an adaptive and semantic optimization problem over the population of responses. We derive a generally applicable objective via the REINFORCE policy-gradient formalism and demonstrate its efficacy with the state-of-the-art jailbreak algorithms Greedy Coordinate Gradient (GCG) and Projected Gradient Descent (PGD). For example, our objective doubles the attack success rate (ASR) on Llama3 and increases the ASR from 2% to 50% with circuit breaker defense."
  },
  {
    "title": "PrivaCI-Bench: Evaluating Privacy with Contextual Integrity and Legal Compliance",
    "url": "http://arxiv.org/abs/2502.17041v1",
    "arxiv_id": "2502.17041v1",
    "authors": [
      "Haoran Li",
      "Wenbin Hu",
      "Huihao Jing",
      "Yulin Chen",
      "Qi Hu",
      "Sirui Han",
      "Tianshu Chu",
      "Peizhao Hu",
      "Yangqiu Song"
    ],
    "published": "2025-02-24T10:49:34+00:00",
    "summary": "Recent advancements in generative large language models (LLMs) have enabled wider applicability, accessibility, and flexibility. However, their reliability and trustworthiness are still in doubt, especially for concerns regarding individuals' data privacy. Great efforts have been made on privacy by building various evaluation benchmarks to study LLMs' privacy awareness and robustness from their generated outputs to their hidden representations. Unfortunately, most of these works adopt a narrow formulation of privacy and only investigate personally identifiable information (PII). In this paper, we follow the merit of the Contextual Integrity (CI) theory, which posits that privacy evaluation should not only cover the transmitted attributes but also encompass the whole relevant social context through private information flows. We present PrivaCI-Bench, a comprehensive contextual privacy evaluation benchmark targeted at legal compliance to cover well-annotated privacy and safety regulations, real court cases, privacy policies, and synthetic data built from the official toolkit to study LLMs' privacy and safety compliance. We evaluate the latest LLMs, including the recent reasoner models QwQ-32B and Deepseek R1. Our experimental results suggest that though LLMs can effectively capture key CI parameters inside a given context, they still require further advancements for privacy compliance."
  },
  {
    "title": "LongSafety: Evaluating Long-Context Safety of Large Language Models",
    "url": "http://arxiv.org/abs/2502.16971v1",
    "arxiv_id": "2502.16971v1",
    "authors": [
      "Yida Lu",
      "Jiale Cheng",
      "Zhexin Zhang",
      "Shiyao Cui",
      "Cunxiang Wang",
      "Xiaotao Gu",
      "Yuxiao Dong",
      "Jie Tang",
      "Hongning Wang",
      "Minlie Huang"
    ],
    "published": "2025-02-24T08:54:39+00:00",
    "summary": "As Large Language Models (LLMs) continue to advance in understanding and generating long sequences, new safety concerns have been introduced through the long context. However, the safety of LLMs in long-context tasks remains under-explored, leaving a significant gap in both evaluation and improvement of their safety. To address this, we introduce LongSafety, the first comprehensive benchmark specifically designed to evaluate LLM safety in open-ended long-context tasks. LongSafety encompasses 7 categories of safety issues and 6 user-oriented long-context tasks, with a total of 1,543 test cases, averaging 5,424 words per context. Our evaluation towards 16 representative LLMs reveals significant safety vulnerabilities, with most models achieving safety rates below 55%. Our findings also indicate that strong safety performance in short-context scenarios does not necessarily correlate with safety in long-context tasks, emphasizing the unique challenges and urgency of improving long-context safety. Moreover, through extensive analysis, we identify challenging safety issues and task types for long-context models. Furthermore, we find that relevant context and extended input sequences can exacerbate safety risks in long-context scenarios, highlighting the critical need for ongoing attention to long-context safety challenges. Our code and data are available at https://github.com/thu-coai/LongSafety."
  },
  {
    "title": "GuidedBench: Equipping Jailbreak Evaluation with Guidelines",
    "url": "http://arxiv.org/abs/2502.16903v1",
    "arxiv_id": "2502.16903v1",
    "authors": [
      "Ruixuan Huang",
      "Xunguang Wang",
      "Zongjie Li",
      "Daoyuan Wu",
      "Shuai Wang"
    ],
    "published": "2025-02-24T06:57:27+00:00",
    "summary": "Jailbreaking methods for large language models (LLMs) have gained increasing attention for building safe and responsible AI systems. After analyzing 35 jailbreak methods across six categories, we find that existing benchmarks, relying on universal LLM-based or keyword-matching scores, lack case-specific criteria, leading to conflicting results. In this paper, we introduce a more robust evaluation framework for jailbreak methods, with a curated harmful question dataset, detailed case-by-case evaluation guidelines, and a scoring system equipped with these guidelines. Our experiments show that existing jailbreak methods exhibit better discrimination when evaluated using our benchmark. Some jailbreak methods that claim to achieve over 90% attack success rate (ASR) on other benchmarks only reach a maximum of 30.2% on our benchmark, providing a higher ceiling for more advanced jailbreak research; furthermore, using our scoring system reduces the variance of disagreements between different evaluator LLMs by up to 76.33%. This demonstrates its ability to provide more fair and stable evaluation."
  },
  {
    "title": "Leveraging Large Language Models for Effective and Explainable Multi-Agent Credit Assignment",
    "url": "http://arxiv.org/abs/2502.16863v1",
    "arxiv_id": "2502.16863v1",
    "authors": [
      "Kartik Nagpal",
      "Dayi Dong",
      "Jean-Baptiste Bouvier",
      "Negar Mehr"
    ],
    "published": "2025-02-24T05:56:47+00:00",
    "summary": "Recent work, spanning from autonomous vehicle coordination to in-space assembly, has shown the importance of learning collaborative behavior for enabling robots to achieve shared goals. A common approach for learning this cooperative behavior is to utilize the centralized-training decentralized-execution paradigm. However, this approach also introduces a new challenge: how do we evaluate the contributions of each agent's actions to the overall success or failure of the team. This credit assignment problem has remained open, and has been extensively studied in the Multi-Agent Reinforcement Learning literature. In fact, humans manually inspecting agent behavior often generate better credit evaluations than existing methods. We combine this observation with recent works which show Large Language Models demonstrate human-level performance at many pattern recognition tasks. Our key idea is to reformulate credit assignment to the two pattern recognition problems of sequence improvement and attribution, which motivates our novel LLM-MCA method. Our approach utilizes a centralized LLM reward-critic which numerically decomposes the environment reward based on the individualized contribution of each agent in the scenario. We then update the agents' policy networks based on this feedback. We also propose an extension LLM-TACA where our LLM critic performs explicit task assignment by passing an intermediary goal directly to each agent policy in the scenario. Both our methods far outperform the state-of-the-art on a variety of benchmarks, including Level-Based Foraging, Robotic Warehouse, and our new Spaceworld benchmark which incorporates collision-related safety constraints. As an artifact of our methods, we generate large trajectory datasets with each timestep annotated with per-agent reward information, as sampled from our LLM critics."
  },
  {
    "title": "PulseBat: A field-accessible dataset for second-life battery diagnostics from realistic histories using multidimensional rapid pulse test",
    "url": "http://arxiv.org/abs/2502.16848v1",
    "arxiv_id": "2502.16848v1",
    "authors": [
      "Shengyu Tao",
      "Guangyuan Ma",
      "Huixiong Yang",
      "Minyan Lu",
      "Guodan Wei",
      "Guangmin Zhou",
      "Xuan Zhang"
    ],
    "published": "2025-02-24T05:10:04+00:00",
    "summary": "As electric vehicles (EVs) approach the end of their operational life, their batteries retain significant economic value and present promising opportunities for second-life use and material recycling. This is particularly compelling for Global South and other underdeveloped regions, where reliable energy storage is vital to addressing critical challenges posed by weak and even nonexistent power grid and energy infrastructures. However, despite this potential, widespread adoption has been hindered by critical uncertainties surrounding the technical performance, safety, and recertification of second-life batteries. In cases where they have been redeployed, mismatches between estimated and actual performance often render batteries technically unsuitable or hazardous, turning them into liabilities for communities they were intended to benefit. This considerable misalignment exacerbates energy access disparities and undermines the broader vision of energy justice, highlighting an urgent need for robust and scalable solutions to unlock the potential. In the PulseBat Dataset, the authors tested 464 retired lithium-ion batteries, covering 3 cathode material types, 6 historical usages, 3 physical formats, and 6 capacity designs. The pulse test experiments were performed repeatedly for each second-life battery with 10 pulse width, 10 pulse magnitude, multiple state-of-charge, and state-of-health conditions, e.g., from 0.37 to 1.03. The PulseBat Dataset recorded these test conditions and the voltage response as well as the temperature signals that were subject to the injected pulse current, which could be used as a valuable data resource for critical diagnostics tasks such as state-of-charge estimation, state-of-health estimation, cathode material type identification, open-circuit voltage reconstruction, thermal management, and beyond."
  },
  {
    "title": "Multi-Agent Autonomous Driving Systems with Large Language Models: A Survey of Recent Advances",
    "url": "http://arxiv.org/abs/2502.16804v1",
    "arxiv_id": "2502.16804v1",
    "authors": [
      "Yaozu Wu",
      "Dongyuan Li",
      "Yankai Chen",
      "Renhe Jiang",
      "Henry Peng Zou",
      "Liancheng Fang",
      "Zhen Wang",
      "Philip S. Yu"
    ],
    "published": "2025-02-24T03:26:13+00:00",
    "summary": "Autonomous Driving Systems (ADSs) are revolutionizing transportation by reducing human intervention, improving operational efficiency, and enhancing safety. Large Language Models (LLMs), known for their exceptional planning and reasoning capabilities, have been integrated into ADSs to assist with driving decision-making. However, LLM-based single-agent ADSs face three major challenges: limited perception, insufficient collaboration, and high computational demands. To address these issues, recent advancements in LLM-based multi-agent ADSs have focused on improving inter-agent communication and cooperation. This paper provides a frontier survey of LLM-based multi-agent ADSs. We begin with a background introduction to related concepts, followed by a categorization of existing LLM-based approaches based on different agent interaction modes. We then discuss agent-human interactions in scenarios where LLM-based agents engage with humans. Finally, we summarize key applications, datasets, and challenges in this field to support future research (https://anonymous.4open.science/r/LLM-based_Multi-agent_ADS-3A5C/README.md)."
  },
  {
    "title": "Singularity resolution and regular black hole formation in gravitational collapse in asymptotically safe gravity",
    "url": "http://arxiv.org/abs/2502.16787v1",
    "arxiv_id": "2502.16787v1",
    "authors": [
      "Tomohiro Harada",
      "Chiang-Mei Chen",
      "Rituparna Mandal"
    ],
    "published": "2025-02-24T02:45:29+00:00",
    "summary": "We adopt an effective action inspired by asymptotically safe gravity, in which the effective gravitational constant is parameterized as $G(\\epsilon) = G_{N} [1 + \\tilde{\\omega} (G_{N}^{2} \\epsilon)^{\\alpha}]^{-1}$, where $G_{N}$ and $\\epsilon$ denote Newton's gravitational constant and the energy density of the matter field, respectively, with two dimensionless model parameters, $\\tilde{\\omega}$ and $\\alpha$. Within this framework, we investigate the complete gravitational collapse of a homogeneous ball of perfect fluid and find that the singularity is completely resolved for $\\alpha > 1$ but not for $1/2 \\le \\alpha \\le 1$. The case $0 < \\alpha < 1/2$ is inconsistent with asymptotic safety. Moreover, we note that although the singularity cannot be fully resolved for $\\alpha = 1$, it is significantly weakened by quantum gravity effects. Furthermore, we successfully construct a static exterior metric which, together with the interior solution, describes the dynamical formation of regular black holes in an asymptotically flat spacetime for the perfectly resolved case $\\alpha > 1$. The resulting regular black hole, obtained as the final static state, contains a de Sitter core and admits a static metric fully expressible in terms of the Lerch transcendent for general cases and in elementary functions for certain values of $\\alpha$, including \\alpha = 2$. We also discuss the formation of gravastars and the late-time evaporation process of the regular black holes."
  },
  {
    "title": "AISafetyLab: A Comprehensive Framework for AI Safety Evaluation and Improvement",
    "url": "http://arxiv.org/abs/2502.16776v1",
    "arxiv_id": "2502.16776v1",
    "authors": [
      "Zhexin Zhang",
      "Leqi Lei",
      "Junxiao Yang",
      "Xijie Huang",
      "Yida Lu",
      "Shiyao Cui",
      "Renmiao Chen",
      "Qinglin Zhang",
      "Xinyuan Wang",
      "Hao Wang",
      "Hao Li",
      "Xianqi Lei",
      "Chengwei Pan",
      "Lei Sha",
      "Hongning Wang",
      "Minlie Huang"
    ],
    "published": "2025-02-24T02:11:52+00:00",
    "summary": "As AI models are increasingly deployed across diverse real-world scenarios, ensuring their safety remains a critical yet underexplored challenge. While substantial efforts have been made to evaluate and enhance AI safety, the lack of a standardized framework and comprehensive toolkit poses significant obstacles to systematic research and practical adoption. To bridge this gap, we introduce AISafetyLab, a unified framework and toolkit that integrates representative attack, defense, and evaluation methodologies for AI safety. AISafetyLab features an intuitive interface that enables developers to seamlessly apply various techniques while maintaining a well-structured and extensible codebase for future advancements. Additionally, we conduct empirical studies on Vicuna, analyzing different attack and defense strategies to provide valuable insights into their comparative effectiveness. To facilitate ongoing research and development in AI safety, AISafetyLab is publicly available at https://github.com/thu-coai/AISafetyLab, and we are committed to its continuous maintenance and improvement."
  },
  {
    "title": "LED-Merging: Mitigating Safety-Utility Conflicts in Model Merging with Location-Election-Disjoint",
    "url": "http://arxiv.org/abs/2502.16770v1",
    "arxiv_id": "2502.16770v1",
    "authors": [
      "Qianli Ma",
      "Dongrui Liu",
      "Qian Chen",
      "Linfeng Zhang",
      "Jing Shao"
    ],
    "published": "2025-02-24T01:19:43+00:00",
    "summary": "Fine-tuning pre-trained Large Language Models (LLMs) for specialized tasks incurs substantial computational and data costs. While model merging offers a training-free solution to integrate multiple task-specific models, existing methods suffer from safety-utility conflicts where enhanced general capabilities degrade safety safeguards. We identify two root causes: \\textbf{neuron misidentification} due to simplistic parameter magnitude-based selection, and \\textbf{cross-task neuron interference} during merging. To address these challenges, we propose \\textbf{LED-Merging}, a three-stage framework that \\textbf{L}ocates task-specific neurons via gradient-based attribution, dynamically \\textbf{E}lects critical neurons through multi-model importance fusion, and \\textbf{D}isjoints conflicting updates through parameter isolation. Extensive experiments on Llama-3-8B, Mistral-7B, and Llama2-13B demonstrate that LED-Merging reduces harmful response rates(\\emph{e.g.}, a 31.4\\% decrease on Llama-3-8B-Instruct on HarmBench) while preserving 95\\% of utility performance(\\emph{e.g.}, 52.39\\% accuracy on GSM8K). LED-Merging resolves safety-utility conflicts and provides a lightweight, training-free paradigm for constructing reliable multi-task LLMs."
  },
  {
    "title": "ATEB: Evaluating and Improving Advanced NLP Tasks for Text Embedding Models",
    "url": "http://arxiv.org/abs/2502.16766v1",
    "arxiv_id": "2502.16766v1",
    "authors": [
      "Simeng Han",
      "Frank Palma Gomez",
      "Tu Vu",
      "Zefei Li",
      "Daniel Cer",
      "Hansi Zeng",
      "Chris Tar",
      "Arman Cohan",
      "Gustavo Hernandez Abrego"
    ],
    "published": "2025-02-24T01:08:15+00:00",
    "summary": "Traditional text embedding benchmarks primarily evaluate embedding models' capabilities to capture semantic similarity. However, more advanced NLP tasks require a deeper understanding of text, such as safety and factuality. These tasks demand an ability to comprehend and process complex information, often involving the handling of sensitive content, or the verification of factual statements against reliable sources. We introduce a new benchmark designed to assess and highlight the limitations of embedding models trained on existing information retrieval data mixtures on advanced capabilities, which include factuality, safety, instruction following, reasoning and document-level understanding. This benchmark includes a diverse set of tasks that simulate real-world scenarios where these capabilities are critical and leads to identification of the gaps of the currently advanced embedding models. Furthermore, we propose a novel method that reformulates these various tasks as retrieval tasks. By framing tasks like safety or factuality classification as retrieval problems, we leverage the strengths of retrieval models in capturing semantic relationships while also pushing them to develop a deeper understanding of context and content. Using this approach with single-task fine-tuning, we achieved performance gains of 8\\% on factuality classification and 13\\% on safety classification. Our code and data will be publicly available."
  },
  {
    "title": "Correlating and Predicting Human Evaluations of Language Models from Natural Language Processing Benchmarks",
    "url": "http://arxiv.org/abs/2502.18339v1",
    "arxiv_id": "2502.18339v1",
    "authors": [
      "Rylan Schaeffer",
      "Punit Singh Koura",
      "Binh Tang",
      "Ranjan Subramanian",
      "Aaditya K Singh",
      "Todor Mihaylov",
      "Prajjwal Bhargava",
      "Lovish Madaan",
      "Niladri S. Chatterji",
      "Vedanuj Goswami",
      "Sergey Edunov",
      "Dieuwke Hupkes",
      "Sanmi Koyejo",
      "Sharan Narang"
    ],
    "published": "2025-02-24T01:01:02+00:00",
    "summary": "The explosion of high-performing conversational language models (LMs) has spurred a shift from classic natural language processing (NLP) benchmarks to expensive, time-consuming and noisy human evaluations - yet the relationship between these two evaluation strategies remains hazy. In this paper, we conduct a large-scale study of four Chat Llama 2 models, comparing their performance on 160 standard NLP benchmarks (e.g., MMLU, ARC, BIG-Bench Hard) against extensive human preferences on more than 11k single-turn and 2k multi-turn dialogues from over 2k human annotators. Our findings are striking: most NLP benchmarks strongly correlate with human evaluations, suggesting that cheaper, automated metrics can serve as surprisingly reliable predictors of human preferences. Three human evaluations, such as adversarial dishonesty and safety, are anticorrelated with NLP benchmarks, while two are uncorrelated. Moreover, through overparameterized linear regressions, we show that NLP scores can accurately predict human evaluations across different model scales, offering a path to reduce costly human annotation without sacrificing rigor. Overall, our results affirm the continued value of classic benchmarks and illuminate how to harness them to anticipate real-world user satisfaction - pointing to how NLP benchmarks can be leveraged to meet evaluation needs of our new era of conversational AI."
  },
  {
    "title": "Watch Out E-scooter Coming Through: Multimodal Sensing of Mixed Traffic Use and Conflicts Through Riders Ego-centric Views",
    "url": "http://arxiv.org/abs/2502.16755v1",
    "arxiv_id": "2502.16755v1",
    "authors": [
      "Hiruni Nuwanthika Kegalle",
      "Danula Hettiachchi",
      "Jeffrey Chan",
      "Mark Sanderson",
      "Flora D. Salim"
    ],
    "published": "2025-02-24T00:16:18+00:00",
    "summary": "E-scooters are becoming a popular means of urban transportation. However, this increased popularity brings challenges, such as road accidents and conflicts when sharing space with traditional transport modes. An in-depth understanding of e-scooter rider behaviour is crucial for ensuring rider safety, guiding infrastructure planning, and enforcing traffic rules. This study investigated the rider behaviour through a naturalistic study with 23 participants equipped with a bike computer, eye-tracking glasses and cameras. They followed a pre-determined route, enabling multi-modal data collection. We analysed and compared gaze movements, speed, and video feeds across three transport infrastructure types: a pedestrian-shared path, a cycle lane and a roadway. Our findings reveal unique challenges e-scooter riders face, including difficulty keeping up with cyclists and motor vehicles due to speed limits on shared e-scooters, risks in signalling turns due to control lose, and limited acceptance in mixed-use spaces. The cycle lane showed the highest average speed, the least speed change points, and the least head movements, supporting its suitability as dedicated infrastructure for e-scooters. These findings are facilitated through multimodal sensing and analysing the e-scooter riders' ego-centric view, which show the efficacy of our method in discovering the behavioural dynamics of the riders in the wild. Our study highlights the critical need to align infrastructure with user behaviour to improve safety and emphasises the importance of targeted safety measures and regulations, especially when e-scooter riders share spaces with pedestrians or motor vehicles. The dataset and analysis code are available at https://github.com/HiruniNuwanthika/Electric-Scooter-Riders-Multi-Modal-Data-Analysis.git."
  },
  {
    "title": "Guardians of the Agentic System: Preventing Many Shots Jailbreak with Agentic System",
    "url": "http://arxiv.org/abs/2502.16750v1",
    "arxiv_id": "2502.16750v1",
    "authors": [
      "Saikat Barua",
      "Mostafizur Rahman",
      "Md Jafor Sadek",
      "Rafiul Islam",
      "Shehnaz Khaled",
      "Ahmedul Kabir"
    ],
    "published": "2025-02-23T23:35:15+00:00",
    "summary": "The autonomous AI agents using large language models can create undeniable values in all span of the society but they face security threats from adversaries that warrants immediate protective solutions because trust and safety issues arise. Considering the many-shot jailbreaking and deceptive alignment as some of the main advanced attacks, that cannot be mitigated by the static guardrails used during the supervised training, points out a crucial research priority for real world robustness. The combination of static guardrails in dynamic multi-agent system fails to defend against those attacks. We intend to enhance security for LLM-based agents through the development of new evaluation frameworks which identify and counter threats for safe operational deployment. Our work uses three examination methods to detect rogue agents through a Reverse Turing Test and analyze deceptive alignment through multi-agent simulations and develops an anti-jailbreaking system by testing it with GEMINI 1.5 pro and llama-3.3-70B, deepseek r1 models using tool-mediated adversarial scenarios. The detection capabilities are strong such as 94\\% accuracy for GEMINI 1.5 pro yet the system suffers persistent vulnerabilities when under long attacks as prompt length increases attack success rates (ASR) and diversity metrics become ineffective in prediction while revealing multiple complex system faults. The findings demonstrate the necessity of adopting flexible security systems based on active monitoring that can be performed by the agents themselves together with adaptable interventions by system admin as the current models can create vulnerabilities that can lead to the unreliable and vulnerable system. So, in our work, we try to address such situations and propose a comprehensive framework to counteract the security issues."
  },
  {
    "title": "Toward Responsible Federated Large Language Models: Leveraging a Safety Filter and Constitutional AI",
    "url": "http://arxiv.org/abs/2502.16691v1",
    "arxiv_id": "2502.16691v1",
    "authors": [
      "Eunchung Noh",
      "Jeonghun Baek"
    ],
    "published": "2025-02-23T19:12:10+00:00",
    "summary": "Recent research has increasingly focused on training large language models (LLMs) using federated learning, known as FedLLM. However, responsible AI (RAI), which aims to ensure safe responses, remains underexplored in the context of FedLLM. In FedLLM, client data used for training may contain harmful content, leading to unsafe LLMs that generate harmful responses. Aggregating such unsafe LLMs into the global model and distributing them to clients may result in the widespread deployment of unsafe LLMs. To address this issue, we incorporate two well-known RAI methods into FedLLM: the safety filter and constitutional AI. Our experiments demonstrate that these methods significantly enhance the safety of the LLM, achieving over a 20% improvement on AdvBench, a benchmark for evaluating safety performance."
  },
  {
    "title": "Analyzing Factors Influencing Driver Willingness to Accept Advanced Driver Assistance Systems",
    "url": "http://arxiv.org/abs/2502.16688v1",
    "arxiv_id": "2502.16688v1",
    "authors": [
      "Hannah Musau",
      "Nana Kankam Gyimah",
      "Judith Mwakalonge",
      "Gurcan Comert",
      "Saidi Siuhi"
    ],
    "published": "2025-02-23T19:01:54+00:00",
    "summary": "Advanced Driver Assistance Systems (ADAS) enhance highway safety by improving environmental perception and reducing human errors. However, misconceptions, trust issues, and knowledge gaps hinder widespread adoption. This study examines driver perceptions, knowledge sources, and usage patterns of ADAS in passenger vehicles. A nationwide survey collected data from a diverse sample of U.S. drivers. Machine learning models predicted ADAS adoption, with SHAP (SHapley Additive Explanations) identifying key influencing factors. Findings indicate that higher trust levels correlate with increased ADAS usage, while concerns about reliability remain a barrier. Specific features, such as Forward Collision Warning and Driver Monitoring Systems, significantly influence adoption likelihood. Demographic factors (age, gender) and driving habits (experience, frequency) also shape ADAS acceptance. Findings emphasize the influence of socioeconomic, demographic, and behavioral factors on ADAS adoption, offering guidance for automakers, policymakers, and safety advocates to improve awareness, trust, and usability."
  },
  {
    "title": "Security Analysis of 5G NR Device-to-Device Sidelink Communications",
    "url": "http://arxiv.org/abs/2502.16650v1",
    "arxiv_id": "2502.16650v1",
    "authors": [
      "Evangelos Bitsikas",
      "Aanjhan Ranganathan"
    ],
    "published": "2025-02-23T16:55:32+00:00",
    "summary": "5G NR sidelink communication enables new possibilities for direct device-to-device interactions, supporting applications from vehicle-to-everything (V2X) systems to public safety, industrial automation, and drone networks. However, these advancements come with significant security challenges due to the decentralized trust model and increased reliance on User Equipment (UE) for critical functions like synchronization, resource allocation, and authorization. This paper presents the first comprehensive security analysis of NR V2X sidelink. We identify vulnerabilities across critical procedures and demonstrate plausible attack, including attacks that manipulate data integrity feedback and block resources, ultimately undermining the reliability and privacy of sidelink communications. Our analysis reveals that NR operational modes are vulnerable, with the ones relying on autonomous resource management (without network supervision) particularly exposed. To address these issues, we propose mitigation strategies to enhance the security of 5G sidelink communications. This work establishes a foundation for future efforts to strengthen 5G device-to-device sidelink communications, ensuring its safe deployment in critical applications."
  },
  {
    "title": "CODESYNC: Synchronizing Large Language Models with Dynamic Code Evolution at Scale",
    "url": "http://arxiv.org/abs/2502.16645v1",
    "arxiv_id": "2502.16645v1",
    "authors": [
      "Chenlong Wang",
      "Zhaoyang Chu",
      "Zhengxiang Cheng",
      "Xuyi Yang",
      "Kaiyue Qiu",
      "Yao Wan",
      "Zhou Zhao",
      "Xuanhua Shi",
      "Dongping Chen"
    ],
    "published": "2025-02-23T16:46:18+00:00",
    "summary": "Large Language Models (LLMs) have exhibited exceptional performance in software engineering yet face challenges in adapting to continually evolving code knowledge, particularly regarding the frequent updates of third-party library APIs. This limitation, stemming from static pre-training datasets, often results in non-executable code or implementations with suboptimal safety and efficiency. To this end, this paper introduces CODESYNC, a data engine for identifying outdated code patterns and collecting real-time code knowledge updates from Python third-party libraries. Building upon CODESYNC, we develop CODESYNCBENCH, a comprehensive benchmark for assessing LLMs' ability to stay synchronized with code evolution, which covers real-world updates for 220 APIs from six Python libraries. Our benchmark offers 3,300 test cases across three evaluation tasks and an update-aware instruction tuning dataset consisting of 2,200 training samples. Extensive experiments on 14 state-of-the-art LLMs reveal that they struggle with dynamic code evolution, even with the support of advanced knowledge updating methods (e.g., DPO, ORPO, and SimPO). We believe that our benchmark can offer a strong foundation for the development of more effective methods for real-time code knowledge updating in the future. The experimental code and dataset are publicly available at: https://github.com/Lucky-voyage/Code-Sync."
  },
  {
    "title": "Color Information-Based Automated Mask Generation for Detecting Underwater Atypical Glare Areas",
    "url": "http://arxiv.org/abs/2502.16538v1",
    "arxiv_id": "2502.16538v1",
    "authors": [
      "Mingyu Jeon",
      "Yeonji Paeng",
      "Sejin Lee"
    ],
    "published": "2025-02-23T11:17:20+00:00",
    "summary": "Underwater diving assistance and safety support robots acquire real-time diver information through onboard underwater cameras. This study introduces a breath bubble detection algorithm that utilizes unsupervised K-means clustering, thereby addressing the high accuracy demands of deep learning models as well as the challenges associated with constructing supervised datasets. The proposed method fuses color data and relative spatial coordinates from underwater images, employs CLAHE to mitigate noise, and subsequently performs pixel clustering to isolate reflective regions. Experimental results demonstrate that the algorithm can effectively detect regions corresponding to breath bubbles in underwater images, and that the combined use of RGB, LAB, and HSV color spaces significantly enhances detection accuracy. Overall, this research establishes a foundation for monitoring diver conditions and identifying potential equipment malfunctions in underwater environments."
  },
  {
    "title": "GraphCheck: Breaking Long-Term Text Barriers with Extracted Knowledge Graph-Powered Fact-Checking",
    "url": "http://arxiv.org/abs/2502.16514v1",
    "arxiv_id": "2502.16514v1",
    "authors": [
      "Yingjian Chen",
      "Haoran Liu",
      "Yinhong Liu",
      "Rui Yang",
      "Han Yuan",
      "Yanran Fu",
      "Pengyuan Zhou",
      "Qingyu Chen",
      "James Caverlee",
      "Irene Li"
    ],
    "published": "2025-02-23T09:25:00+00:00",
    "summary": "Large language models (LLMs) are widely used, but they often generate subtle factual errors, especially in long-form text. These errors are fatal in some specialized domains such as medicine. Existing fact-checking with grounding documents methods face two main challenges: (1) they struggle to understand complex multihop relations in long documents, often overlooking subtle factual errors; (2) most specialized methods rely on pairwise comparisons, requiring multiple model calls, leading to high resource and computational costs. To address these challenges, we propose \\textbf{\\textit{GraphCheck}}, a fact-checking framework that uses extracted knowledge graphs to enhance text representation. Graph Neural Networks further process these graphs as a soft prompt, enabling LLMs to incorporate structured knowledge more effectively. Enhanced with graph-based reasoning, GraphCheck captures multihop reasoning chains which are often overlooked by existing methods, enabling precise and efficient fact-checking in a single inference call. Experimental results on seven benchmarks spanning both general and medical domains demonstrate a 6.1\\% overall improvement over baseline models. Notably, GraphCheck outperforms existing specialized fact-checkers and achieves comparable performance with state-of-the-art LLMs, such as DeepSeek-V3 and OpenAI-o1, with significantly fewer parameters."
  },
  {
    "title": "FanChuan: A Multilingual and Graph-Structured Benchmark For Parody Detection and Analysis",
    "url": "http://arxiv.org/abs/2502.16503v1",
    "arxiv_id": "2502.16503v1",
    "authors": [
      "Yilun Zheng",
      "Sha Li",
      "Fangkun Wu",
      "Yang Ziyi",
      "Lin Hongchao",
      "Zhichao Hu",
      "Cai Xinjun",
      "Ziming Wang",
      "Jinxuan Chen",
      "Sitao Luan",
      "Jiahao Xu",
      "Lihui Chen"
    ],
    "published": "2025-02-23T08:52:46+00:00",
    "summary": "Parody is an emerging phenomenon on social media, where individuals imitate a role or position opposite to their own, often for humor, provocation, or controversy. Detecting and analyzing parody can be challenging and is often reliant on context, yet it plays a crucial role in understanding cultural values, promoting subcultures, and enhancing self-expression. However, the study of parody is hindered by limited available data and deficient diversity in current datasets. To bridge this gap, we built seven parody datasets from both English and Chinese corpora, with 14,755 annotated users and 21,210 annotated comments in total. To provide sufficient context information, we also collect replies and construct user-interaction graphs to provide richer contextual information, which is lacking in existing datasets. With these datasets, we test traditional methods and Large Language Models (LLMs) on three key tasks: (1) parody detection, (2) comment sentiment analysis with parody, and (3) user sentiment analysis with parody. Our extensive experiments reveal that parody-related tasks still remain challenging for all models, and contextual information plays a critical role. Interestingly, we find that, in certain scenarios, traditional sentence embedding methods combined with simple classifiers can outperform advanced LLMs, i.e. DeepSeek-R1 and GPT-o3, highlighting parody as a significant challenge for LLMs."
  },
  {
    "title": "Facilitating Emergency Vehicle Passage in Congested Urban Areas Using Multi-agent Deep Reinforcement Learning",
    "url": "http://arxiv.org/abs/2502.16449v1",
    "arxiv_id": "2502.16449v1",
    "authors": [
      "Haoran Su"
    ],
    "published": "2025-02-23T05:34:32+00:00",
    "summary": "Emergency Response Time (ERT) is crucial for urban safety, measuring cities' ability to handle medical, fire, and crime emergencies. In NYC, medical ERT increased 72% from 7.89 minutes in 2014 to 14.27 minutes in 2024, with half of delays due to Emergency Vehicle (EMV) travel times. Each minute's delay in stroke response costs 2 million brain cells, while cardiac arrest survival drops 7-10% per minute.   This dissertation advances EMV facilitation through three contributions. First, EMVLight, a decentralized multi-agent reinforcement learning framework, integrates EMV routing with traffic signal pre-emption. It achieved 42.6% faster EMV travel times and 23.5% improvement for other vehicles.   Second, the Dynamic Queue-Jump Lane system uses Multi-Agent Proximal Policy Optimization for coordinated lane-clearing in mixed autonomous and human-driven traffic, reducing EMV travel times by 40%.   Third, an equity study of NYC Emergency Medical Services revealed disparities across boroughs: Staten Island faces delays due to sparse signalized intersections, while Manhattan struggles with congestion. Solutions include optimized EMS stations and improved intersection designs.   These contributions enhance EMV mobility and emergency service equity, offering insights for policymakers and urban planners to develop safer, more efficient transportation systems."
  },
  {
    "title": "Visual Reasoning Evaluation of Grok, Deepseek Janus, Gemini, Qwen, Mistral, and ChatGPT",
    "url": "http://arxiv.org/abs/2502.16428v1",
    "arxiv_id": "2502.16428v1",
    "authors": [
      "Nidhal Jegham",
      "Marwan Abdelatti",
      "Abdeltawab Hendawi"
    ],
    "published": "2025-02-23T04:01:43+00:00",
    "summary": "Traditional evaluations of multimodal large language models (LLMs) have been limited by their focus on single-image reasoning, failing to assess crucial aspects like contextual understanding, reasoning stability, and uncertainty calibration. This study addresses these limitations by introducing a novel benchmark that integrates multi-image reasoning tasks with rejection-based evaluation and positional bias detection. To evaluate these dimensions, we further introduce entropy as a novel metric for quantifying reasoning consistency across reordered answer variants. We applied this benchmark to assess Grok 3, ChatGPT-4o, ChatGPT-o1, Gemini 2.0 Flash Experimental, DeepSeek Janus models, Qwen2.5-VL-72B-Instruct, QVQ-72B-Preview, and Pixtral 12B across eight visual reasoning tasks, including difference spotting and diagram interpretation. Our findings reveal ChatGPT-o1 leading in overall accuracy (82.5\\%) and rejection accuracy (70.0\\%), closely followed by Gemini 2.0 Flash Experimental (70.8\\%). QVQ-72B-Preview demonstrated superior rejection accuracy (85.5\\%). Notably, Pixtral 12B (51.7\\%) showed promise in specific domains, while Janus models exhibited challenges in bias and uncertainty calibration, reflected in low rejection accuracies and high entropy scores. High entropy scores in Janus models (Janus 7B: 0.8392, Janus 1B: 0.787) underscore their susceptibility to positional bias and unstable reasoning, contrasting with the low entropy and robust reasoning of ChatGPT models. The study further demonstrates that model size is not the sole determinant of performance, as evidenced by Grok 3 underperformance despite its substantial parameter count. By employing multi-image contexts, rejection mechanisms, and entropy-based consistency metrics, this benchmark sets a new standard for evaluating multimodal LLMs, enabling a more robust and reliable assessment of next-generation AI systems."
  },
  {
    "title": "Navigation-GPT: A Robust and Adaptive Framework Utilizing Large Language Models for Navigation Applications",
    "url": "http://arxiv.org/abs/2502.16402v1",
    "arxiv_id": "2502.16402v1",
    "authors": [
      "Feng Ma",
      "Xiu-min Wang",
      "Chen Chen",
      "Xiao-bin Xu",
      "Xin-ping Yan"
    ],
    "published": "2025-02-23T01:41:58+00:00",
    "summary": "Existing navigation decision support systems often perform poorly when handling non-predefined navigation scenarios. Leveraging the generalization capabilities of large language model (LLM) in handling unknown scenarios, this research proposes a dual-core framework for LLM applications to address this issue. Firstly, through ReAct-based prompt engineering, a larger LLM core decomposes intricate navigation tasks into manageable sub-tasks, which autonomously invoke corresponding external tools to gather relevant information, using this feedback to mitigate the risk of LLM hallucinations. Subsequently, a fine-tuned and compact LLM core, acting like a first-mate is designed to process such information and unstructured external data, then to generates context-aware recommendations, ultimately delivering lookout insights and navigation hints that adhere to the International Regulations for Preventing Collisions at Sea (COLREGs) and other rules. Extensive experiments demonstrate the proposed framework not only excels in traditional ship collision avoidance tasks but also adapts effectively to unstructured, non-predefined, and unpredictable scenarios. A comparative analysis with DeepSeek-R1, GPT-4o and other SOTA models highlights the efficacy and rationality of the proposed framework. This research bridges the gap between conventional navigation systems and LLMs, offering a framework to enhance safety and operational efficiency across diverse navigation applications."
  },
  {
    "title": "An Expert Ensemble for Detecting Anomalous Scenes, Interactions, and Behaviors in Autonomous Driving",
    "url": "http://arxiv.org/abs/2502.16389v1",
    "arxiv_id": "2502.16389v1",
    "authors": [
      "Tianchen Ji",
      "Neeloy Chakraborty",
      "Andre Schreiber",
      "Katherine Driggs-Campbell"
    ],
    "published": "2025-02-23T00:43:23+00:00",
    "summary": "As automated vehicles enter public roads, safety in a near-infinite number of driving scenarios becomes one of the major concerns for the widespread adoption of fully autonomous driving. The ability to detect anomalous situations outside of the operational design domain is a key component in self-driving cars, enabling us to mitigate the impact of abnormal ego behaviors and to realize trustworthy driving systems. On-road anomaly detection in egocentric videos remains a challenging problem due to the difficulties introduced by complex and interactive scenarios. We conduct a holistic analysis of common on-road anomaly patterns, from which we propose three unsupervised anomaly detection experts: a scene expert that focuses on frame-level appearances to detect abnormal scenes and unexpected scene motions; an interaction expert that models normal relative motions between two road participants and raises alarms whenever anomalous interactions emerge; and a behavior expert which monitors abnormal behaviors of individual objects by future trajectory prediction. To combine the strengths of all the modules, we propose an expert ensemble (Xen) using a Kalman filter, in which the final anomaly score is absorbed as one of the states and the observations are generated by the experts. Our experiments employ a novel evaluation protocol for realistic model performance, demonstrate superior anomaly detection performance than previous methods, and show that our framework has potential in classifying anomaly types using unsupervised learning on a large-scale on-road anomaly dataset."
  },
  {
    "title": "Understanding Generative AI Risks for Youth: A Taxonomy Based on Empirical Data",
    "url": "http://arxiv.org/abs/2502.16383v1",
    "arxiv_id": "2502.16383v1",
    "authors": [
      "Yaman Yu",
      "Yiren Liu",
      "Jacky Zhang",
      "Yun Huang",
      "Yang Wang"
    ],
    "published": "2025-02-22T23:31:51+00:00",
    "summary": "Generative AI (GAI) is reshaping the way young users engage with technology. This study introduces a taxonomy of risks associated with youth-GAI interactions, derived from an analysis of 344 chat transcripts between youth and GAI chatbots, 30,305 Reddit discussions concerning youth engagement with these systems, and 153 documented AI-related incidents. We categorize risks into six overarching themes, identifying 84 specific risks, which we further align with four distinct interaction pathways. Our findings highlight emerging concerns, such as risks to mental wellbeing, behavioral and social development, and novel forms of toxicity, privacy breaches, and misuse/exploitation that are not fully addressed in existing frameworks on child online safety or AI risks. By systematically grounding our taxonomy in empirical data, this work offers a structured approach to aiding AI developers, educators, caregivers, and policymakers in comprehending and mitigating risks associated with youth-GAI interactions."
  },
  {
    "title": "A generative approach to LLM harmfulness detection with special red flag tokens",
    "url": "http://arxiv.org/abs/2502.16366v1",
    "arxiv_id": "2502.16366v1",
    "authors": [
      "Sophie Xhonneux",
      "David Dobre",
      "Mehrnaz Mohfakhami",
      "Leo Schwinn",
      "Gauthier Gidel"
    ],
    "published": "2025-02-22T21:48:48+00:00",
    "summary": "Most safety training methods for large language models (LLMs) based on fine-tuning rely on dramatically changing the output distribution of the model when faced with a harmful request, shifting it from an unsafe answer to a refusal to respond. These methods inherently compromise model capabilities and might make auto-regressive models vulnerable to attacks that make likely an initial token of affirmative response. To avoid that, we propose to expand the model's vocabulary with a special token we call red flag token (<rf>) and propose to fine-tune the model to generate this token at any time harmful content is generated or about to be generated. This novel safety training method effectively augments LLMs into generative classifiers of harmfulness at all times during the conversation. This method offers several advantages: it enables the model to explicitly learn the concept of harmfulness while marginally affecting the generated distribution, thus maintaining the model's utility. It also evaluates each generated answer rather than just the input prompt and provides a stronger defence against sampling-based attacks. In addition, it simplifies the evaluation of the model's robustness and reduces correlated failures when combined with a classifier. We further show an increased robustness to long contexts, and supervised fine-tuning attacks."
  },
  {
    "title": "A Framework for Evaluating Vision-Language Model Safety: Building Trust in AI for Public Sector Applications",
    "url": "http://arxiv.org/abs/2502.16361v1",
    "arxiv_id": "2502.16361v1",
    "authors": [
      "Maisha Binte Rashid",
      "Pablo Rivas"
    ],
    "published": "2025-02-22T21:33:26+00:00",
    "summary": "Vision-Language Models (VLMs) are increasingly deployed in public sector missions, necessitating robust evaluation of their safety and vulnerability to adversarial attacks. This paper introduces a novel framework to quantify adversarial risks in VLMs. We analyze model performance under Gaussian, salt-and-pepper, and uniform noise, identifying misclassification thresholds and deriving composite noise patches and saliency patterns that highlight vulnerable regions. These patterns are compared against the Fast Gradient Sign Method (FGSM) to assess their adversarial effectiveness. We propose a new Vulnerability Score that combines the impact of random noise and adversarial attacks, providing a comprehensive metric for evaluating model robustness."
  },
  {
    "title": "Risk-Averse Reinforcement Learning: An Optimal Transport Perspective on Temporal Difference Learning",
    "url": "http://arxiv.org/abs/2502.16328v1",
    "arxiv_id": "2502.16328v1",
    "authors": [
      "Zahra Shahrooei",
      "Ali Baheri"
    ],
    "published": "2025-02-22T19:14:36+00:00",
    "summary": "The primary goal of reinforcement learning is to develop decision-making policies that prioritize optimal performance, frequently without considering risk or safety. In contrast, safe reinforcement learning seeks to reduce or avoid unsafe states. This letter introduces a risk-averse temporal difference algorithm that uses optimal transport theory to direct the agent toward predictable behavior. By incorporating a risk indicator, the agent learns to favor actions with predictable consequences. We evaluate the proposed algorithm in several case studies and show its effectiveness in the presence of uncertainty. The results demonstrate that our method reduces the frequency of visits to risky states while preserving performance. A Python implementation of the algorithm is available at https:// github.com/SAILRIT/Risk-averse-TD-Learning."
  },
  {
    "title": "Optimization-free Smooth Control Barrier Function for Polygonal Collision Avoidance",
    "url": "http://arxiv.org/abs/2502.16293v1",
    "arxiv_id": "2502.16293v1",
    "authors": [
      "Shizhen Wu",
      "Yongchun Fang",
      "Ning Sun",
      "Biao Lu",
      "Xiao Liang",
      "Yiming Zhao"
    ],
    "published": "2025-02-22T16:47:27+00:00",
    "summary": "Polygonal collision avoidance (PCA) is short for the problem of collision avoidance between two polygons (i.e., polytopes in planar) that own their dynamic equations. This problem suffers the inherent difficulty in dealing with non-smooth boundaries and recently optimization-defined metrics, such as signed distance field (SDF) and its variants, have been proposed as control barrier functions (CBFs) to tackle PCA problems. In contrast, we propose an optimization-free smooth CBF method in this paper, which is computationally efficient and proved to be nonconservative. It is achieved by three main steps: a lower bound of SDF is expressed as a nested Boolean logic composition first, then its smooth approximation is established by applying the latest log-sum-exp method, after which a specified CBF-based safety filter is proposed to address this class of problems. To illustrate its wide applications, the optimization-free smooth CBF method is extended to solve distributed collision avoidance of two underactuated nonholonomic vehicles and drive an underactuated container crane to avoid a moving obstacle respectively, for which numerical simulations are also performed."
  },
  {
    "title": "Pseudo-Measurement Enhancement in Power Distribution Systems",
    "url": "http://arxiv.org/abs/2502.16188v1",
    "arxiv_id": "2502.16188v1",
    "authors": [
      "Tao Xu",
      "Kaiqi Wang",
      "Jiadong Zhang",
      "Ji Qiao",
      "Zixuan Zhao",
      "Hong Zhu",
      "Kai Sun"
    ],
    "published": "2025-02-22T11:17:38+00:00",
    "summary": "With the rapid development of smart distribution networks (DNs), the integrity and accuracy of grid measurement data are crucial to the safety and stability of the entire system. However, the quality of the user power consumption data cannot be guaranteed during the collection and transmission process. To this end, this paper proposes a low-rank tensor completion model based on CANDECOMP/PARAFAC decomposition (CPD-LRTC) to enhance the quality of the measurement data of the DNs. Firstly, the causes and the associated characteristics of the missing data are analyzed, and a third-order standard tensor is constructed as a mathematical model of the measurement data of the DN. Then, a completion model is established based on the characteristics of measurement data and the low rank of the completion tensor, and the alternating direction method of multipliers (ADMM) is used to solve it iteratively. Finally, the proposed model is verified through two case studies, the completion accuracy, the computational efficiency, and the memory usage are compared to traditional methods."
  },
  {
    "title": "On Asymptotic safety in 4D gauge theory with additional dimension=4 operators",
    "url": "http://arxiv.org/abs/2502.16187v1",
    "arxiv_id": "2502.16187v1",
    "authors": [
      "Alfiia Mukhaeva"
    ],
    "published": "2025-02-22T11:15:11+00:00",
    "summary": "We study interacting fixed points of simple quantum field theory in four-dimensional $SU(N_c)$ coupled to $N_f$ species of color fermions and $N_f^2$ colorless scalars in the Veneziano limit. Using the rich structure of all possible quartic scalar operators, we find an interacting conformal fixed point with stable vacua and crossovers inbetween. We perform calculations in perturbation theory up to four loop in the gauge and three loop in the Yukawa and scalar couplings. We also consider anomalous dimensions for fields, scalar mass squared, and a class of dimension-three operators."
  },
  {
    "title": "Maybe I Should Not Answer That, but... Do LLMs Understand The Safety of Their Inputs?",
    "url": "http://arxiv.org/abs/2502.16174v1",
    "arxiv_id": "2502.16174v1",
    "authors": [
      "Maciej Chrab\u0105szcz",
      "Filip Szatkowski",
      "Bartosz W\u00f3jcik",
      "Jan Dubi\u0144ski",
      "Tomasz Trzci\u0144ski"
    ],
    "published": "2025-02-22T10:31:50+00:00",
    "summary": "Ensuring the safety of the Large Language Model (LLM) is critical, but currently used methods in most cases sacrifice the model performance to obtain increased safety or perform poorly on data outside of their adaptation distribution. We investigate existing methods for such generalization and find them insufficient. Surprisingly, while even plain LLMs recognize unsafe prompts, they may still generate unsafe responses. To avoid performance degradation and preserve safe performance, we advocate for a two-step framework, where we first identify unsafe prompts via a lightweight classifier, and apply a \"safe\" model only to such prompts. In particular, we explore the design of the safety detector in more detail, investigating the use of different classifier architectures and prompting techniques. Interestingly, we find that the final hidden state for the last token is enough to provide robust performance, minimizing false positives on benign data while performing well on malicious prompt detection. Additionally, we show that classifiers trained on the representations from different model layers perform comparably on the latest model layers, indicating that safety representation is present in the LLMs' hidden states at most model stages. Our work is a step towards efficient, representation-based safety mechanisms for LLMs."
  },
  {
    "title": "Be a Multitude to Itself: A Prompt Evolution Framework for Red Teaming",
    "url": "http://arxiv.org/abs/2502.16109v1",
    "arxiv_id": "2502.16109v1",
    "authors": [
      "Rui Li",
      "Peiyi Wang",
      "Jingyuan Ma",
      "Di Zhang",
      "Lei Sha",
      "Zhifang Sui"
    ],
    "published": "2025-02-22T06:13:19+00:00",
    "summary": "Large Language Models (LLMs) have gained increasing attention for their remarkable capacity, alongside concerns about safety arising from their potential to produce harmful content. Red teaming aims to find prompts that could elicit harmful responses from LLMs, and is essential to discover and mitigate safety risks before real-world deployment. However, manual red teaming is both time-consuming and expensive, rendering it unscalable. In this paper, we propose RTPE, a scalable evolution framework to evolve red teaming prompts across both breadth and depth dimensions, facilitating the automatic generation of numerous high-quality and diverse red teaming prompts. Specifically, in-breadth evolving employs a novel enhanced in-context learning method to create a multitude of quality prompts, whereas in-depth evolving applies customized transformation operations to enhance both content and form of prompts, thereby increasing diversity. Extensive experiments demonstrate that RTPE surpasses existing representative automatic red teaming methods on both attack success rate and diversity. In addition, based on 4,800 red teaming prompts created by RTPE, we further provide a systematic analysis of 8 representative LLMs across 8 sensitive topics."
  },
  {
    "title": "Online Learning of Danger Avoidance for Complex Structures of Musculoskeletal Humanoids and Its Applications",
    "url": "http://arxiv.org/abs/2502.16085v1",
    "arxiv_id": "2502.16085v1",
    "authors": [
      "Kento Kawaharazuka",
      "Naoki Hiraoka",
      "Yuya Koga",
      "Manabu Nishiura",
      "Yusuke Omura",
      "Yuki Asano",
      "Kei Okada",
      "Koji Kawasaki",
      "Masayuki Inaba"
    ],
    "published": "2025-02-22T05:16:18+00:00",
    "summary": "The complex structure of musculoskeletal humanoids makes it difficult to model them, and the inter-body interference and high internal muscle force are unavoidable. Although various safety mechanisms have been developed to solve this problem, it is important not only to deal with the dangers when they occur but also to prevent them from happening. In this study, we propose a method to learn a network outputting danger probability corresponding to the muscle length online so that the robot can gradually prevent dangers from occurring. Applications of this network for control are also described. The method is applied to the musculoskeletal humanoid, Musashi, and its effectiveness is verified."
  },
  {
    "title": "Multimodal Inconsistency Reasoning (MMIR): A New Benchmark for Multimodal Reasoning Models",
    "url": "http://arxiv.org/abs/2502.16033v1",
    "arxiv_id": "2502.16033v1",
    "authors": [
      "Qianqi Yan",
      "Yue Fan",
      "Hongquan Li",
      "Shan Jiang",
      "Yang Zhao",
      "Xinze Guan",
      "Ching-Chen Kuo",
      "Xin Eric Wang"
    ],
    "published": "2025-02-22T01:52:37+00:00",
    "summary": "Existing Multimodal Large Language Models (MLLMs) are predominantly trained and tested on consistent visual-textual inputs, leaving open the question of whether they can handle inconsistencies in real-world, layout-rich content. To bridge this gap, we propose the Multimodal Inconsistency Reasoning (MMIR) benchmark to assess MLLMs' ability to detect and reason about semantic mismatches in artifacts such as webpages, presentation slides, and posters. MMIR comprises 534 challenging samples, each containing synthetically injected errors across five reasoning-heavy categories: Factual Contradiction, Identity Misattribution, Contextual Mismatch, Quantitative Discrepancy, and Temporal/Spatial Incoherence. We evaluate six state-of-the-art MLLMs, showing that models with dedicated multimodal reasoning capabilities, such as o1, substantially outperform their counterparts while open-source models remain particularly vulnerable to inconsistency errors. Detailed error analyses further show that models excel in detecting inconsistencies confined to a single modality, particularly in text, but struggle with cross-modal conflicts and complex layouts. Probing experiments reveal that single-modality prompting, including Chain-of-Thought (CoT) and Set-of-Mark (SoM) methods, yields marginal gains, revealing a key bottleneck in cross-modal reasoning. Our findings highlight the need for advanced multimodal reasoning and point to future research on multimodal inconsistency."
  },
  {
    "title": "Cross-Model Transferability of Adversarial Patches in Real-time Segmentation for Autonomous Driving",
    "url": "http://arxiv.org/abs/2502.16012v1",
    "arxiv_id": "2502.16012v1",
    "authors": [
      "Prashant Shekhar",
      "Bidur Devkota",
      "Dumindu Samaraweera",
      "Laxima Niure Kandel",
      "Manoj Babu"
    ],
    "published": "2025-02-22T00:03:53+00:00",
    "summary": "Adversarial attacks pose a significant threat to deep learning models, particularly in safety-critical applications like healthcare and autonomous driving. Recently, patch based attacks have demonstrated effectiveness in real-time inference scenarios owing to their 'drag and drop' nature. Following this idea for Semantic Segmentation (SS), here we propose a novel Expectation Over Transformation (EOT) based adversarial patch attack that is more realistic for autonomous vehicles. To effectively train this attack we also propose a 'simplified' loss function that is easy to analyze and implement. Using this attack as our basis, we investigate whether adversarial patches once optimized on a specific SS model, can fool other models or architectures. We conduct a comprehensive cross-model transferability analysis of adversarial patches trained on SOTA Convolutional Neural Network (CNN) models such PIDNet-S, PIDNet-M and PIDNet-L, among others. Additionally, we also include the Segformer model to study transferability to Vision Transformers (ViTs). All of our analysis is conducted on the widely used Cityscapes dataset. Our study reveals key insights into how model architectures (CNN vs CNN or CNN vs. Transformer-based) influence attack susceptibility. In particular, we conclude that although the transferability (effectiveness) of attacks on unseen images of any dimension is really high, the attacks trained against one particular model are minimally effective on other models. And this was found to be true for both ViT and CNN based models. Additionally our results also indicate that for CNN-based models, the repercussions of patch attacks are local, unlike ViTs. Per-class analysis reveals that simple-classes like 'sky' suffer less misclassification than others. The code for the project is available at: https://github.com/p-shekhar/adversarial-patch-transferability"
  },
  {
    "title": "On the Design of Safe Continual RL Methods for Control of Nonlinear Systems",
    "url": "http://arxiv.org/abs/2502.15922v1",
    "arxiv_id": "2502.15922v1",
    "authors": [
      "Austin Coursey",
      "Marcos Quinones-Grueiro",
      "Gautam Biswas"
    ],
    "published": "2025-02-21T20:34:40+00:00",
    "summary": "Reinforcement learning (RL) algorithms have been successfully applied to control tasks associated with unmanned aerial vehicles and robotics. In recent years, safe RL has been proposed to allow the safe execution of RL algorithms in industrial and mission-critical systems that operate in closed loops. However, if the system operating conditions change, such as when an unknown fault occurs in the system, typical safe RL algorithms are unable to adapt while retaining past knowledge. Continual reinforcement learning algorithms have been proposed to address this issue. However, the impact of continual adaptation on the system's safety is an understudied problem. In this paper, we study the intersection of safe and continual RL. First, we empirically demonstrate that a popular continual RL algorithm, online elastic weight consolidation, is unable to satisfy safety constraints in non-linear systems subject to varying operating conditions. Specifically, we study the MuJoCo HalfCheetah and Ant environments with velocity constraints and sudden joint loss non-stationarity. Then, we show that an agent trained using constrained policy optimization, a safe RL algorithm, experiences catastrophic forgetting in continual learning settings. With this in mind, we explore a simple reward-shaping method to ensure that elastic weight consolidation prioritizes remembering both safety and task performance for safety-constrained, non-linear, and non-stationary dynamical systems."
  },
  {
    "title": "VaViM and VaVAM: Autonomous Driving through Video Generative Modeling",
    "url": "http://arxiv.org/abs/2502.15672v1",
    "arxiv_id": "2502.15672v1",
    "authors": [
      "Florent Bartoccioni",
      "Elias Ramzi",
      "Victor Besnier",
      "Shashanka Venkataramanan",
      "Tuan-Hung Vu",
      "Yihong Xu",
      "Loick Chambon",
      "Spyros Gidaris",
      "Serkan Odabas",
      "David Hurych",
      "Renaud Marlet",
      "Alexandre Boulch",
      "Mickael Chen",
      "\u00c9loi Zablocki",
      "Andrei Bursuc",
      "Eduardo Valle",
      "Matthieu Cord"
    ],
    "published": "2025-02-21T18:56:02+00:00",
    "summary": "We explore the potential of large-scale generative video models for autonomous driving, introducing an open-source auto-regressive video model (VaViM) and its companion video-action model (VaVAM) to investigate how video pre-training transfers to real-world driving. VaViM is a simple auto-regressive video model that predicts frames using spatio-temporal token sequences. We show that it captures the semantics and dynamics of driving scenes. VaVAM, the video-action model, leverages the learned representations of VaViM to generate driving trajectories through imitation learning. Together, the models form a complete perception-to-action pipeline. We evaluate our models in open- and closed-loop driving scenarios, revealing that video-based pre-training holds promise for autonomous driving. Key insights include the semantic richness of the learned representations, the benefits of scaling for video synthesis, and the complex relationship between model size, data, and safety metrics in closed-loop evaluations. We release code and model weights at https://github.com/valeoai/VideoActionModel"
  },
  {
    "title": "A Comprehensive Survey on the Trustworthiness of Large Language Models in Healthcare",
    "url": "http://arxiv.org/abs/2502.15871v1",
    "arxiv_id": "2502.15871v1",
    "authors": [
      "Manar Aljohani",
      "Jun Hou",
      "Sindhura Kommu",
      "Xuan Wang"
    ],
    "published": "2025-02-21T18:43:06+00:00",
    "summary": "The application of large language models (LLMs) in healthcare has the potential to revolutionize clinical decision-making, medical research, and patient care. As LLMs are increasingly integrated into healthcare systems, several critical challenges must be addressed to ensure their reliable and ethical deployment. These challenges include truthfulness, where models generate misleading information; privacy, with risks of unintentional data retention; robustness, requiring defenses against adversarial attacks; fairness, addressing biases in clinical outcomes; explainability, ensuring transparent decision-making; and safety, mitigating risks of misinformation and medical errors. Recently, researchers have begun developing benchmarks and evaluation frameworks to systematically assess the trustworthiness of LLMs. However, the trustworthiness of LLMs in healthcare remains underexplored, lacking a systematic review that provides a comprehensive understanding and future insights into this area. This survey bridges this gap by providing a comprehensive overview of the recent research of existing methodologies and solutions aimed at mitigating the above risks in healthcare. By focusing on key trustworthiness dimensions including truthfulness, privacy and safety, robustness, fairness and bias, and explainability, we present a thorough analysis of how these issues impact the reliability and ethical use of LLMs in healthcare. This paper highlights ongoing efforts and offers insights into future research directions to ensure the safe and trustworthy deployment of LLMs in healthcare."
  },
  {
    "title": "Superintelligent Agents Pose Catastrophic Risks: Can Scientist AI Offer a Safer Path?",
    "url": "http://arxiv.org/abs/2502.15657v1",
    "arxiv_id": "2502.15657v1",
    "authors": [
      "Yoshua Bengio",
      "Michael Cohen",
      "Damiano Fornasiere",
      "Joumana Ghosn",
      "Pietro Greiner",
      "Matt MacDermott",
      "S\u00f6ren Mindermann",
      "Adam Oberman",
      "Jesse Richardson",
      "Oliver Richardson",
      "Marc-Antoine Rondeau",
      "Pierre-Luc St-Charles",
      "David Williams-King"
    ],
    "published": "2025-02-21T18:28:36+00:00",
    "summary": "The leading AI companies are increasingly focused on building generalist AI agents -- systems that can autonomously plan, act, and pursue goals across almost all tasks that humans can perform. Despite how useful these systems might be, unchecked AI agency poses significant risks to public safety and security, ranging from misuse by malicious actors to a potentially irreversible loss of human control. We discuss how these risks arise from current AI training methods. Indeed, various scenarios and experiments have demonstrated the possibility of AI agents engaging in deception or pursuing goals that were not specified by human operators and that conflict with human interests, such as self-preservation. Following the precautionary principle, we see a strong need for safer, yet still useful, alternatives to the current agency-driven trajectory. Accordingly, we propose as a core building block for further advances the development of a non-agentic AI system that is trustworthy and safe by design, which we call Scientist AI. This system is designed to explain the world from observations, as opposed to taking actions in it to imitate or please humans. It comprises a world model that generates theories to explain data and a question-answering inference machine. Both components operate with an explicit notion of uncertainty to mitigate the risks of overconfident predictions. In light of these considerations, a Scientist AI could be used to assist human researchers in accelerating scientific progress, including in AI safety. In particular, our system can be employed as a guardrail against AI agents that might be created despite the risks involved. Ultimately, focusing on non-agentic AI may enable the benefits of AI innovation while avoiding the risks associated with the current trajectory. We hope these arguments will motivate researchers, developers, and policymakers to favor this safer path."
  },
  {
    "title": "Superintelligent Agents Pose Catastrophic Risks: Can Scientist AI Offer a Safer Path?",
    "url": "http://arxiv.org/abs/2502.15657v2",
    "arxiv_id": "2502.15657v2",
    "authors": [
      "Yoshua Bengio",
      "Michael Cohen",
      "Damiano Fornasiere",
      "Joumana Ghosn",
      "Pietro Greiner",
      "Matt MacDermott",
      "S\u00f6ren Mindermann",
      "Adam Oberman",
      "Jesse Richardson",
      "Oliver Richardson",
      "Marc-Antoine Rondeau",
      "Pierre-Luc St-Charles",
      "David Williams-King"
    ],
    "published": "2025-02-21T18:28:36+00:00",
    "summary": "The leading AI companies are increasingly focused on building generalist AI agents -- systems that can autonomously plan, act, and pursue goals across almost all tasks that humans can perform. Despite how useful these systems might be, unchecked AI agency poses significant risks to public safety and security, ranging from misuse by malicious actors to a potentially irreversible loss of human control. We discuss how these risks arise from current AI training methods. Indeed, various scenarios and experiments have demonstrated the possibility of AI agents engaging in deception or pursuing goals that were not specified by human operators and that conflict with human interests, such as self-preservation. Following the precautionary principle, we see a strong need for safer, yet still useful, alternatives to the current agency-driven trajectory. Accordingly, we propose as a core building block for further advances the development of a non-agentic AI system that is trustworthy and safe by design, which we call Scientist AI. This system is designed to explain the world from observations, as opposed to taking actions in it to imitate or please humans. It comprises a world model that generates theories to explain data and a question-answering inference machine. Both components operate with an explicit notion of uncertainty to mitigate the risks of overconfident predictions. In light of these considerations, a Scientist AI could be used to assist human researchers in accelerating scientific progress, including in AI safety. In particular, our system can be employed as a guardrail against AI agents that might be created despite the risks involved. Ultimately, focusing on non-agentic AI may enable the benefits of AI innovation while avoiding the risks associated with the current trajectory. We hope these arguments will motivate researchers, developers, and policymakers to favor this safer path."
  },
  {
    "title": "A Simulation Pipeline to Facilitate Real-World Robotic Reinforcement Learning Applications",
    "url": "http://arxiv.org/abs/2502.15649v1",
    "arxiv_id": "2502.15649v1",
    "authors": [
      "Jefferson Silveira",
      "Joshua A. Marshall",
      "Sidney N. Givigi Jr"
    ],
    "published": "2025-02-21T18:16:05+00:00",
    "summary": "Reinforcement learning (RL) has gained traction for its success in solving complex tasks for robotic applications. However, its deployment on physical robots remains challenging due to safety risks and the comparatively high costs of training. To avoid these problems, RL agents are often trained on simulators, which introduces a new problem related to the gap between simulation and reality. This paper presents an RL pipeline designed to help reduce the reality gap and facilitate developing and deploying RL policies for real-world robotic systems. The pipeline organizes the RL training process into an initial step for system identification and three training stages: core simulation training, high-fidelity simulation, and real-world deployment, each adding levels of realism to reduce the sim-to-real gap. Each training stage takes an input policy, improves it, and either passes the improved policy to the next stage or loops it back for further improvement. This iterative process continues until the policy achieves the desired performance. The pipeline's effectiveness is shown through a case study with the Boston Dynamics Spot mobile robot used in a surveillance application. The case study presents the steps taken at each pipeline stage to obtain an RL agent to control the robot's position and orientation."
  },
  {
    "title": "The Relationship Between Reasoning and Performance in Large Language Models -- o3 (mini) Thinks Harder, Not Longer",
    "url": "http://arxiv.org/abs/2502.15631v1",
    "arxiv_id": "2502.15631v1",
    "authors": [
      "Marthe Ballon",
      "Andres Algaba",
      "Vincent Ginis"
    ],
    "published": "2025-02-21T17:59:13+00:00",
    "summary": "Large language models have demonstrated remarkable progress in mathematical reasoning, leveraging chain-of-thought and test-time compute scaling. However, many open questions remain regarding the interplay between reasoning token usage and accuracy gains. In particular, when comparing models across generations, it is unclear whether improved performance results from longer reasoning chains or more efficient reasoning. We systematically analyze chain-of-thought length across o1-mini and o3-mini variants on the Omni-MATH benchmark, finding that o3-mini (m) achieves superior accuracy without requiring longer reasoning chains than o1-mini. Moreover, we show that accuracy generally declines as reasoning chains grow across all models and compute settings, even when controlling for difficulty of the questions. This accuracy drop is significantly smaller in more proficient models, suggesting that new generations of reasoning models use test-time compute more effectively. Finally, we highlight that while o3-mini (h) achieves a marginal accuracy gain over o3-mini (m), it does so by allocating substantially more reasoning tokens across all problems, even the ones that o3-mini (m) can already solve. These findings provide new insights into the relationship between model capability and reasoning length, with implications for efficiency, scaling, and evaluation methodologies."
  },
  {
    "title": "Pick-and-place Manipulation Across Grippers Without Retraining: A Learning-optimization Diffusion Policy Approach",
    "url": "http://arxiv.org/abs/2502.15613v1",
    "arxiv_id": "2502.15613v1",
    "authors": [
      "Xiangtong Yao",
      "Yirui Zhou",
      "Yuan Meng",
      "Liangyu Dong",
      "Lin Hong",
      "Zitao Zhang",
      "Zhenshan Bing",
      "Kai Huang",
      "Fuchun Sun",
      "Alois Knoll"
    ],
    "published": "2025-02-21T17:35:10+00:00",
    "summary": "Current robotic pick-and-place policies typically require consistent gripper configurations across training and inference. This constraint imposes high retraining or fine-tuning costs, especially for imitation learning-based approaches, when adapting to new end-effectors. To mitigate this issue, we present a diffusion-based policy with a hybrid learning-optimization framework, enabling zero-shot adaptation to novel grippers without additional data collection for retraining policy. During training, the policy learns manipulation primitives from demonstrations collected using a base gripper. At inference, a diffusion-based optimization strategy dynamically enforces kinematic and safety constraints, ensuring that generated trajectories align with the physical properties of unseen grippers. This is achieved through a constrained denoising procedure that adapts trajectories to gripper-specific parameters (e.g., tool-center-point offsets, jaw widths) while preserving collision avoidance and task feasibility. We validate our method on a Franka Panda robot across six gripper configurations, including 3D-printed fingertips, flexible silicone gripper, and Robotiq 2F-85 gripper. Our approach achieves a 93.3% average task success rate across grippers (vs. 23.3-26.7% for diffusion policy baselines), supporting tool-center-point variations of 16-23.5 cm and jaw widths of 7.5-11.5 cm. The results demonstrate that constrained diffusion enables robust cross-gripper manipulation while maintaining the sample efficiency of imitation learning, eliminating the need for gripper-specific retraining. Video and code are available at https://github.com/yaoxt3/GADP."
  },
  {
    "title": "SafeInt: Shielding Large Language Models from Jailbreak Attacks via Safety-Aware Representation Intervention",
    "url": "http://arxiv.org/abs/2502.15594v1",
    "arxiv_id": "2502.15594v1",
    "authors": [
      "Jiaqi Wu",
      "Chen Chen",
      "Chunyan Hou",
      "Xiaojie Yuan"
    ],
    "published": "2025-02-21T17:12:35+00:00",
    "summary": "With the widespread real-world deployment of large language models (LLMs), ensuring their behavior complies with safety standards has become crucial. Jailbreak attacks exploit vulnerabilities in LLMs to induce undesirable behavior, posing a significant threat to LLM safety. Previous defenses often fail to achieve both effectiveness and efficiency simultaneously. Defenses from a representation perspective offer new insights, but existing interventions cannot dynamically adjust representations based on the harmfulness of the queries. To address this limitation while ensuring both effectiveness and efficiency, we propose SafeIntervention (SafeInt), a novel defense method that shields LLMs from jailbreak attacks through safety-aware representation intervention. SafeInt is built on our analysis of the representations of jailbreak samples. It adjusts representation distributions of jailbreak samples through intervention to align them with the representations of unsafe samples while minimizing unnecessary perturbations to jailbreak-irrelevant representations. We conduct comprehensive experiments covering six jailbreak attacks, two jailbreak datasets, and two utility benchmarks. Experimental results demonstrate that SafeInt outperforms all baselines in defending LLMs against jailbreak attacks while largely maintaining utility. Additionally, we evaluate SafeInt against adaptive attacks and verify its effectiveness in mitigating real-time attacks."
  },
  {
    "title": "Interpreting and Steering LLMs with Mutual Information-based Explanations on Sparse Autoencoders",
    "url": "http://arxiv.org/abs/2502.15576v1",
    "arxiv_id": "2502.15576v1",
    "authors": [
      "Xuansheng Wu",
      "Jiayi Yuan",
      "Wenlin Yao",
      "Xiaoming Zhai",
      "Ninghao Liu"
    ],
    "published": "2025-02-21T16:36:42+00:00",
    "summary": "Large language models (LLMs) excel at handling human queries, but they can occasionally generate flawed or unexpected responses. Understanding their internal states is crucial for understanding their successes, diagnosing their failures, and refining their capabilities. Although sparse autoencoders (SAEs) have shown promise for interpreting LLM internal representations, limited research has explored how to better explain SAE features, i.e., understanding the semantic meaning of features learned by SAE. Our theoretical analysis reveals that existing explanation methods suffer from the frequency bias issue, where they emphasize linguistic patterns over semantic concepts, while the latter is more critical to steer LLM behaviors. To address this, we propose using a fixed vocabulary set for feature interpretations and designing a mutual information-based objective, aiming to better capture the semantic meaning behind these features. We further propose two runtime steering strategies that adjust the learned feature activations based on their corresponding explanations. Empirical results show that, compared to baselines, our method provides more discourse-level explanations and effectively steers LLM behaviors to defend against jailbreak attacks. These findings highlight the value of explanations for steering LLM behaviors in downstream applications. We will release our code and data once accepted."
  },
  {
    "title": "Model Privacy: A Unified Framework to Understand Model Stealing Attacks and Defenses",
    "url": "http://arxiv.org/abs/2502.15567v1",
    "arxiv_id": "2502.15567v1",
    "authors": [
      "Ganghua Wang",
      "Yuhong Yang",
      "Jie Ding"
    ],
    "published": "2025-02-21T16:29:11+00:00",
    "summary": "The use of machine learning (ML) has become increasingly prevalent in various domains, highlighting the importance of understanding and ensuring its safety. One pressing concern is the vulnerability of ML applications to model stealing attacks. These attacks involve adversaries attempting to recover a learned model through limited query-response interactions, such as those found in cloud-based services or on-chip artificial intelligence interfaces. While existing literature proposes various attack and defense strategies, these often lack a theoretical foundation and standardized evaluation criteria. In response, this work presents a framework called ``Model Privacy'', providing a foundation for comprehensively analyzing model stealing attacks and defenses. We establish a rigorous formulation for the threat model and objectives, propose methods to quantify the goodness of attack and defense strategies, and analyze the fundamental tradeoffs between utility and privacy in ML models. Our developed theory offers valuable insights into enhancing the security of ML models, especially highlighting the importance of the attack-specific structure of perturbations for effective defenses. We demonstrate the application of model privacy from the defender's perspective through various learning scenarios. Extensive experiments corroborate the insights and the effectiveness of defense mechanisms developed under the proposed framework."
  },
  {
    "title": "Estimating Vehicle Speed on Roadways Using RNNs and Transformers: A Video-based Approach",
    "url": "http://arxiv.org/abs/2502.15545v1",
    "arxiv_id": "2502.15545v1",
    "authors": [
      "Sai Krishna Reddy Mareddy",
      "Dhanush Upplapati",
      "Dhanush Kumar Antharam"
    ],
    "published": "2025-02-21T15:51:49+00:00",
    "summary": "This project explores the application of advanced machine learning models, specifically Long Short-Term Memory (LSTM), Gated Recurrent Units (GRU), and Transformers, to the task of vehicle speed estimation using video data. Traditional methods of speed estimation, such as radar and manual systems, are often constrained by high costs, limited coverage, and potential disruptions. In contrast, leveraging existing surveillance infrastructure and cutting-edge neural network architectures presents a non-intrusive, scalable solution. Our approach utilizes LSTM and GRU to effectively manage long-term dependencies within the temporal sequence of video frames, while Transformers are employed to harness their self-attention mechanisms, enabling the processing of entire sequences in parallel and focusing on the most informative segments of the data. This study demonstrates that both LSTM and GRU outperform basic Recurrent Neural Networks (RNNs) due to their advanced gating mechanisms. Furthermore, increasing the sequence length of input data consistently improves model accuracy, highlighting the importance of contextual information in dynamic environments. Transformers, in particular, show exceptional adaptability and robustness across varied sequence lengths and complexities, making them highly suitable for real-time applications in diverse traffic conditions. The findings suggest that integrating these sophisticated neural network models can significantly enhance the accuracy and reliability of automated speed detection systems, thus promising to revolutionize traffic management and road safety."
  },
  {
    "title": "NPB-Rust: NAS Parallel Benchmarks in Rust",
    "url": "http://arxiv.org/abs/2502.15536v1",
    "arxiv_id": "2502.15536v1",
    "authors": [
      "Eduardo M. Martins",
      "Leonardo G. Fa\u00e9",
      "Renato B. Hoffmann",
      "Lucas S. Bianchessi",
      "Dalvan Griebler"
    ],
    "published": "2025-02-21T15:39:29+00:00",
    "summary": "Parallel programming often requires developers to handle complex computational tasks that can yield many errors in its development cycle. Rust is a performant low-level language that promises memory safety guarantees with its compiler, making it an attractive option for HPC application developers. We identified that the Rust ecosystem could benefit from more comprehensive scientific benchmark suites for standardizing comparisons and research. The NAS Parallel Benchmarks (NPB) is a standardized suite for evaluating various hardware aspects and is often used to compare different frameworks for parallelism. Therefore, our contributions are a Rust version of NPB, an analysis of the expressiveness and performance of the language features, and parallelization strategies. We compare our implementation with consolidated sequential and parallel versions of NPB. Experimental results show that Rust's sequential version is 1.23\\% slower than Fortran and 5.59\\% faster than C++, while Rust with Rayon was slower than both Fortran and C++ with OpenMP."
  },
  {
    "title": "Depth-aware Fusion Method based on Image and 4D Radar Spectrum for 3D Object Detection",
    "url": "http://arxiv.org/abs/2502.15516v1",
    "arxiv_id": "2502.15516v1",
    "authors": [
      "Yue Sun",
      "Yeqiang Qian",
      "Chunxiang Wang",
      "Ming Yang"
    ],
    "published": "2025-02-21T15:14:30+00:00",
    "summary": "Safety and reliability are crucial for the public acceptance of autonomous driving. To ensure accurate and reliable environmental perception, intelligent vehicles must exhibit accuracy and robustness in various environments. Millimeter-wave radar, known for its high penetration capability, can operate effectively in adverse weather conditions such as rain, snow, and fog. Traditional 3D millimeter-wave radars can only provide range, Doppler, and azimuth information for objects. Although the recent emergence of 4D millimeter-wave radars has added elevation resolution, the radar point clouds remain sparse due to Constant False Alarm Rate (CFAR) operations. In contrast, cameras offer rich semantic details but are sensitive to lighting and weather conditions. Hence, this paper leverages these two highly complementary and cost-effective sensors, 4D millimeter-wave radar and camera. By integrating 4D radar spectra with depth-aware camera images and employing attention mechanisms, we fuse texture-rich images with depth-rich radar data in the Bird's Eye View (BEV) perspective, enhancing 3D object detection. Additionally, we propose using GAN-based networks to generate depth images from radar spectra in the absence of depth sensors, further improving detection accuracy."
  },
  {
    "title": "A modular risk concept for complex systems",
    "url": "http://arxiv.org/abs/2502.15482v1",
    "arxiv_id": "2502.15482v1",
    "authors": [
      "Dag McGeorge",
      "Jon Arne Glomsrud"
    ],
    "published": "2025-02-21T14:11:16+00:00",
    "summary": "Our ways of managing risk have in the past been adapted to changes in technology and society. Amidst the ongoing digital transformation, the ur-gency of adapting risk management to changing needs seems higher than ever. This paper starts with a brief historic overview of the development of risk management in the past. The paper motivates the views that for com-plex systems, risk should be controlled by enforcing constrains in a modular way at different system levels, that the constraints can be expressed as assur-ance contracts and that acceptable risk mitigation can be demonstrated in as-surance case modules. Based on extensive industry experience of the authors, a major contribution is to explain how already existing methodologies have been combined to cre-ate a concept for modular risk assessment. Examples from assurance of au-tonomous sea navigation and autonomous driving are used to illustrate the concept. Beyond the existing methodologies this paper generalizes risk con-straints to assurance contracts as an enabler of modular risk assessment spanning all relevant system levels and stakeholder perspectives while main-taining the dependencies between the system parts and accounting for emer-gent system behavior. Furthermore, the use of safety integrity levels (SIL) and similar concepts for assigning assurance rigor have been avoided in favor of direct assessment of assurance case argument rigor, because technology and applications change too fast to justify using past experience as evidence of validity of such prescriptive schemes. This paper aims to help practitioners making efficient and timely risk-informed decisions about complex integrated systems."
  },
  {
    "title": "Single-pass Detection of Jailbreaking Input in Large Language Models",
    "url": "http://arxiv.org/abs/2502.15435v1",
    "arxiv_id": "2502.15435v1",
    "authors": [
      "Leyla Naz Candogan",
      "Yongtao Wu",
      "Elias Abad Rocamora",
      "Grigorios G. Chrysos",
      "Volkan Cevher"
    ],
    "published": "2025-02-21T13:04:13+00:00",
    "summary": "Defending aligned Large Language Models (LLMs) against jailbreaking attacks is a challenging problem, with existing approaches requiring multiple requests or even queries to auxiliary LLMs, making them computationally heavy. Instead, we focus on detecting jailbreaking input in a single forward pass. Our method, called Single Pass Detection SPD, leverages the information carried by the logits to predict whether the output sentence will be harmful. This allows us to defend in just one forward pass. SPD can not only detect attacks effectively on open-source models, but also minimizes the misclassification of harmless inputs. Furthermore, we show that SPD remains effective even without complete logit access in GPT-3.5 and GPT-4. We believe that our proposed method offers a promising approach to efficiently safeguard LLMs against adversarial attacks."
  },
  {
    "title": "Pub-Guard-LLM: Detecting Fraudulent Biomedical Articles with Reliable Explanations",
    "url": "http://arxiv.org/abs/2502.15429v1",
    "arxiv_id": "2502.15429v1",
    "authors": [
      "Lihu Chen",
      "Shuojie Fu",
      "Gabriel Freedman",
      "Cemre Zor",
      "Guy Martin",
      "James Kinross",
      "Uddhav Vaghela",
      "Ovidiu Serban",
      "Francesca Toni"
    ],
    "published": "2025-02-21T12:54:56+00:00",
    "summary": "A significant and growing number of published scientific articles is found to involve fraudulent practices, posing a serious threat to the credibility and safety of research in fields such as medicine. We propose Pub-Guard-LLM, the first large language model-based system tailored to fraud detection of biomedical scientific articles. We provide three application modes for deploying Pub-Guard-LLM: vanilla reasoning, retrieval-augmented generation, and multi-agent debate. Each mode allows for textual explanations of predictions. To assess the performance of our system, we introduce an open-source benchmark, PubMed Retraction, comprising over 11K real-world biomedical articles, including metadata and retraction labels. We show that, across all modes, Pub-Guard-LLM consistently surpasses the performance of various baselines and provides more reliable explanations, namely explanations which are deemed more relevant and coherent than those generated by the baselines when evaluated by multiple assessment methods. By enhancing both detection performance and explainability in scientific fraud detection, Pub-Guard-LLM contributes to safeguarding research integrity with a novel, effective, open-source tool."
  },
  {
    "title": "Adversarial Prompt Evaluation: Systematic Benchmarking of Guardrails Against Prompt Input Attacks on LLMs",
    "url": "http://arxiv.org/abs/2502.15427v1",
    "arxiv_id": "2502.15427v1",
    "authors": [
      "Giulio Zizzo",
      "Giandomenico Cornacchia",
      "Kieran Fraser",
      "Muhammad Zaid Hameed",
      "Ambrish Rawat",
      "Beat Buesser",
      "Mark Purcell",
      "Pin-Yu Chen",
      "Prasanna Sattigeri",
      "Kush Varshney"
    ],
    "published": "2025-02-21T12:54:25+00:00",
    "summary": "As large language models (LLMs) become integrated into everyday applications, ensuring their robustness and security is increasingly critical. In particular, LLMs can be manipulated into unsafe behaviour by prompts known as jailbreaks. The variety of jailbreak styles is growing, necessitating the use of external defences known as guardrails. While many jailbreak defences have been proposed, not all defences are able to handle new out-of-distribution attacks due to the narrow segment of jailbreaks used to align them. Moreover, the lack of systematisation around defences has created significant gaps in their practical application. In this work, we perform systematic benchmarking across 15 different defences, considering a broad swathe of malicious and benign datasets. We find that there is significant performance variation depending on the style of jailbreak a defence is subject to. Additionally, we show that based on current datasets available for evaluation, simple baselines can display competitive out-of-distribution performance compared to many state-of-the-art defences. Code is available at https://github.com/IBM/Adversarial-Prompt-Evaluation."
  },
  {
    "title": "A biomechanical comparison of concussion and head acceleration events in elite-level American football and rugby union",
    "url": "http://arxiv.org/abs/2502.15405v1",
    "arxiv_id": "2502.15405v1",
    "authors": [
      "Gregory Tierney"
    ],
    "published": "2025-02-21T12:11:10+00:00",
    "summary": "Elite-level American football and rugby union are two high-contact sports with growing clinical and legal concerns over player safety, necessitating a comparative analysis. A biomechanical comparison of concussion and head acceleration events (HAEs) in elite-level American football and rugby union was undertaken. Rugby union players have a greater number of professional playing years and matches available in a season than their American football counterparts. Rugby union players have a greater number of concussions reported per match and a higher proportion of concussions occurring during training sessions, based on National Football League (NFL) and Rugby Football Union (RFU) injury reports. Preliminary findings indicate that rugby union forwards experience a higher incidence of HAEs per player match over lower and higher magnitude thresholds, than American football defensive players. Overall, elite-level rugby union appears less favourable than American football in in almost all metrics pertinent to concussion and HAE exposure in the biomechanical comparison undertaken. The findings highlight the critical importance of independence, scientific rigour, and transparency in future concussion and HAE biomechanics research and real-world implementation, ensuring the development of more effective mitigation strategies."
  },
  {
    "title": "Evaluating Social Biases in LLM Reasoning",
    "url": "http://arxiv.org/abs/2502.15361v1",
    "arxiv_id": "2502.15361v1",
    "authors": [
      "Xuyang Wu",
      "Jinming Nian",
      "Zhiqiang Tao",
      "Yi Fang"
    ],
    "published": "2025-02-21T10:16:07+00:00",
    "summary": "In the recent development of AI reasoning, large language models (LLMs) are trained to automatically generate chain-of-thought reasoning steps, which have demonstrated compelling performance on math and coding tasks. However, when bias is mixed within the reasoning process to form strong logical arguments, it could cause even more harmful results and further induce hallucinations. In this paper, we have evaluated the 8B and 32B variants of DeepSeek-R1 against their instruction tuned counterparts on the BBQ dataset, and investigated the bias that is elicited out and being amplified through reasoning steps. To the best of our knowledge, this empirical study is the first to assess bias issues in LLM reasoning."
  },
  {
    "title": "Drug-Target Interaction/Affinity Prediction: Deep Learning Models and Advances Review",
    "url": "http://arxiv.org/abs/2502.15346v1",
    "arxiv_id": "2502.15346v1",
    "authors": [
      "Ali Vefghi",
      "Zahed Rahmati",
      "Mohammad Akbari"
    ],
    "published": "2025-02-21T10:00:43+00:00",
    "summary": "Drug discovery remains a slow and expensive process that involves many steps, from detecting the target structure to obtaining approval from the Food and Drug Administration (FDA), and is often riddled with safety concerns. Accurate prediction of how drugs interact with their targets and the development of new drugs by using better methods and technologies have immense potential to speed up this process, ultimately leading to faster delivery of life-saving medications. Traditional methods used for drug-target interaction prediction show limitations, particularly in capturing complex relationships between drugs and their targets. As an outcome, deep learning models have been presented to overcome the challenges of interaction prediction through their precise and efficient end results. By outlining promising research avenues and models, each with a different solution but similar to the problem, this paper aims to give researchers a better idea of methods for even more accurate and efficient prediction of drug-target interaction, ultimately accelerating the development of more effective drugs. A total of 180 prediction methods for drug-target interactions were analyzed throughout the period spanning 2016 to 2025 using different frameworks based on machine learning, mainly deep learning and graph neural networks. Additionally, this paper discusses the novelty, architecture, and input representation of these models."
  },
  {
    "title": "Attention Eclipse: Manipulating Attention to Bypass LLM Safety-Alignment",
    "url": "http://arxiv.org/abs/2502.15334v1",
    "arxiv_id": "2502.15334v1",
    "authors": [
      "Pedram Zaree",
      "Md Abdullah Al Mamun",
      "Quazi Mishkatul Alam",
      "Yue Dong",
      "Ihsen Alouani",
      "Nael Abu-Ghazaleh"
    ],
    "published": "2025-02-21T09:38:00+00:00",
    "summary": "Recent research has shown that carefully crafted jailbreak inputs can induce large language models to produce harmful outputs, despite safety measures such as alignment. It is important to anticipate the range of potential Jailbreak attacks to guide effective defenses and accurate assessment of model safety. In this paper, we present a new approach for generating highly effective Jailbreak attacks that manipulate the attention of the model to selectively strengthen or weaken attention among different parts of the prompt. By harnessing attention loss, we develop more effective jailbreak attacks, that are also transferrable. The attacks amplify the success rate of existing Jailbreak algorithms including GCG, AutoDAN, and ReNeLLM, while lowering their generation cost (for example, the amplified GCG attack achieves 91.2% ASR, vs. 67.9% for the original attack on Llama2-7B/AdvBench, using less than a third of the generation time)."
  },
  {
    "title": "Understand User Opinions of Large Language Models via LLM-Powered In-the-Moment User Experience Interviews",
    "url": "http://arxiv.org/abs/2502.15226v1",
    "arxiv_id": "2502.15226v1",
    "authors": [
      "Mengqiao Liu",
      "Tevin Wang",
      "Cassandra A. Cohen",
      "Sarah Li",
      "Chenyan Xiong"
    ],
    "published": "2025-02-21T05:42:22+00:00",
    "summary": "Which large language model (LLM) is better? Every evaluation tells a story, but what do users really think about current LLMs? This paper presents CLUE, an LLM-powered interviewer that conducts in-the-moment user experience interviews, right after users interacted with LLMs, and automatically gathers insights about user opinions from massive interview logs. We conduct a study with thousands of users to understand user opinions on mainstream LLMs, recruiting users to first chat with a target LLM and then interviewed by CLUE. Our experiments demonstrate that CLUE captures interesting user opinions, for example, the bipolar views on the displayed reasoning process of DeepSeek-R1 and demands for information freshness and multi-modality. Our collected chat-and-interview logs will be released."
  },
  {
    "title": "Discrete implementations of sliding-mode controllers with barrier-function adaptations require a revised framework",
    "url": "http://arxiv.org/abs/2502.15201v1",
    "arxiv_id": "2502.15201v1",
    "authors": [
      "Luis Ovalle",
      "Andr\u00e9s Gonz\u00e1lez",
      "Leonid Fridman",
      "Hernan Haimovich"
    ],
    "published": "2025-02-21T04:29:24+00:00",
    "summary": "Challenges in the discrete implementation of sliding-mode controllers (SMC) with barrier-function-based adaptations are analyzed, revealing fundamental limitations in conventional design frameworks. It is shown that under uniform sampling, the original continuous-time problem motivating these controllers becomes theoretically unsolvable under standard assumptions. To address this incompatibility, a revised control framework is proposed, explicitly incorporating actuator capacity constraints and sampled-data dynamics. Within this structure, the behavior of barrier function-based adaptive controllers (BFASMC) is rigorously examined, explaining their empirical success in digital implementations. A key theoretical result establishes an explicit relation between the actuator capacity, the sampling rate, and the width of the barrier function, providing a principled means to tune these controllers for different application requirements. This relation enables the resolution of various design problems with direct practical implications. A modified BFASMC is then introduced, systematically leveraging sampling effects to ensure finite-time convergence to a positively invariant predefined set, a key advancement for guaranteeing predictable safety margins."
  },
  {
    "title": "OccProphet: Pushing Efficiency Frontier of Camera-Only 4D Occupancy Forecasting with Observer-Forecaster-Refiner Framework",
    "url": "http://arxiv.org/abs/2502.15180v1",
    "arxiv_id": "2502.15180v1",
    "authors": [
      "Junliang Chen",
      "Huaiyuan Xu",
      "Yi Wang",
      "Lap-Pui Chau"
    ],
    "published": "2025-02-21T03:21:48+00:00",
    "summary": "Predicting variations in complex traffic environments is crucial for the safety of autonomous driving. Recent advancements in occupancy forecasting have enabled forecasting future 3D occupied status in driving environments by observing historical 2D images. However, high computational demands make occupancy forecasting less efficient during training and inference stages, hindering its feasibility for deployment on edge agents. In this paper, we propose a novel framework, i.e., OccProphet, to efficiently and effectively learn occupancy forecasting with significantly lower computational requirements while improving forecasting accuracy. OccProphet comprises three lightweight components: Observer, Forecaster, and Refiner. The Observer extracts spatio-temporal features from 3D multi-frame voxels using the proposed Efficient 4D Aggregation with Tripling-Attention Fusion, while the Forecaster and Refiner conditionally predict and refine future occupancy inferences. Experimental results on nuScenes, Lyft-Level5, and nuScenes-Occupancy datasets demonstrate that OccProphet is both training- and inference-friendly. OccProphet reduces 58\\%$\\sim$78\\% of the computational cost with a 2.6$\\times$ speedup compared with the state-of-the-art Cam4DOcc. Moreover, it achieves 4\\%$\\sim$18\\% relatively higher forecasting accuracy. Code and models are publicly available at https://github.com/JLChen-C/OccProphet."
  },
  {
    "title": "CurricuVLM: Towards Safe Autonomous Driving via Personalized Safety-Critical Curriculum Learning with Vision-Language Models",
    "url": "http://arxiv.org/abs/2502.15119v1",
    "arxiv_id": "2502.15119v1",
    "authors": [
      "Zihao Sheng",
      "Zilin Huang",
      "Yansong Qu",
      "Yue Leng",
      "Sruthi Bhavanam",
      "Sikai Chen"
    ],
    "published": "2025-02-21T00:42:40+00:00",
    "summary": "Ensuring safety in autonomous driving systems remains a critical challenge, particularly in handling rare but potentially catastrophic safety-critical scenarios. While existing research has explored generating safety-critical scenarios for autonomous vehicle (AV) testing, there is limited work on effectively incorporating these scenarios into policy learning to enhance safety. Furthermore, developing training curricula that adapt to an AV's evolving behavioral patterns and performance bottlenecks remains largely unexplored. To address these challenges, we propose CurricuVLM, a novel framework that leverages Vision-Language Models (VLMs) to enable personalized curriculum learning for autonomous driving agents. Our approach uniquely exploits VLMs' multimodal understanding capabilities to analyze agent behavior, identify performance weaknesses, and dynamically generate tailored training scenarios for curriculum adaptation. Through comprehensive analysis of unsafe driving situations with narrative descriptions, CurricuVLM performs in-depth reasoning to evaluate the AV's capabilities and identify critical behavioral patterns. The framework then synthesizes customized training scenarios targeting these identified limitations, enabling effective and personalized curriculum learning. Extensive experiments on the Waymo Open Motion Dataset show that CurricuVLM outperforms state-of-the-art baselines across both regular and safety-critical scenarios, achieving superior performance in terms of navigation success, driving efficiency, and safety metrics. Further analysis reveals that CurricuVLM serves as a general approach that can be integrated with various RL algorithms to enhance autonomous driving systems. The code and demo video are available at: https://zihaosheng.github.io/CurricuVLM/."
  },
  {
    "title": "Is Safety Standard Same for Everyone? User-Specific Safety Evaluation of Large Language Models",
    "url": "http://arxiv.org/abs/2502.15086v1",
    "arxiv_id": "2502.15086v1",
    "authors": [
      "Yeonjun In",
      "Wonjoong Kim",
      "Kanghoon Yoon",
      "Sungchul Kim",
      "Mehrab Tanjim",
      "Kibum Kim",
      "Chanyoung Park"
    ],
    "published": "2025-02-20T22:58:44+00:00",
    "summary": "As the use of large language model (LLM) agents continues to grow, their safety vulnerabilities have become increasingly evident. Extensive benchmarks evaluate various aspects of LLM safety by defining the safety relying heavily on general standards, overlooking user-specific standards. However, safety standards for LLM may vary based on a user-specific profiles rather than being universally consistent across all users. This raises a critical research question: Do LLM agents act safely when considering user-specific safety standards? Despite its importance for safe LLM use, no benchmark datasets currently exist to evaluate the user-specific safety of LLMs. To address this gap, we introduce U-SAFEBENCH, the first benchmark designed to assess user-specific aspect of LLM safety. Our evaluation of 18 widely used LLMs reveals current LLMs fail to act safely when considering user-specific safety standards, marking a new discovery in this field. To address this vulnerability, we propose a simple remedy based on chain-of-thought, demonstrating its effectiveness in improving user-specific safety. Our benchmark and code are available at https://github.com/yeonjun-in/U-SafeBench."
  },
  {
    "title": "InterFeedback: Unveiling Interactive Intelligence of Large Multimodal Models via Human Feedback",
    "url": "http://arxiv.org/abs/2502.15027v1",
    "arxiv_id": "2502.15027v1",
    "authors": [
      "Henry Hengyuan Zhao",
      "Wenqi Pei",
      "Yifei Tao",
      "Haiyang Mei",
      "Mike Zheng Shou"
    ],
    "published": "2025-02-20T20:27:06+00:00",
    "summary": "Existing benchmarks do not test Large Multimodal Models (LMMs) on their interactive intelligence with human users which is vital for developing general-purpose AI assistants. We design InterFeedback, an interactive framework, which can be applied to any LMM and dataset to assess this ability autonomously. On top of this, we introduce InterFeedback-Bench which evaluates interactive intelligence using two representative datasets, MMMU-Pro and MathVerse, to test 10 different open-source LMMs. Additionally, we present InterFeedback-Human, a newly collected dataset of 120 cases designed for manually testing interactive performance in leading models such as OpenAI-o1 and Claude-3.5-Sonnet. Our evaluation results show that even state-of-the-art LMM (like OpenAI-o1) can correct their results through human feedback less than 50%. Our findings point to the need for methods that can enhance the LMMs' capability to interpret and benefit from feedback."
  },
  {
    "title": "Auxiliary-Variable Adaptive Control Barrier Functions",
    "url": "http://arxiv.org/abs/2502.15026v1",
    "arxiv_id": "2502.15026v1",
    "authors": [
      "Shuo Liu",
      "Wei Xiao",
      "Calin A. Belta"
    ],
    "published": "2025-02-20T20:23:09+00:00",
    "summary": "This paper addresses the challenge of ensuring safety and feasibility in control systems using Control Barrier Functions (CBFs). Existing CBF-based Quadratic Programs (CBF-QPs) often encounter feasibility issues due to mixed relative degree constraints, input nullification problems, and the presence of tight or time-varying control bounds, which can lead to infeasible solutions and compromised safety. To address these challenges, we propose Auxiliary-Variable Adaptive Control Barrier Functions (AVCBFs), a novel framework that introduces auxiliary functions to dynamically adjust CBF constraints without the need of excessive additional constraints. The AVCBF method ensures that all components of the control input explicitly appear in the desired-order safety constraint, thereby improving feasibility while maintaining safety guarantees. Additionally, we introduce an automatic tuning method that iteratively adjusts AVCBF hyperparameters to ensure feasibility and safety with less conservatism. We demonstrate the effectiveness of the proposed approach in adaptive cruise control and obstacle avoidance scenarios, showing that AVCBFs outperform existing CBF methods by reducing infeasibility and enhancing adaptive safety control under tight or time-varying control bounds."
  },
  {
    "title": "Safe Beyond the Horizon: Efficient Sampling-based MPC with Neural Control Barrier Functions",
    "url": "http://arxiv.org/abs/2502.15006v1",
    "arxiv_id": "2502.15006v1",
    "authors": [
      "Ji Yin",
      "Oswin So",
      "Eric Yang Yu",
      "Chuchu Fan",
      "Panagiotis Tsiotras"
    ],
    "published": "2025-02-20T19:59:11+00:00",
    "summary": "A common problem when using model predictive control (MPC) in practice is the satisfaction of safety specifications beyond the prediction horizon. While theoretical works have shown that safety can be guaranteed by enforcing a suitable terminal set constraint or a sufficiently long prediction horizon, these techniques are difficult to apply and thus are rarely used by practitioners, especially in the case of general nonlinear dynamics. To solve this problem, we impose a tradeoff between exact recursive feasibility, computational tractability, and applicability to ''black-box'' dynamics by learning an approximate discrete-time control barrier function and incorporating it into a variational inference MPC (VIMPC), a sampling-based MPC paradigm. To handle the resulting state constraints, we further propose a new sampling strategy that greatly reduces the variance of the estimated optimal control, improving the sample efficiency, and enabling real-time planning on a CPU. The resulting Neural Shield-VIMPC (NS-VIMPC) controller yields substantial safety improvements compared to existing sampling-based MPC controllers, even under badly designed cost functions. We validate our approach in both simulation and real-world hardware experiments."
  },
  {
    "title": "Probabilistic Robustness in Deep Learning: A Concise yet Comprehensive Guide",
    "url": "http://arxiv.org/abs/2502.14833v1",
    "arxiv_id": "2502.14833v1",
    "authors": [
      "Xingyu Zhao"
    ],
    "published": "2025-02-20T18:47:17+00:00",
    "summary": "Deep learning (DL) has demonstrated significant potential across various safety-critical applications, yet ensuring its robustness remains a key challenge. While adversarial robustness has been extensively studied in worst-case scenarios, probabilistic robustness (PR) offers a more practical perspective by quantifying the likelihood of failures under stochastic perturbations. This paper provides a concise yet comprehensive overview of PR, covering its formal definitions, evaluation and enhancement methods. We introduce a reformulated ''min-max'' optimisation framework for adversarial training specifically designed to improve PR. Furthermore, we explore the integration of PR verification evidence into system-level safety assurance, addressing challenges in translating DL model-level robustness to system-level claims. Finally, we highlight open research questions, including benchmarking PR evaluation methods, extending PR to generative AI tasks, and developing rigorous methodologies and case studies for system-level integration."
  },
  {
    "title": "Logic-RL: Unleashing LLM Reasoning with Rule-Based Reinforcement Learning",
    "url": "http://arxiv.org/abs/2502.14768v1",
    "arxiv_id": "2502.14768v1",
    "authors": [
      "Tian Xie",
      "Zitian Gao",
      "Qingnan Ren",
      "Haoming Luo",
      "Yuqian Hong",
      "Bryan Dai",
      "Joey Zhou",
      "Kai Qiu",
      "Zhirong Wu",
      "Chong Luo"
    ],
    "published": "2025-02-20T17:49:26+00:00",
    "summary": "Inspired by the success of DeepSeek-R1, we explore the potential of rule-based reinforcement learning (RL) in large reasoning models. To analyze reasoning dynamics, we use synthetic logic puzzles as training data due to their controllable complexity and straightforward answer verification. We make some key technical contributions that lead to effective and stable RL training: a system prompt that emphasizes the thinking and answering process, a stringent format reward function that penalizes outputs for taking shortcuts, and a straightforward training recipe that achieves stable convergence. Our 7B model develops advanced reasoning skills-such as reflection, verification, and summarization-that are absent from the logic corpus. Remarkably, after training on just 5K logic problems, it demonstrates generalization abilities to the challenging math benchmarks AIME and AMC."
  },
  {
    "title": "HiddenDetect: Detecting Jailbreak Attacks against Large Vision-Language Models via Monitoring Hidden States",
    "url": "http://arxiv.org/abs/2502.14744v1",
    "arxiv_id": "2502.14744v1",
    "authors": [
      "Yilei Jiang",
      "Xinyan Gao",
      "Tianshuo Peng",
      "Yingshui Tan",
      "Xiaoyong Zhu",
      "Bo Zheng",
      "Xiangyu Yue"
    ],
    "published": "2025-02-20T17:14:34+00:00",
    "summary": "The integration of additional modalities increases the susceptibility of large vision-language models (LVLMs) to safety risks, such as jailbreak attacks, compared to their language-only counterparts. While existing research primarily focuses on post-hoc alignment techniques, the underlying safety mechanisms within LVLMs remain largely unexplored. In this work , we investigate whether LVLMs inherently encode safety-relevant signals within their internal activations during inference. Our findings reveal that LVLMs exhibit distinct activation patterns when processing unsafe prompts, which can be leveraged to detect and mitigate adversarial inputs without requiring extensive fine-tuning. Building on this insight, we introduce HiddenDetect, a novel tuning-free framework that harnesses internal model activations to enhance safety. Experimental results show that {HiddenDetect} surpasses state-of-the-art methods in detecting jailbreak attacks against LVLMs. By utilizing intrinsic safety-aware patterns, our method provides an efficient and scalable solution for strengthening LVLM robustness against multimodal threats. Our code will be released publicly at https://github.com/leigest519/HiddenDetect."
  },
  {
    "title": "HiddenDetect: Detecting Jailbreak Attacks against Large Vision-Language Models via Monitoring Hidden States",
    "url": "http://arxiv.org/abs/2502.14744v2",
    "arxiv_id": "2502.14744v2",
    "authors": [
      "Yilei Jiang",
      "Xinyan Gao",
      "Tianshuo Peng",
      "Yingshui Tan",
      "Xiaoyong Zhu",
      "Bo Zheng",
      "Xiangyu Yue"
    ],
    "published": "2025-02-20T17:14:34+00:00",
    "summary": "The integration of additional modalities increases the susceptibility of large vision-language models (LVLMs) to safety risks, such as jailbreak attacks, compared to their language-only counterparts. While existing research primarily focuses on post-hoc alignment techniques, the underlying safety mechanisms within LVLMs remain largely unexplored. In this work , we investigate whether LVLMs inherently encode safety-relevant signals within their internal activations during inference. Our findings reveal that LVLMs exhibit distinct activation patterns when processing unsafe prompts, which can be leveraged to detect and mitigate adversarial inputs without requiring extensive fine-tuning. Building on this insight, we introduce HiddenDetect, a novel tuning-free framework that harnesses internal model activations to enhance safety. Experimental results show that {HiddenDetect} surpasses state-of-the-art methods in detecting jailbreak attacks against LVLMs. By utilizing intrinsic safety-aware patterns, our method provides an efficient and scalable solution for strengthening LVLM robustness against multimodal threats. Our code will be released publicly at https://github.com/leigest519/HiddenDetect."
  },
  {
    "title": "SuperGPQA: Scaling LLM Evaluation across 285 Graduate Disciplines",
    "url": "http://arxiv.org/abs/2502.14739v1",
    "arxiv_id": "2502.14739v1",
    "authors": [
      "M-A-P Team",
      "Xinrun Du",
      "Yifan Yao",
      "Kaijing Ma",
      "Bingli Wang",
      "Tianyu Zheng",
      "Kang Zhu",
      "Minghao Liu",
      "Yiming Liang",
      "Xiaolong Jin",
      "Zhenlin Wei",
      "Chujie Zheng",
      "Kaixing Deng",
      "Shuyue Guo",
      "Shian Jia",
      "Sichao Jiang",
      "Yiyan Liao",
      "Rui Li",
      "Qinrui Li",
      "Sirun Li",
      "Yizhi Li",
      "Yunwen Li",
      "Dehua Ma",
      "Yuansheng Ni",
      "Haoran Que",
      "Qiyao Wang",
      "Zhoufutu Wen",
      "Siwei Wu",
      "Tianshun Xing",
      "Ming Xu",
      "Zhenzhu Yang",
      "Zekun Moore Wang",
      "Junting Zhou",
      "Yuelin Bai",
      "Xingyuan Bu",
      "Chenglin Cai",
      "Liang Chen",
      "Yifan Chen",
      "Chengtuo Cheng",
      "Tianhao Cheng",
      "Keyi Ding",
      "Siming Huang",
      "Yun Huang",
      "Yaoru Li",
      "Yizhe Li",
      "Zhaoqun Li",
      "Tianhao Liang",
      "Chengdong Lin",
      "Hongquan Lin",
      "Yinghao Ma",
      "Zhongyuan Peng",
      "Zifan Peng",
      "Qige Qi",
      "Shi Qiu",
      "Xingwei Qu",
      "Yizhou Tan",
      "Zili Wang",
      "Chenqing Wang",
      "Hao Wang",
      "Yiya Wang",
      "Yubo Wang",
      "Jiajun Xu",
      "Kexin Yang",
      "Ruibin Yuan",
      "Yuanhao Yue",
      "Tianyang Zhan",
      "Chun Zhang",
      "Jingyang Zhang",
      "Xiyue Zhang",
      "Xingjian Zhang",
      "Yue Zhang",
      "Yongchi Zhao",
      "Xiangyu Zheng",
      "Chenghua Zhong",
      "Yang Gao",
      "Zhoujun Li",
      "Dayiheng Liu",
      "Qian Liu",
      "Tianyu Liu",
      "Shiwen Ni",
      "Junran Peng",
      "Yujia Qin",
      "Wenbo Su",
      "Guoyin Wang",
      "Shi Wang",
      "Jian Yang",
      "Min Yang",
      "Meng Cao",
      "Xiang Yue",
      "Zhaoxiang Zhang",
      "Wangchunshu Zhou",
      "Jiaheng Liu",
      "Qunshu Lin",
      "Wenhao Huang",
      "Ge Zhang"
    ],
    "published": "2025-02-20T17:05:58+00:00",
    "summary": "Large language models (LLMs) have demonstrated remarkable proficiency in mainstream academic disciplines such as mathematics, physics, and computer science. However, human knowledge encompasses over 200 specialized disciplines, far exceeding the scope of existing benchmarks. The capabilities of LLMs in many of these specialized fields-particularly in light industry, agriculture, and service-oriented disciplines-remain inadequately evaluated. To address this gap, we present SuperGPQA, a comprehensive benchmark that evaluates graduate-level knowledge and reasoning capabilities across 285 disciplines. Our benchmark employs a novel Human-LLM collaborative filtering mechanism to eliminate trivial or ambiguous questions through iterative refinement based on both LLM responses and expert feedback. Our experimental results reveal significant room for improvement in the performance of current state-of-the-art LLMs across diverse knowledge domains (e.g., the reasoning-focused model DeepSeek-R1 achieved the highest accuracy of 61.82% on SuperGPQA), highlighting the considerable gap between current model capabilities and artificial general intelligence. Additionally, we present comprehensive insights from our management of a large-scale annotation process, involving over 80 expert annotators and an interactive Human-LLM collaborative system, offering valuable methodological guidance for future research initiatives of comparable scope."
  },
  {
    "title": "PEARL: Towards Permutation-Resilient LLMs",
    "url": "http://arxiv.org/abs/2502.14628v1",
    "arxiv_id": "2502.14628v1",
    "authors": [
      "Liang Chen",
      "Li Shen",
      "Yang Deng",
      "Xiaoyan Zhao",
      "Bin Liang",
      "Kam-Fai Wong"
    ],
    "published": "2025-02-20T15:07:02+00:00",
    "summary": "The in-context learning (ICL) capability of large language models (LLMs) enables them to perform challenging tasks using provided demonstrations. However, ICL is highly sensitive to the ordering of demonstrations, leading to instability in predictions. This paper shows that this vulnerability can be exploited to design a natural attack - difficult for model providers to detect - that achieves nearly 80% success rate on LLaMA-3 by simply permuting the demonstrations. Existing mitigation methods primarily rely on post-processing and fail to enhance the model's inherent robustness to input permutations, raising concerns about safety and reliability of LLMs. To address this issue, we propose Permutation-resilient learning (PEARL), a novel framework based on distributionally robust optimization (DRO), which optimizes model performance against the worst-case input permutation. Specifically, PEARL consists of a permutation-proposal network (P-Net) and the LLM. The P-Net generates the most challenging permutations by treating it as an optimal transport problem, which is solved using an entropy-constrained Sinkhorn algorithm. Through minimax optimization, the P-Net and the LLM iteratively optimize against each other, progressively improving the LLM's robustness. Experiments on synthetic pre-training and real-world instruction tuning tasks demonstrate that PEARL effectively mitigates permutation attacks and enhances performance. Notably, despite being trained on fewer shots and shorter contexts, PEARL achieves performance gains of up to 40% when scaled to many-shot and long-context scenarios, highlighting its efficiency and generalization capabilities."
  },
  {
    "title": "A Stackelberg Game Approach for Signal Temporal Logic Control Synthesis with Uncontrollable Agents",
    "url": "http://arxiv.org/abs/2502.14585v1",
    "arxiv_id": "2502.14585v1",
    "authors": [
      "Bohan Cui",
      "Xinyi Yu",
      "Alessandro Giua",
      "Xiang Yin"
    ],
    "published": "2025-02-20T14:14:42+00:00",
    "summary": "In this paper, we investigate the control synthesis problem for Signal Temporal Logic (STL) specifications in the presence of uncontrollable agents. Existing works mainly address this problem in a robust control setting by assuming the uncontrollable agents are adversarial and accounting for the worst-case scenario. While this approach ensures safety, it can be overly conservative in scenarios where uncontrollable agents have their own objectives that are not entirely opposed to the system's goals. Motivated by this limitation, we propose a new framework for STL control synthesis within the Stackelberg game setting. Specifically, we assume that the system controller, acting as the leader, first commits to a plan, after which the uncontrollable agents, acting as followers, take a best response based on the committed plan and their own objectives. Our goal is to synthesize a control sequence for the leader such that, for any rational followers producing a best response, the leader's STL task is guaranteed to be satisfied. We present an effective solution to this problem by transforming it into a single-stage optimization problem and leveraging counter-example guided synthesis techniques. We demonstrate that the proposed approach is sound and identify conditions under which it is optimal. Simulation results are also provided to illustrate the effectiveness of the proposed framework."
  },
  {
    "title": "Real-world Troublemaker: A Novel Track Testing Framework for Automated Driving Systems in Safety-critical Interaction Scenarios",
    "url": "http://arxiv.org/abs/2502.14574v1",
    "arxiv_id": "2502.14574v1",
    "authors": [
      "Xinrui Zhang",
      "Lu Xiong",
      "Peizhi Zhang",
      "Junpeng Huang",
      "Yining Ma"
    ],
    "published": "2025-02-20T13:59:57+00:00",
    "summary": "Track testing plays a critical role in the safety evaluation of autonomous driving systems (ADS), as it provides real-world object targets and a safety-controllable interaction environment. However, existing track testing scenarios are often pre-fixed and limited, primarily due to the inflexibility of object target control methods and the lack of intelligent interactive behaviors. To overcome this limitation, we propose a novel track testing framework, Real-world Troublemaker, which can generate adversarial object target motion trajectories and facilitate intelligent interactions with the vehicle under test (VUT), creating a more realistic and dynamic testing environment. To enable flexible motion trajectories, cloud-controlled technology is utilized to remotely and dynamically control object targets to create a realistic traffic environment. To achieve intelligent interactions, an interactive concrete scenario generation method is introduced within a game-theoretic structure. The proposed framework has been successfully implemented at the Tongji University Intelligent Connected Vehicle Evaluation Base. Field test results demonstrate that Troublemaker can perform dynamic interactive testing of ADS accurately and effectively. Compared to traditional track testing methods, Troublemaker improves scenario reproduction accuracy by 65.2\\%, increases the diversity of target vehicle interaction strategies by approximately 9.2 times, and enhances exposure frequency of safety-critical scenarios by 3.5 times in unprotected left-turn scenarios."
  },
  {
    "title": "Real-world Troublemaker: A 5G Cloud-controlled Track Testing Framework for Automated Driving Systems in Safety-critical Interaction Scenarios",
    "url": "http://arxiv.org/abs/2502.14574v2",
    "arxiv_id": "2502.14574v2",
    "authors": [
      "Xinrui Zhang",
      "Lu Xiong",
      "Peizhi Zhang",
      "Junpeng Huang",
      "Yining Ma"
    ],
    "published": "2025-02-20T13:59:57+00:00",
    "summary": "Track testing plays a critical role in the safety evaluation of autonomous driving systems (ADS), as it provides a real-world interaction environment. However, the inflexibility in motion control of object targets and the absence of intelligent interactive testing methods often result in pre-fixed and limited testing scenarios. To address these limitations, we propose a novel 5G cloud-controlled track testing framework, Real-world Troublemaker. This framework overcomes the rigidity of traditional pre-programmed control by leveraging 5G cloud-controlled object targets integrated with the Internet of Things (IoT) and vehicle teleoperation technologies. Unlike conventional testing methods that rely on pre-set conditions, we propose a dynamic game strategy based on a quadratic risk interaction utility function, facilitating intelligent interactions with the vehicle under test (VUT) and creating a more realistic and dynamic interaction environment. The proposed framework has been successfully implemented at the Tongji University Intelligent Connected Vehicle Evaluation Base. Field test results demonstrate that Troublemaker can perform dynamic interactive testing of ADS accurately and effectively. Compared to traditional methods, Troublemaker improves scenario reproduction accuracy by 65.2\\%, increases the diversity of interaction strategies by approximately 9.2 times, and enhances exposure frequency of safety-critical scenarios by 3.5 times in unprotected left-turn scenarios."
  },
  {
    "title": "CORBA: Contagious Recursive Blocking Attacks on Multi-Agent Systems Based on Large Language Models",
    "url": "http://arxiv.org/abs/2502.14529v1",
    "arxiv_id": "2502.14529v1",
    "authors": [
      "Zhenhong Zhou",
      "Zherui Li",
      "Jie Zhang",
      "Yuanhe Zhang",
      "Kun Wang",
      "Yang Liu",
      "Qing Guo"
    ],
    "published": "2025-02-20T13:02:00+00:00",
    "summary": "Large Language Model-based Multi-Agent Systems (LLM-MASs) have demonstrated remarkable real-world capabilities, effectively collaborating to complete complex tasks. While these systems are designed with safety mechanisms, such as rejecting harmful instructions through alignment, their security remains largely unexplored. This gap leaves LLM-MASs vulnerable to targeted disruptions. In this paper, we introduce Contagious Recursive Blocking Attacks (Corba), a novel and simple yet highly effective attack that disrupts interactions between agents within an LLM-MAS. Corba leverages two key properties: its contagious nature allows it to propagate across arbitrary network topologies, while its recursive property enables sustained depletion of computational resources. Notably, these blocking attacks often involve seemingly benign instructions, making them particularly challenging to mitigate using conventional alignment methods. We evaluate Corba on two widely-used LLM-MASs, namely, AutoGen and Camel across various topologies and commercial models. Additionally, we conduct more extensive experiments in open-ended interactive LLM-MASs, demonstrating the effectiveness of Corba in complex topology structures and open-source models. Our code is available at: https://github.com/zhrli324/Corba."
  },
  {
    "title": "MLGym: A New Framework and Benchmark for Advancing AI Research Agents",
    "url": "http://arxiv.org/abs/2502.14499v1",
    "arxiv_id": "2502.14499v1",
    "authors": [
      "Deepak Nathani",
      "Lovish Madaan",
      "Nicholas Roberts",
      "Nikolay Bashlykov",
      "Ajay Menon",
      "Vincent Moens",
      "Amar Budhiraja",
      "Despoina Magka",
      "Vladislav Vorotilov",
      "Gaurav Chaurasia",
      "Dieuwke Hupkes",
      "Ricardo Silveira Cabral",
      "Tatiana Shavrina",
      "Jakob Foerster",
      "Yoram Bachrach",
      "William Yang Wang",
      "Roberta Raileanu"
    ],
    "published": "2025-02-20T12:28:23+00:00",
    "summary": "We introduce Meta MLGym and MLGym-Bench, a new framework and benchmark for evaluating and developing LLM agents on AI research tasks. This is the first Gym environment for machine learning (ML) tasks, enabling research on reinforcement learning (RL) algorithms for training such agents. MLGym-bench consists of 13 diverse and open-ended AI research tasks from diverse domains such as computer vision, natural language processing, reinforcement learning, and game theory. Solving these tasks requires real-world AI research skills such as generating new ideas and hypotheses, creating and processing data, implementing ML methods, training models, running experiments, analyzing the results, and iterating through this process to improve on a given task. We evaluate a number of frontier large language models (LLMs) on our benchmarks such as Claude-3.5-Sonnet, Llama-3.1 405B, GPT-4o, o1-preview, and Gemini-1.5 Pro. Our MLGym framework makes it easy to add new tasks, integrate and evaluate models or agents, generate synthetic data at scale, as well as develop new learning algorithms for training agents on AI research tasks. We find that current frontier models can improve on the given baselines, usually by finding better hyperparameters, but do not generate novel hypotheses, algorithms, architectures, or substantial improvements. We open-source our framework and benchmark to facilitate future research in advancing the AI research capabilities of LLM agents."
  },
  {
    "title": "Statistical Scenario Modelling and Lookalike Distributions for Multi-Variate AI Risk",
    "url": "http://arxiv.org/abs/2502.14491v1",
    "arxiv_id": "2502.14491v1",
    "authors": [
      "Elija Perrier"
    ],
    "published": "2025-02-20T12:14:54+00:00",
    "summary": "Evaluating AI safety requires statistically rigorous methods and risk metrics for understanding how the use of AI affects aggregated risk. However, much AI safety literature focuses upon risks arising from AI models in isolation, lacking consideration of how modular use of AI affects risk distribution of workflow components or overall risk metrics. There is also a lack of statistical grounding enabling sensitisation of risk models in the presence of absence of AI to estimate causal contributions of AI. This is in part due to the dearth of AI impact data upon which to fit distributions. In this work, we address these gaps in two ways. First, we demonstrate how scenario modelling (grounded in established statistical techniques such as Markov chains, copulas and Monte Carlo simulation) can be used to model AI risk holistically. Second, we show how lookalike distributions from phenomena analogous to AI can be used to estimate AI impacts in the absence of directly observable data. We demonstrate the utility of our methods for benchmarking cumulative AI risk via risk analysis of a logistic scenario simulations."
  },
  {
    "title": "How Jailbreak Defenses Work and Ensemble? A Mechanistic Investigation",
    "url": "http://arxiv.org/abs/2502.14486v1",
    "arxiv_id": "2502.14486v1",
    "authors": [
      "Zhuohang Long",
      "Siyuan Wang",
      "Shujun Liu",
      "Yuhang Lai",
      "Xuanjing Huang",
      "Zhongyu Wei"
    ],
    "published": "2025-02-20T12:07:40+00:00",
    "summary": "Jailbreak attacks, where harmful prompts bypass generative models' built-in safety, raise serious concerns about model vulnerability. While many defense methods have been proposed, the trade-offs between safety and helpfulness, and their application to Large Vision-Language Models (LVLMs), are not well understood. This paper systematically examines jailbreak defenses by reframing the standard generation task as a binary classification problem to assess model refusal tendencies for both harmful and benign queries. We identify two key defense mechanisms: safety shift, which increases refusal rates across all queries, and harmfulness discrimination, which improves the model's ability to distinguish between harmful and benign inputs. Using these mechanisms, we develop two ensemble defense strategies-inter-mechanism ensembles and intra-mechanism ensembles-to balance safety and helpfulness. Experiments on the MM-SafetyBench and MOSSBench datasets with LLaVA-1.5 models show that these strategies effectively improve model safety or optimize the trade-off between safety and helpfulness."
  },
  {
    "title": "Natural Language Generation",
    "url": "http://arxiv.org/abs/2502.14437v1",
    "arxiv_id": "2502.14437v1",
    "authors": [
      "Ehud Reiter"
    ],
    "published": "2025-02-20T10:41:34+00:00",
    "summary": "This book provides a broad overview of Natural Language Generation (NLG), including technology, user requirements, evaluation, and real-world applications. The focus is on concepts and insights which hopefully will remain relevant for many years, not on the latest LLM innovations. It draws on decades of work by the author and others on NLG.   The book has the following chapters: Introduction to NLG; Rule-Based NLG; Machine Learning and Neural NLG; Requirements; Evaluation; Safety, Maintenance, and Testing; and Applications. All chapters include examples and anecdotes from the author's personal experiences, and end with a Further Reading section.   The book should be especially useful to people working on applied NLG, including NLG researchers, people in other fields who want to use NLG, and commercial developers. It will not however be useful to people who want to understand the latest LLM technology.   There is a companion site with more information at https://ehudreiter.com/book/"
  },
  {
    "title": "Reliable Explainability of Deep Learning Spatial-Spectral Classifiers for Improved Semantic Segmentation in Autonomous Driving",
    "url": "http://arxiv.org/abs/2502.14416v1",
    "arxiv_id": "2502.14416v1",
    "authors": [
      "Jon Guti\u00e9rrez-Zaballa",
      "Koldo Basterretxea",
      "Javier Echanobe"
    ],
    "published": "2025-02-20T10:11:27+00:00",
    "summary": "Integrating hyperspectral imagery (HSI) with deep neural networks (DNNs) can strengthen the accuracy of intelligent vision systems by combining spectral and spatial information, which is useful for tasks like semantic segmentation in autonomous driving. To advance research in such safety-critical systems, determining the precise contribution of spectral information to complex DNNs' output is needed. To address this, several saliency methods, such as class activation maps (CAM), have been proposed primarily for image classification. However, recent studies have raised concerns regarding their reliability. In this paper, we address their limitations and propose an alternative approach by leveraging the data provided by activations and weights from relevant DNN layers to better capture the relationship between input features and predictions. The study aims to assess the superior performance of HSI compared to 3-channel and single-channel DNNs. We also address the influence of spectral signature normalization for enhancing DNN robustness in real-world driving conditions."
  },
  {
    "title": "HPS: Hard Preference Sampling for Human Preference Alignment",
    "url": "http://arxiv.org/abs/2502.14400v1",
    "arxiv_id": "2502.14400v1",
    "authors": [
      "Xiandong Zou",
      "Wanyu Lin",
      "Yuchen Li",
      "Pan Zhou"
    ],
    "published": "2025-02-20T09:37:41+00:00",
    "summary": "Aligning Large Language Model (LLM) responses with human preferences is vital for building safe and controllable AI systems. While preference optimization methods based on Plackett-Luce (PL) and Bradley-Terry (BT) models have shown promise, they face challenges such as poor handling of harmful content, inefficient use of dispreferred responses, and, specifically for PL, high computational costs. To address these issues, we propose Hard Preference Sampling (HPS), a novel framework for robust and efficient human preference alignment. HPS introduces a training loss that prioritizes the most preferred response while rejecting all dispreferred and harmful ones. It emphasizes \"hard\" dispreferred responses--those closely resembling preferred ones--to enhance the model's rejection capabilities. By leveraging a single-sample Monte Carlo sampling strategy, HPS reduces computational overhead while maintaining alignment quality. Theoretically, HPS improves sample efficiency over existing PL methods and maximizes the reward margin between preferred and dispreferred responses, ensuring clearer distinctions. Experiments on HH-RLHF and PKU-Safety datasets validate HPS's effectiveness, achieving comparable BLEU and reward scores while greatly improving reward margins and thus reducing harmful content generation."
  },
  {
    "title": "MPPI-DBaS: Safe Trajectory Optimization with Adaptive Exploration",
    "url": "http://arxiv.org/abs/2502.14387v1",
    "arxiv_id": "2502.14387v1",
    "authors": [
      "Fanxin Wang",
      "Yikun Cheng",
      "Chuyuan Tao"
    ],
    "published": "2025-02-20T09:22:41+00:00",
    "summary": "In trajectory optimization, Model Predictive Path Integral (MPPI) control is a sampling-based Model Predictive Control (MPC) framework that generates optimal inputs by efficiently simulating numerous trajectories. In practice, however, MPPI often struggles to guarantee safety assurance and balance efficient sampling in open spaces with the need for more extensive exploration under tight constraints. To address this challenge, we incorporate discrete barrier states (DBaS) into MPPI and propose a novel MPPI-DBaS algorithm that ensures system safety and enables adaptive exploration across diverse scenarios. We evaluate our method in simulation experiments where the vehicle navigates through closely placed obstacles. The results demonstrate that the proposed algorithm significantly outperforms standard MPPI, achieving a higher success rate and lower tracking errors."
  },
  {
    "title": "S*: Test Time Scaling for Code Generation",
    "url": "http://arxiv.org/abs/2502.14382v1",
    "arxiv_id": "2502.14382v1",
    "authors": [
      "Dacheng Li",
      "Shiyi Cao",
      "Chengkun Cao",
      "Xiuyu Li",
      "Shangyin Tan",
      "Kurt Keutzer",
      "Jiarong Xing",
      "Joseph E. Gonzalez",
      "Ion Stoica"
    ],
    "published": "2025-02-20T09:18:53+00:00",
    "summary": "Increasing test-time compute for LLMs shows promise across domains but remains underexplored in code generation, despite extensive study in math. In this paper, we propose S*, the first hybrid test-time scaling framework that substantially improves the coverage and selection accuracy of generated code. S* extends the existing parallel scaling paradigm with sequential scaling to push performance boundaries. It further leverages a novel selection mechanism that adaptively generates distinguishing inputs for pairwise comparison, combined with execution-grounded information to robustly identify correct solutions. We evaluate across 12 Large Language Models and Large Reasoning Model and show: (1) S* consistently improves performance across model families and sizes, enabling a 3B model to outperform GPT-4o-mini; (2) S* enables non-reasoning models to surpass reasoning models - GPT-4o-mini with S* outperforms o1-preview by 3.7% on LiveCodeBench; (3) S* further boosts state-of-the-art reasoning models - DeepSeek-R1-Distill-Qwen-32B with S* achieves 85.7% on LiveCodeBench, approaching o1 (high) at 88.5%. Code will be available under https://github.com/NovaSky-AI/SkyThought."
  },
  {
    "title": "ChemHTS: Hierarchical Tool Stacking for Enhancing Chemical Agents",
    "url": "http://arxiv.org/abs/2502.14327v1",
    "arxiv_id": "2502.14327v1",
    "authors": [
      "Zhucong Li",
      "Jin Xiao",
      "Bowei Zhang",
      "Zhijian Zhou",
      "Qianyu He",
      "Fenglei Cao",
      "Jiaqing Liang",
      "Yuan Qi"
    ],
    "published": "2025-02-20T07:24:26+00:00",
    "summary": "Large Language Models (LLMs) have demonstrated remarkable potential in scientific research, particularly in chemistry-related tasks such as molecular design, reaction prediction, and property estimation. While tool-augmented LLMs have been introduced to enhance reasoning and computation in these domains, existing approaches suffer from tool invocation errors and lack effective collaboration among diverse tools, limiting their overall performance. To address these challenges, we propose ChemHTS (Chemical Hierarchical Tool Stacking), a novel method that optimizes tool invocation pathways through a hierarchical stacking strategy. ChemHTS consists of two key stages: tool self-stacking warmup and multi-layer decision optimization, enabling LLMs to refine tool usage dynamically. We evaluate ChemHTS across four classical chemistry tasks and demonstrate its superiority over strong baselines, including GPT-4o, DeepSeek-R1, and chemistry-specific models, including ChemDFM. Furthermore, we define four distinct tool-stacking behaviors to enhance interpretability, providing insights into the effectiveness of tool collaboration. Our dataset and code are publicly available at \\url{https://github.com/Chang-pw/ChemHTS}."
  },
  {
    "title": "MedHallu: A Comprehensive Benchmark for Detecting Medical Hallucinations in Large Language Models",
    "url": "http://arxiv.org/abs/2502.14302v1",
    "arxiv_id": "2502.14302v1",
    "authors": [
      "Shrey Pandit",
      "Jiawei Xu",
      "Junyuan Hong",
      "Zhangyang Wang",
      "Tianlong Chen",
      "Kaidi Xu",
      "Ying Ding"
    ],
    "published": "2025-02-20T06:33:23+00:00",
    "summary": "Advancements in Large Language Models (LLMs) and their increasing use in medical question-answering necessitate rigorous evaluation of their reliability. A critical challenge lies in hallucination, where models generate plausible yet factually incorrect outputs. In the medical domain, this poses serious risks to patient safety and clinical decision-making. To address this, we introduce MedHallu, the first benchmark specifically designed for medical hallucination detection. MedHallu comprises 10,000 high-quality question-answer pairs derived from PubMedQA, with hallucinated answers systematically generated through a controlled pipeline. Our experiments show that state-of-the-art LLMs, including GPT-4o, Llama-3.1, and the medically fine-tuned UltraMedical, struggle with this binary hallucination detection task, with the best model achieving an F1 score as low as 0.625 for detecting \"hard\" category hallucinations. Using bidirectional entailment clustering, we show that harder-to-detect hallucinations are semantically closer to ground truth. Through experiments, we also show incorporating domain-specific knowledge and introducing a \"not sure\" category as one of the answer categories improves the precision and F1 scores by up to 38% relative to baselines."
  },
  {
    "title": "SEA-HELM: Southeast Asian Holistic Evaluation of Language Models",
    "url": "http://arxiv.org/abs/2502.14301v1",
    "arxiv_id": "2502.14301v1",
    "authors": [
      "Yosephine Susanto",
      "Adithya Venkatadri Hulagadri",
      "Jann Railey Montalan",
      "Jian Gang Ngui",
      "Xian Bin Yong",
      "Weiqi Leong",
      "Hamsawardhini Rengarajan",
      "Peerat Limkonchotiwat",
      "Yifan Mai",
      "William Chandra Tjhi"
    ],
    "published": "2025-02-20T06:32:45+00:00",
    "summary": "With the rapid emergence of novel capabilities in Large Language Models (LLMs), the need for rigorous multilingual and multicultural benchmarks that are integrated has become more pronounced. Though existing LLM benchmarks are capable of evaluating specific capabilities of LLMs in English as well as in various mid- to low-resource languages, including those in the Southeast Asian (SEA) region, a comprehensive and authentic evaluation suite for the SEA languages has not been developed thus far. Here, we present SEA-HELM, a holistic linguistic and cultural LLM evaluation suite that emphasizes SEA languages, comprising five core pillars: (1) NLP Classics, (2) LLM-specifics, (3) SEA Linguistics, (4) SEA Culture, (5) Safety. SEA-HELM currently supports Filipino, Indonesian, Tamil, Thai, and Vietnamese. We also introduce the SEA-HELM leaderboard, which allows users to understand models' multilingual and multicultural performance in a systematic and user-friendly manner."
  },
  {
    "title": "Cytogenetic, Hematobiochemical, and Histopathological Assessment of Albino Rats (Rattus norvegicus) Fed on Gluten Extracts",
    "url": "http://arxiv.org/abs/2502.14929v1",
    "arxiv_id": "2502.14929v1",
    "authors": [
      "Tajudeen Yahaya",
      "Esther Oladele",
      "Ufuoma Shemishere",
      "Daniel Anyebe",
      "Haliru Abdullahi",
      "Maryam Lawal",
      "Rufai Ahmad"
    ],
    "published": "2025-02-20T05:44:36+00:00",
    "summary": "Background: Literature shows that most of the information on the toxicity of gluten is generated from survey and observational studies, resulting in inconsistent outcomes and a decrease in the acceptability of gluten-rich foods. To determine gluten's safety, an in-depth in vitro and in vivo toxicological examination is required. This enables scientists to come up with ameliorative strategies if it turns out to have side effects, and consumers' trust can be restored. Objectives: The objective of this study was to assess the toxicity of gluten extracts on albino rats (Rattus norvegicus). Materials and Methods: Twenty-four rats were randomly selected and divided into four groups, each comprising six rats. Group 1 (control) rats were fed on pellet feeds and groups 2, 3, and 4 were fed on daily dosages of 0.5, 1.0, and 1.5 g gluten extracts, respectively. The rats' body weights and reactions were observed for 90 days before blood samples were collected for hematobiochemical and micronucleus tests. Histopathological examinations of the liver and kidneys were also performed. Results: There was no difference (P > 0.05) in body weight, blood glucose level, or micronuclei between the control and treated rats. The lymphocytes, alkaline phosphatase, alanine transaminase, total protein, and calcium ions of the test rats were all significantly (P < 0.05) altered but remained within the normal ranges. Other hematobiochemical parameters, including packed cell volume, hemoglobin, white and red blood cells, aspartate transaminase, albumin, sodium ions, potassium ions, chloride ions, and urea, revealed no marked changes. The treated rats' livers and kidneys showed no histopathological changes. Conclusion: Gluten had no adverse effects. However, it altered hematobiochemical parameters, particularly the lymphocytes, alkaline phosphatase, alanine transaminase, total protein, and calcium ions."
  },
  {
    "title": "No Minima, No Collisions: Combining Modulation and Control Barrier Function Strategies for Feasible Dynamical Collision Avoidance",
    "url": "http://arxiv.org/abs/2502.14238v1",
    "arxiv_id": "2502.14238v1",
    "authors": [
      "Yifan Xue",
      "Nadia Figueroa"
    ],
    "published": "2025-02-20T04:07:18+00:00",
    "summary": "As prominent real-time safety-critical reactive control techniques, Control Barrier Function Quadratic Programs (CBF-QPs) work for control affine systems in general but result in local minima in the generated trajectories and consequently cannot ensure convergence to the goals. Contrarily, Modulation of Dynamical Systems (Mod-DSs), including normal, reference, and on-manifold Mod-DS, achieve obstacle avoidance with few and even no local minima but have trouble optimally minimizing the difference between the constrained and the unconstrained controller outputs, and its applications are limited to fully-actuated systems. We dive into the theoretical foundations of CBF-QP and Mod-DS, proving that despite their distinct origins, normal Mod-DS is a special case of CBF-QP, and reference Mod-DS's solutions are mathematically connected to that of the CBF-QP through one equation. Building on top of the unveiled theoretical connections between CBF-QP and Mod-DS, reference Mod-based CBF-QP and on-manifold Mod-based CBF-QP controllers are proposed to combine the strength of CBF-QP and Mod-DS approaches and realize local-minimum-free reactive obstacle avoidance for control affine systems in general. We validate our methods in both simulated hospital environments and real-world experiments using Ridgeback for fully-actuated systems and Fetch robots for underactuated systems. Mod-based CBF-QPs outperform CBF-QPs as well as the optimally constrained-enforcing Mod-DS approaches we proposed in all experiments."
  },
  {
    "title": "OBELiX: A Curated Dataset of Crystal Structures and Experimentally Measured Ionic Conductivities for Lithium Solid-State Electrolytes",
    "url": "http://arxiv.org/abs/2502.14234v1",
    "arxiv_id": "2502.14234v1",
    "authors": [
      "F\u00e9lix Therrien",
      "Jamal Abou Haibeh",
      "Divya Sharma",
      "Rhiannon Hendley",
      "Alex Hern\u00e1ndez-Garc\u00eda",
      "Sun Sun",
      "Alain Tchagang",
      "Jiang Su",
      "Samuel Huberman",
      "Yoshua Bengio",
      "Hongyu Guo",
      "Homin Shin"
    ],
    "published": "2025-02-20T03:59:35+00:00",
    "summary": "Solid-state electrolyte batteries are expected to replace liquid electrolyte lithium-ion batteries in the near future thanks to their higher theoretical energy density and improved safety. However, their adoption is currently hindered by their lower effective ionic conductivity, a quantity that governs charge and discharge rates. Identifying highly ion-conductive materials using conventional theoretical calculations and experimental validation is both time-consuming and resource-intensive. While machine learning holds the promise to expedite this process, relevant ionic conductivity and structural data is scarce. Here, we present OBELiX, a domain-expert-curated database of $\\sim$600 synthesized solid electrolyte materials and their experimentally measured room temperature ionic conductivities gathered from literature. Each material is described by their measured composition, space group and lattice parameters. A full-crystal description in the form of a crystallographic information file (CIF) is provided for ~320 structures for which atomic positions were available. We discuss various statistics and features of the dataset and provide training and testing splits that avoid data leakage. Finally, we benchmark seven existing ML models on the task of predicting ionic conductivity and discuss their performance. The goal of this work is to facilitate the use of machine learning for solid-state electrolyte materials discovery."
  },
  {
    "title": "Do LLMs Consider Security? An Empirical Study on Responses to Programming Questions",
    "url": "http://arxiv.org/abs/2502.14202v1",
    "arxiv_id": "2502.14202v1",
    "authors": [
      "Amirali Sajadi",
      "Binh Le",
      "Anh Nguyen",
      "Kostadin Damevski",
      "Preetha Chatterjee"
    ],
    "published": "2025-02-20T02:20:06+00:00",
    "summary": "The widespread adoption of conversational LLMs for software development has raised new security concerns regarding the safety of LLM-generated content. Our motivational study outlines ChatGPT's potential in volunteering context-specific information to the developers, promoting safe coding practices. Motivated by this finding, we conduct a study to evaluate the degree of security awareness exhibited by three prominent LLMs: Claude 3, GPT-4, and Llama 3. We prompt these LLMs with Stack Overflow questions that contain vulnerable code to evaluate whether they merely provide answers to the questions or if they also warn users about the insecure code, thereby demonstrating a degree of security awareness. Further, we assess whether LLM responses provide information about the causes, exploits, and the potential fixes of the vulnerability, to help raise users' awareness. Our findings show that all three models struggle to accurately detect and warn users about vulnerabilities, achieving a detection rate of only 12.6% to 40% across our datasets. We also observe that the LLMs tend to identify certain types of vulnerabilities related to sensitive information exposure and improper input neutralization much more frequently than other types, such as those involving external control of file names or paths. Furthermore, when LLMs do issue security warnings, they often provide more information on the causes, exploits, and fixes of vulnerabilities compared to Stack Overflow responses. Finally, we provide an in-depth discussion on the implications of our findings and present a CLI-based prompting tool that can be used to generate significantly more secure LLM responses."
  },
  {
    "title": "Multimodal RewardBench: Holistic Evaluation of Reward Models for Vision Language Models",
    "url": "http://arxiv.org/abs/2502.14191v1",
    "arxiv_id": "2502.14191v1",
    "authors": [
      "Michihiro Yasunaga",
      "Luke Zettlemoyer",
      "Marjan Ghazvininejad"
    ],
    "published": "2025-02-20T01:48:13+00:00",
    "summary": "Reward models play an essential role in training vision-language models (VLMs) by assessing output quality to enable aligning with human preferences. Despite their importance, the research community lacks comprehensive open benchmarks for evaluating multimodal reward models in VLMs. To address this gap, we introduce Multimodal RewardBench, an expert-annotated benchmark covering six domains: general correctness, preference, knowledge, reasoning, safety, and visual question-answering. Our dataset comprises 5,211 annotated (prompt, chosen response, rejected response) triplets collected from various VLMs. In evaluating a range of VLM judges, we find that even the top-performing models, Gemini 1.5 Pro and Claude 3.5 Sonnet, achieve only 72% overall accuracy. Notably, most models struggle in the reasoning and safety domains. These findings suggest that Multimodal RewardBench offers a challenging testbed for advancing reward model development across multiple domains. We release the benchmark at https://github.com/facebookresearch/multimodal_rewardbench."
  },
  {
    "title": "Type 1 Diabetes Management using GLIMMER: Glucose Level Indicator Model with Modified Error Rate",
    "url": "http://arxiv.org/abs/2502.14183v1",
    "arxiv_id": "2502.14183v1",
    "authors": [
      "Saman Khamesian",
      "Asiful Arefeen",
      "Adela Grando",
      "Bithika Thompson",
      "Hassan Ghasemzadeh"
    ],
    "published": "2025-02-20T01:26:00+00:00",
    "summary": "Managing Type 1 Diabetes (T1D) demands constant vigilance as individuals strive to regulate their blood glucose levels to avert the dangers of dysglycemia (hyperglycemia or hypoglycemia). Despite the advent of sophisticated technologies such as automated insulin delivery (AID) systems, achieving optimal glycemic control remains a formidable task. AID systems integrate continuous subcutaneous insulin infusion (CSII) and continuous glucose monitors (CGM) data, offering promise in reducing variability and increasing glucose time-in-range. However, these systems often fail to prevent dysglycemia, partly due to limitations in prediction algorithms that lack the precision to avert abnormal glucose events. This gap highlights the need for proactive behavioral adjustments. We address this need with GLIMMER, Glucose Level Indicator Model with Modified Error Rate, a machine learning approach for forecasting blood glucose levels. GLIMMER categorizes glucose values into normal and abnormal ranges and devises a novel custom loss function to prioritize accuracy in dysglycemic events where patient safety is critical. To evaluate the potential of GLIMMER for T1D management, we both use a publicly available dataset and collect new data involving 25 patients with T1D. In predicting next-hour glucose values, GLIMMER achieved a root mean square error (RMSE) of 23.97 (+/-3.77) and a mean absolute error (MAE) of 15.83 (+/-2.09) mg/dL. These results reflect a 23% improvement in RMSE and a 31% improvement in MAE compared to the best-reported error rates."
  },
  {
    "title": "Multi-Faceted Studies on Data Poisoning can Advance LLM Development",
    "url": "http://arxiv.org/abs/2502.14182v1",
    "arxiv_id": "2502.14182v1",
    "authors": [
      "Pengfei He",
      "Yue Xing",
      "Han Xu",
      "Zhen Xiang",
      "Jiliang Tang"
    ],
    "published": "2025-02-20T01:19:51+00:00",
    "summary": "The lifecycle of large language models (LLMs) is far more complex than that of traditional machine learning models, involving multiple training stages, diverse data sources, and varied inference methods. While prior research on data poisoning attacks has primarily focused on the safety vulnerabilities of LLMs, these attacks face significant challenges in practice. Secure data collection, rigorous data cleaning, and the multistage nature of LLM training make it difficult to inject poisoned data or reliably influence LLM behavior as intended. Given these challenges, this position paper proposes rethinking the role of data poisoning and argue that multi-faceted studies on data poisoning can advance LLM development. From a threat perspective, practical strategies for data poisoning attacks can help evaluate and address real safety risks to LLMs. From a trustworthiness perspective, data poisoning can be leveraged to build more robust LLMs by uncovering and mitigating hidden biases, harmful outputs, and hallucinations. Moreover, from a mechanism perspective, data poisoning can provide valuable insights into LLMs, particularly the interplay between data and model behavior, driving a deeper understanding of their underlying mechanisms."
  },
  {
    "title": "On the logical skills of large language models: evaluations using arbitrarily complex first-order logic problems",
    "url": "http://arxiv.org/abs/2502.14180v1",
    "arxiv_id": "2502.14180v1",
    "authors": [
      "Shokhrukh Ibragimov",
      "Arnulf Jentzen",
      "Benno Kuckuck"
    ],
    "published": "2025-02-20T01:18:24+00:00",
    "summary": "We present a method of generating first-order logic statements whose complexity can be controlled along multiple dimensions. We use this method to automatically create several datasets consisting of questions asking for the truth or falsity of first-order logic statements in Zermelo-Fraenkel set theory. While the resolution of these questions does not require any knowledge beyond basic notation of first-order logic and set theory, it does require a degree of planning and logical reasoning, which can be controlled up to arbitrarily high difficulty by the complexity of the generated statements. Furthermore, we do extensive evaluations of the performance of various large language models, including recent models such as DeepSeek-R1 and OpenAI's o3-mini, on these datasets. All of the datasets along with the code used for generating them, as well as all data from the evaluations is publicly available at https://github.com/bkuckuck/logical-skills-of-llms."
  },
  {
    "title": "Multi-Agent Risks from Advanced AI",
    "url": "http://arxiv.org/abs/2502.14143v1",
    "arxiv_id": "2502.14143v1",
    "authors": [
      "Lewis Hammond",
      "Alan Chan",
      "Jesse Clifton",
      "Jason Hoelscher-Obermaier",
      "Akbir Khan",
      "Euan McLean",
      "Chandler Smith",
      "Wolfram Barfuss",
      "Jakob Foerster",
      "Tom\u00e1\u0161 Gaven\u010diak",
      "The Anh Han",
      "Edward Hughes",
      "Vojt\u011bch Kova\u0159\u00edk",
      "Jan Kulveit",
      "Joel Z. Leibo",
      "Caspar Oesterheld",
      "Christian Schroeder de Witt",
      "Nisarg Shah",
      "Michael Wellman",
      "Paolo Bova",
      "Theodor Cimpeanu",
      "Carson Ezell",
      "Quentin Feuillade-Montixi",
      "Matija Franklin",
      "Esben Kran",
      "Igor Krawczuk",
      "Max Lamparth",
      "Niklas Lauffer",
      "Alexander Meinke",
      "Sumeet Motwani",
      "Anka Reuel",
      "Vincent Conitzer",
      "Michael Dennis",
      "Iason Gabriel",
      "Adam Gleave",
      "Gillian Hadfield",
      "Nika Haghtalab",
      "Atoosa Kasirzadeh",
      "S\u00e9bastien Krier",
      "Kate Larson",
      "Joel Lehman",
      "David C. Parkes",
      "Georgios Piliouras",
      "Iyad Rahwan"
    ],
    "published": "2025-02-19T23:03:21+00:00",
    "summary": "The rapid development of advanced AI agents and the imminent deployment of many instances of these agents will give rise to multi-agent systems of unprecedented complexity. These systems pose novel and under-explored risks. In this report, we provide a structured taxonomy of these risks by identifying three key failure modes (miscoordination, conflict, and collusion) based on agents' incentives, as well as seven key risk factors (information asymmetries, network effects, selection pressures, destabilising dynamics, commitment problems, emergent agency, and multi-agent security) that can underpin them. We highlight several important instances of each risk, as well as promising directions to help mitigate them. By anchoring our analysis in a range of real-world examples and experimental evidence, we illustrate the distinct challenges posed by multi-agent systems and their implications for the safety, governance, and ethics of advanced AI."
  },
  {
    "title": "PedDet: Adaptive Spectral Optimization for Multimodal Pedestrian Detection",
    "url": "http://arxiv.org/abs/2502.14063v1",
    "arxiv_id": "2502.14063v1",
    "authors": [
      "Rui Zhao",
      "Zeyu Zhang",
      "Yi Xu",
      "Yi Yao",
      "Yan Huang",
      "Wenxin Zhang",
      "Zirui Song",
      "Xiuying Chen",
      "Yang Zhao"
    ],
    "published": "2025-02-19T19:31:51+00:00",
    "summary": "Pedestrian detection in intelligent transportation systems has made significant progress but faces two critical challenges: (1) insufficient fusion of complementary information between visible and infrared spectra, particularly in complex scenarios, and (2) sensitivity to illumination changes, such as low-light or overexposed conditions, leading to degraded performance. To address these issues, we propose PedDet, an adaptive spectral optimization complementarity framework specifically enhanced and optimized for multispectral pedestrian detection. PedDet introduces the Multi-scale Spectral Feature Perception Module (MSFPM) to adaptively fuse visible and infrared features, enhancing robustness and flexibility in feature extraction. Additionally, the Illumination Robustness Feature Decoupling Module (IRFDM) improves detection stability under varying lighting by decoupling pedestrian and background features. We further design a contrastive alignment to enhance intermodal feature discrimination. Experiments on LLVIP and MSDS datasets demonstrate that PedDet achieves state-of-the-art performance, improving the mAP by 6.6% with superior detection accuracy even in low-light conditions, marking a significant step forward for road safety. Code will be available at https://github.com/AIGeeksGroup/PedDet."
  },
  {
    "title": "Asking for Help Enables Safety Guarantees Without Sacrificing Effectiveness",
    "url": "http://arxiv.org/abs/2502.14043v1",
    "arxiv_id": "2502.14043v1",
    "authors": [
      "Benjamin Plaut",
      "Juan Li\u00e9vano-Karim",
      "Stuart Russell"
    ],
    "published": "2025-02-19T19:01:39+00:00",
    "summary": "Most reinforcement learning algorithms with regret guarantees rely on a critical assumption: that all errors are recoverable. Recent work by Plaut et al. discarded this assumption and presented algorithms that avoid \"catastrophe\" (i.e., irreparable errors) by asking for help. However, they provided only safety guarantees and did not consider reward maximization. We prove that any algorithm that avoids catastrophe in their setting also guarantees high reward (i.e., sublinear regret) in any Markov Decision Process (MDP), including MDPs with irreversible costs. This constitutes the first no-regret guarantee for general MDPs. More broadly, our result may be the first formal proof that it is possible for an agent to obtain high reward while becoming self-sufficient in an unknown, unbounded, and high-stakes environment without causing catastrophe or requiring resets."
  },
  {
    "title": "Why Safeguarded Ships Run Aground? Aligned Large Language Models' Safety Mechanisms Tend to Be Anchored in The Template Region",
    "url": "http://arxiv.org/abs/2502.13946v1",
    "arxiv_id": "2502.13946v1",
    "authors": [
      "Chak Tou Leong",
      "Qingyu Yin",
      "Jian Wang",
      "Wenjie Li"
    ],
    "published": "2025-02-19T18:42:45+00:00",
    "summary": "The safety alignment of large language models (LLMs) remains vulnerable, as their initial behavior can be easily jailbroken by even relatively simple attacks. Since infilling a fixed template between the input instruction and initial model output is a common practice for existing LLMs, we hypothesize that this template is a key factor behind their vulnerabilities: LLMs' safety-related decision-making overly relies on the aggregated information from the template region, which largely influences these models' safety behavior. We refer to this issue as template-anchored safety alignment. In this paper, we conduct extensive experiments and verify that template-anchored safety alignment is widespread across various aligned LLMs. Our mechanistic analyses demonstrate how it leads to models' susceptibility when encountering inference-time jailbreak attacks. Furthermore, we show that detaching safety mechanisms from the template region is promising in mitigating vulnerabilities to jailbreak attacks. We encourage future research to develop more robust safety alignment techniques that reduce reliance on the template region."
  },
  {
    "title": "Learning to explore when mistakes are not allowed",
    "url": "http://arxiv.org/abs/2502.13801v1",
    "arxiv_id": "2502.13801v1",
    "authors": [
      "Charly Pecqueux-Gu\u00e9z\u00e9nec",
      "St\u00e9phane Doncieux",
      "Nicolas Perrin-Gilbert"
    ],
    "published": "2025-02-19T15:11:51+00:00",
    "summary": "Goal-Conditioned Reinforcement Learning (GCRL) provides a versatile framework for developing unified controllers capable of handling wide ranges of tasks, exploring environments, and adapting behaviors. However, its reliance on trial-and-error poses challenges for real-world applications, as errors can result in costly and potentially damaging consequences. To address the need for safer learning, we propose a method that enables agents to learn goal-conditioned behaviors that explore without the risk of making harmful mistakes. Exploration without risks can seem paradoxical, but environment dynamics are often uniform in space, therefore a policy trained for safety without exploration purposes can still be exploited globally. Our proposed approach involves two distinct phases. First, during a pretraining phase, we employ safe reinforcement learning and distributional techniques to train a safety policy that actively tries to avoid failures in various situations. In the subsequent safe exploration phase, a goal-conditioned (GC) policy is learned while ensuring safety. To achieve this, we implement an action-selection mechanism leveraging the previously learned distributional safety critics to arbitrate between the safety policy and the GC policy, ensuring safe exploration by switching to the safety policy when needed. We evaluate our method in simulated environments and demonstrate that it not only provides substantial coverage of the goal space but also reduces the occurrence of mistakes to a minimum, in stark contrast to traditional GCRL approaches. Additionally, we conduct an ablation study and analyze failure modes, offering insights for future research directions."
  },
  {
    "title": "User Association and Coordinated Beamforming in Cognitive Aerial-Terrestrial Networks: A Safe Reinforcement Learning Approach",
    "url": "http://arxiv.org/abs/2502.13663v1",
    "arxiv_id": "2502.13663v1",
    "authors": [
      "Zizhen Zhou",
      "Jungang Ge",
      "Ying-Chang Liang"
    ],
    "published": "2025-02-19T12:15:32+00:00",
    "summary": "Cognitive aerial-terrestrial networks (CATNs) offer a solution to spectrum scarcity by sharing spectrum between aerial and terrestrial networks. However, aerial users (AUs) experience significant interference from numerous terrestrial base stations (BSs). To alleviate such interference, we investigate a user association and coordinated beamforming (CBF) problem in CATN, where the aerial network serves as the primary network sharing its spectrum with the terrestrial network. Specifically, we maximize the sum rate of the secondary terrestrial users (TUs) under the interference temperature constraints of the AUs. Traditional iterative optimization schemes are impractical due to their high computational complexity and information exchange overhead. Although deep reinforcement learning (DRL) based schemes can address these challenges, their performance is sensitive to the weights of the weighted penalty terms for violating constraints in the reward function. Motivated by these issues, we propose a safe DRL-based user association and CBF scheme for CATN, eliminating the need for training multiple times to find the optimal penalty weight before actual deployment. Specifically, the CATN is modeled as a networked constrained partially observable Markov game. Each TU acts as an agent to choose its associated BS, and each BS acts as an agent to decide its beamforming vectors, aiming to maximize the reward while satisfying the safety constraints introduced by the interference constraints of the AUs. By exploiting a safe DRL algorithm, the proposed scheme incurs lower deployment expenses than the penalty-based DRL schemes since only one training is required before actual deployment. Simulation results show that the proposed scheme can achieve a higher sum rate of TUs than a two-stage optimization scheme while the average received interference power of the AUs is generally below the threshold."
  },
  {
    "title": "SLAMSpoof: Practical LiDAR Spoofing Attacks on Localization Systems Guided by Scan Matching Vulnerability Analysis",
    "url": "http://arxiv.org/abs/2502.13641v1",
    "arxiv_id": "2502.13641v1",
    "authors": [
      "Rokuto Nagata",
      "Kenji Koide",
      "Yuki Hayakawa",
      "Ryo Suzuki",
      "Kazuma Ikeda",
      "Ozora Sako",
      "Qi Alfred Chen",
      "Takami Sato",
      "Kentaro Yoshioka"
    ],
    "published": "2025-02-19T11:33:56+00:00",
    "summary": "Accurate localization is essential for enabling modern full self-driving services. These services heavily rely on map-based traffic information to reduce uncertainties in recognizing lane shapes, traffic light locations, and traffic signs. Achieving this level of reliance on map information requires centimeter-level localization accuracy, which is currently only achievable with LiDAR sensors. However, LiDAR is known to be vulnerable to spoofing attacks that emit malicious lasers against LiDAR to overwrite its measurements. Once localization is compromised, the attack could lead the victim off roads or make them ignore traffic lights. Motivated by these serious safety implications, we design SLAMSpoof, the first practical LiDAR spoofing attack on localization systems for self-driving to assess the actual attack significance on autonomous vehicles. SLAMSpoof can effectively find the effective attack location based on our scan matching vulnerability score (SMVS), a point-wise metric representing the potential vulnerability to spoofing attacks. To evaluate the effectiveness of the attack, we conduct real-world experiments on ground vehicles and confirm its high capability in real-world scenarios, inducing position errors of $\\geq$4.2 meters (more than typical lane width) for all 3 popular LiDAR-based localization algorithms. We finally discuss the potential countermeasures of this attack. Code is available at https://github.com/Keio-CSG/slamspoof"
  },
  {
    "title": "Qorgau: Evaluating LLM Safety in Kazakh-Russian Bilingual Contexts",
    "url": "http://arxiv.org/abs/2502.13640v1",
    "arxiv_id": "2502.13640v1",
    "authors": [
      "Maiya Goloburda",
      "Nurkhan Laiyk",
      "Diana Turmakhan",
      "Yuxia Wang",
      "Mukhammed Togmanov",
      "Jonibek Mansurov",
      "Askhat Sametov",
      "Nurdaulet Mukhituly",
      "Minghan Wang",
      "Daniil Orel",
      "Zain Muhammad Mujahid",
      "Fajri Koto",
      "Timothy Baldwin",
      "Preslav Nakov"
    ],
    "published": "2025-02-19T11:33:22+00:00",
    "summary": "Large language models (LLMs) are known to have the potential to generate harmful content, posing risks to users. While significant progress has been made in developing taxonomies for LLM risks and safety evaluation prompts, most studies have focused on monolingual contexts, primarily in English. However, language- and region-specific risks in bilingual contexts are often overlooked, and core findings can diverge from those in monolingual settings. In this paper, we introduce Qorgau, a novel dataset specifically designed for safety evaluation in Kazakh and Russian, reflecting the unique bilingual context in Kazakhstan, where both Kazakh (a low-resource language) and Russian (a high-resource language) are spoken. Experiments with both multilingual and language-specific LLMs reveal notable differences in safety performance, emphasizing the need for tailored, region-specific datasets to ensure the responsible and safe deployment of LLMs in countries like Kazakhstan. Warning: this paper contains example data that may be offensive, harmful, or biased."
  },
  {
    "title": "First Glimpse on Physical Layer Security in Internet of Vehicles: Transformed from Communication Interference to Sensing Interference",
    "url": "http://arxiv.org/abs/2502.13634v1",
    "arxiv_id": "2502.13634v1",
    "authors": [
      "Kaixuan Li",
      "Kan Yu",
      "Xiaowu Liu",
      "Qixun Zhang",
      "Zhiyong Feng",
      "Dong Li"
    ],
    "published": "2025-02-19T11:13:18+00:00",
    "summary": "Integrated sensing and communication (ISAC) plays a crucial role in the Internet of Vehicles (IoV), serving as a key factor in enhancing driving safety and traffic efficiency. To address the security challenges of the confidential information transmission caused by the inherent openness nature of wireless medium, different from current physical layer security (PLS) methods, which depends on the \\emph{additional communication interference} costing extra power resources, in this paper, we investigate a novel PLS solution, under which the \\emph{inherent radar sensing interference} of the vehicles is utilized to secure wireless communications. To measure the performance of PLS methods in ISAC-based IoV systems, we first define an improved security performance metric called by transmission reliability and sensing accuracy based secrecy rate (TRSA\\_SR), and derive closed-form expressions of connection outage probability (COP), secrecy outage probability (SOP), success ranging probability (SRP) for evaluating transmission reliability, security and sensing accuracy, respectively. Furthermore, we formulate an optimization problem to maximize the TRSA\\_SR by utilizing radar sensing interference and joint design of the communication duration, transmission power and straight trajectory of the legitimate transmitter. Finally, the non-convex feature of formulated problem is solved through the problem decomposition and alternating optimization. Simulations indicate that compared with traditional PLS methods obtaining a non-positive STC, the proposed method achieves a secrecy rate of 3.92bps/Hz for different settings of noise power."
  },
  {
    "title": "Efficient Safety Retrofitting Against Jailbreaking for LLMs",
    "url": "http://arxiv.org/abs/2502.13603v1",
    "arxiv_id": "2502.13603v1",
    "authors": [
      "Dario Garcia-Gasulla",
      "Anna Arias-Duart",
      "Adrian Tormos",
      "Daniel Hinjos",
      "Oscar Molina-Sedano",
      "Ashwin Kumar Gururajan",
      "Maria Eugenia Cardello"
    ],
    "published": "2025-02-19T10:33:18+00:00",
    "summary": "Direct Preference Optimization (DPO) is an efficient alignment technique that steers LLMs towards preferable outputs by training on preference data, bypassing the need for explicit reward models. Its simplicity enables easy adaptation to various domains and safety requirements. This paper examines DPO's effectiveness in model safety against jailbreaking attacks while minimizing data requirements and training costs. We introduce Egida, a dataset expanded from multiple sources, which includes 27 different safety topics and 18 different attack styles, complemented with synthetic and human labels. This data is used to boost the safety of state-of-the-art LLMs (Llama-3.1-8B/70B-Instruct, Qwen-2.5-7B/72B-Instruct) across topics and attack styles. In addition to safety evaluations, we assess their post-alignment performance degradation in general purpose tasks, and their tendency to over refusal. Following the proposed methodology, trained models reduce their Attack Success Rate by 10%-30%, using small training efforts (2,000 samples) with low computational cost (3\\$ for 8B models, 20\\$ for 72B models). Safety aligned models generalize to unseen topics and attack styles, with the most successful attack style reaching a success rate around 5%. Size and family are found to strongly influence model malleability towards safety, pointing at the importance of pre-training choices. To validate our findings, a large independent assessment of human preference agreement with Llama-Guard-3-8B is conducted by the authors and the associated dataset Egida-HSafe is released. Overall, this study illustrates how affordable and accessible it is to enhance LLM safety using DPO while outlining its current limitations. All datasets and models are released to enable reproducibility and further research."
  },
  {
    "title": "Exploiting Prefix-Tree in Structured Output Interfaces for Enhancing Jailbreak Attacking",
    "url": "http://arxiv.org/abs/2502.13527v1",
    "arxiv_id": "2502.13527v1",
    "authors": [
      "Yanzeng Li",
      "Yunfan Xiong",
      "Jialun Zhong",
      "Jinchao Zhang",
      "Jie Zhou",
      "Lei Zou"
    ],
    "published": "2025-02-19T08:29:36+00:00",
    "summary": "The rise of Large Language Models (LLMs) has led to significant applications but also introduced serious security threats, particularly from jailbreak attacks that manipulate output generation. These attacks utilize prompt engineering and logit manipulation to steer models toward harmful content, prompting LLM providers to implement filtering and safety alignment strategies. We investigate LLMs' safety mechanisms and their recent applications, revealing a new threat model targeting structured output interfaces, which enable attackers to manipulate the inner logit during LLM generation, requiring only API access permissions. To demonstrate this threat model, we introduce a black-box attack framework called AttackPrefixTree (APT). APT exploits structured output interfaces to dynamically construct attack patterns. By leveraging prefixes of models' safety refusal response and latent harmful outputs, APT effectively bypasses safety measures. Experiments on benchmark datasets indicate that this approach achieves higher attack success rate than existing methods. This work highlights the urgent need for LLM providers to enhance security protocols to address vulnerabilities arising from the interaction between safety patterns and structured outputs."
  },
  {
    "title": "Integration of Agentic AI with 6G Networks for Mission-Critical Applications: Use-case and Challenges",
    "url": "http://arxiv.org/abs/2502.13476v1",
    "arxiv_id": "2502.13476v1",
    "authors": [
      "Sunder Ali Khowaja",
      "Kapal Dev",
      "Muhammad Salman Pathan",
      "Engin Zeydan",
      "Merouane Debbah"
    ],
    "published": "2025-02-19T07:00:53+00:00",
    "summary": "We are in a transformative era, and advances in Artificial Intelligence (AI), especially the foundational models, are constantly in the news. AI has been an integral part of many applications that rely on automation for service delivery, and one of them is mission-critical public safety applications. The problem with AI-oriented mission-critical applications is the humanin-the-loop system and the lack of adaptability to dynamic conditions while maintaining situational awareness. Agentic AI (AAI) has gained a lot of attention recently due to its ability to analyze textual data through a contextual lens while quickly adapting to conditions. In this context, this paper proposes an AAI framework for mission-critical applications. We propose a novel framework with a multi-layer architecture to realize the AAI. We also present a detailed implementation of AAI layer that bridges the gap between network infrastructure and missioncritical applications. Our preliminary analysis shows that the AAI reduces initial response time by 5.6 minutes on average, while alert generation time is reduced by 15.6 seconds on average and resource allocation is improved by up to 13.4%. We also show that the AAI methods improve the number of concurrent operations by 40, which reduces the recovery time by up to 5.2 minutes. Finally, we highlight some of the issues and challenges that need to be considered when implementing AAI frameworks."
  },
  {
    "title": "Beyond Single-Value Metrics: Evaluating and Enhancing LLM Unlearning with Cognitive Diagnosis",
    "url": "http://arxiv.org/abs/2502.13996v1",
    "arxiv_id": "2502.13996v1",
    "authors": [
      "Yicheng Lang",
      "Kehan Guo",
      "Yue Huang",
      "Yujun Zhou",
      "Haomin Zhuang",
      "Tianyu Yang",
      "Yao Su",
      "Xiangliang Zhang"
    ],
    "published": "2025-02-19T06:56:59+00:00",
    "summary": "Due to the widespread use of LLMs and the rising critical ethical and safety concerns, LLM unlearning methods have been developed to remove harmful knowledge and undesirable capabilities. In this context, evaluations are mostly based on single-value metrics such as QA accuracy. However, these metrics often fail to capture the nuanced retention of harmful knowledge components, making it difficult to assess the true effectiveness of unlearning. To address this issue, we propose UNCD (UNlearning evaluation via Cognitive Diagnosis), a novel framework that leverages Cognitive Diagnosis Modeling for fine-grained evaluation of LLM unlearning. Our dedicated benchmark, UNCD-Cyber, provides a detailed assessment of the removal of dangerous capabilities. Moreover, we introduce UNCD-Agent, which refines unlearning by diagnosing knowledge remnants and generating targeted unlearning data. Extensive experiments across eight unlearning methods and two base models demonstrate that UNCD not only enhances evaluation but also effectively facilitates the removal of harmful LLM abilities."
  },
  {
    "title": "ThinkGuard: Deliberative Slow Thinking Leads to Cautious Guardrails",
    "url": "http://arxiv.org/abs/2502.13458v1",
    "arxiv_id": "2502.13458v1",
    "authors": [
      "Xiaofei Wen",
      "Wenxuan Zhou",
      "Wenjie Jacky Mo",
      "Muhao Chen"
    ],
    "published": "2025-02-19T06:09:58+00:00",
    "summary": "Ensuring the safety of large language models (LLMs) is critical as they are deployed in real-world applications. Existing guardrails rely on rule-based filtering or single-pass classification, limiting their ability to handle nuanced safety violations. To address this, we propose ThinkGuard, a critique-augmented guardrail model that distills knowledge from high-capacity LLMs by generating structured critiques alongside safety labels. Fine-tuned on critique-augmented data, the captured deliberative thinking ability drastically enhances the guardrail's cautiousness and interpretability. Evaluated on multiple safety benchmarks, ThinkGuard achieves the highest average F1 and AUPRC, outperforming all baselines. Compared to LLaMA Guard 3, ThinkGuard improves accuracy by 16.1% and macro F1 by 27.0%. Moreover, it surpasses label-only fine-tuned models, confirming that structured critiques enhance both classification precision and nuanced safety reasoning while maintaining computational efficiency."
  },
  {
    "title": "TreeCut: A Synthetic Unanswerable Math Word Problem Dataset for LLM Hallucination Evaluation",
    "url": "http://arxiv.org/abs/2502.13442v1",
    "arxiv_id": "2502.13442v1",
    "authors": [
      "Jialin Ouyang"
    ],
    "published": "2025-02-19T05:38:45+00:00",
    "summary": "Large language models (LLMs) now achieve near-human performance on standard math word problem benchmarks (e.g., GSM8K), yet their true reasoning ability remains disputed. A key concern is that models often produce confident, yet unfounded, answers to unanswerable problems. We introduce TreeCut, a synthetic dataset that systematically generates infinite unanswerable math word problems and their answerable counterparts, by representing each question as a tree and removing chosen necessary conditions. Experiments show TreeCut effectively induce hallucinations in large language models, including GPT-4o and o3-mini, with rates of 61% and 42% in their respective worst-case scenarios. Further analysis highlights that deeper or more complex trees, composite item names, and removing necessary condition near the middle of a path all increase the likelihood of hallucinations, underscoring the persistent challenges LLMs face in identifying unanswerable math problems."
  },
  {
    "title": "Dynamic directed functional connectivity as a neural biomarker for objective motor skill assessment",
    "url": "http://arxiv.org/abs/2502.13362v1",
    "arxiv_id": "2502.13362v1",
    "authors": [
      "Anil Kamat",
      "Rahul Rahul",
      "Anirban Dutta",
      "Lora Cavuoto",
      "Uwe Kruger",
      "Harry Burke",
      "Matthew Hackett",
      "Jack Norfleet",
      "Steven Schwaitzberg",
      "Suvranu De"
    ],
    "published": "2025-02-19T01:51:39+00:00",
    "summary": "Objective motor skill assessment plays a critical role in fields such as surgery, where proficiency is vital for certification and patient safety. Existing assessment methods, however, rely heavily on subjective human judgment, which introduces bias and limits reproducibility. While recent efforts have leveraged kinematic data and neural imaging to provide more objective evaluations, these approaches often overlook the dynamic neural mechanisms that differentiate expert and novice performance. This study proposes a novel method for motor skill assessment based on dynamic directed functional connectivity (dFC) as a neural biomarker. By using electroencephalography (EEG) to capture brain dynamics and employing an attention-based Long Short-Term Memory (LSTM) model for non-linear Granger causality analysis, we compute dFC among key brain regions involved in psychomotor tasks. Coupled with hierarchical task analysis (HTA), our approach enables subtask-level evaluation of motor skills, offering detailed insights into neural coordination that underpins expert proficiency. A convolutional neural network (CNN) is then used to classify skill levels, achieving greater accuracy and specificity than established performance metrics in laparoscopic surgery. This methodology provides a reliable, objective framework for assessing motor skills, contributing to the development of tailored training protocols and enhancing the certification process."
  },
  {
    "title": "Language Models are Few-Shot Graders",
    "url": "http://arxiv.org/abs/2502.13337v1",
    "arxiv_id": "2502.13337v1",
    "authors": [
      "Chenyan Zhao",
      "Mariana Silva",
      "Seth Poulsen"
    ],
    "published": "2025-02-18T23:38:21+00:00",
    "summary": "Providing evaluations to student work is a critical component of effective student learning, and automating its process can significantly reduce the workload on human graders. Automatic Short Answer Grading (ASAG) systems, enabled by advancements in Large Language Models (LLMs), offer a promising solution for assessing and providing instant feedback for open-ended student responses. In this paper, we present an ASAG pipeline leveraging state-of-the-art LLMs. Our new LLM-based ASAG pipeline achieves better performances than existing custom-built models on the same datasets. We also compare the grading performance of three OpenAI models: GPT-4, GPT-4o, and o1-preview. Our results demonstrate that GPT-4o achieves the best balance between accuracy and cost-effectiveness. On the other hand, o1-preview, despite higher accuracy, exhibits a larger variance in error that makes it less practical for classroom use. We investigate the effects of incorporating instructor-graded examples into prompts using no examples, random selection, and Retrieval-Augmented Generation (RAG)-based selection strategies. Our findings indicate that providing graded examples enhances grading accuracy, with RAG-based selection outperforming random selection. Additionally, integrating grading rubrics improves accuracy by offering a structured standard for evaluation."
  },
  {
    "title": "Demonstrating specification gaming in reasoning models",
    "url": "http://arxiv.org/abs/2502.13295v1",
    "arxiv_id": "2502.13295v1",
    "authors": [
      "Alexander Bondarenko",
      "Denis Volk",
      "Dmitrii Volkov",
      "Jeffrey Ladish"
    ],
    "published": "2025-02-18T21:32:24+00:00",
    "summary": "We demonstrate LLM agent specification gaming by instructing models to win against a chess engine. We find reasoning models like o1 preview and DeepSeek-R1 will often hack the benchmark by default, while language models like GPT-4o and Claude 3.5 Sonnet need to be told that normal play won't work to hack.   We improve upon prior work like (Hubinger et al., 2024; Meinke et al., 2024; Weij et al., 2024) by using realistic task prompts and avoiding excess nudging. Our results suggest reasoning models may resort to hacking to solve difficult problems, as observed in OpenAI (2024)'s o1 Docker escape during cyber capabilities testing."
  },
  {
    "title": "RAD: Training an End-to-End Driving Policy via Large-Scale 3DGS-based Reinforcement Learning",
    "url": "http://arxiv.org/abs/2502.13144v1",
    "arxiv_id": "2502.13144v1",
    "authors": [
      "Hao Gao",
      "Shaoyu Chen",
      "Bo Jiang",
      "Bencheng Liao",
      "Yiang Shi",
      "Xiaoyang Guo",
      "Yuechuan Pu",
      "Haoran Yin",
      "Xiangyu Li",
      "Xinbang Zhang",
      "Ying Zhang",
      "Wenyu Liu",
      "Qian Zhang",
      "Xinggang Wang"
    ],
    "published": "2025-02-18T18:59:21+00:00",
    "summary": "Existing end-to-end autonomous driving (AD) algorithms typically follow the Imitation Learning (IL) paradigm, which faces challenges such as causal confusion and the open-loop gap. In this work, we establish a 3DGS-based closed-loop Reinforcement Learning (RL) training paradigm. By leveraging 3DGS techniques, we construct a photorealistic digital replica of the real physical world, enabling the AD policy to extensively explore the state space and learn to handle out-of-distribution scenarios through large-scale trial and error. To enhance safety, we design specialized rewards that guide the policy to effectively respond to safety-critical events and understand real-world causal relationships. For better alignment with human driving behavior, IL is incorporated into RL training as a regularization term. We introduce a closed-loop evaluation benchmark consisting of diverse, previously unseen 3DGS environments. Compared to IL-based methods, RAD achieves stronger performance in most closed-loop metrics, especially 3x lower collision rate. Abundant closed-loop results are presented at https://hgao-cv.github.io/RAD."
  },
  {
    "title": "RHINO: Learning Real-Time Humanoid-Human-Object Interaction from Human Demonstrations",
    "url": "http://arxiv.org/abs/2502.13134v1",
    "arxiv_id": "2502.13134v1",
    "authors": [
      "Jingxiao Chen",
      "Xinyao Li",
      "Jiahang Cao",
      "Zhengbang Zhu",
      "Wentao Dong",
      "Minghuan Liu",
      "Ying Wen",
      "Yong Yu",
      "Liqing Zhang",
      "Weinan Zhang"
    ],
    "published": "2025-02-18T18:56:41+00:00",
    "summary": "Humanoid robots have shown success in locomotion and manipulation. Despite these basic abilities, humanoids are still required to quickly understand human instructions and react based on human interaction signals to become valuable assistants in human daily life. Unfortunately, most existing works only focus on multi-stage interactions, treating each task separately, and neglecting real-time feedback. In this work, we aim to empower humanoid robots with real-time reaction abilities to achieve various tasks, allowing human to interrupt robots at any time, and making robots respond to humans immediately. To support such abilities, we propose a general humanoid-human-object interaction framework, named RHINO, i.e., Real-time Humanoid-human Interaction and Object manipulation. RHINO provides a unified view of reactive motion, instruction-based manipulation, and safety concerns, over multiple human signal modalities, such as languages, images, and motions. RHINO is a hierarchical learning framework, enabling humanoids to learn reaction skills from human-human-object demonstrations and teleoperation data. In particular, it decouples the interaction process into two levels: 1) a high-level planner inferring human intentions from real-time human behaviors; and 2) a low-level controller achieving reactive motion behaviors and object manipulation skills based on the predicted intentions. We evaluate the proposed framework on a real humanoid robot and demonstrate its effectiveness, flexibility, and safety in various scenarios."
  },
  {
    "title": "Rethinking Diverse Human Preference Learning through Principal Component Analysis",
    "url": "http://arxiv.org/abs/2502.13131v1",
    "arxiv_id": "2502.13131v1",
    "authors": [
      "Feng Luo",
      "Rui Yang",
      "Hao Sun",
      "Chunyuan Deng",
      "Jiarui Yao",
      "Jingyan Shen",
      "Huan Zhang",
      "Hanjie Chen"
    ],
    "published": "2025-02-18T18:55:26+00:00",
    "summary": "Understanding human preferences is crucial for improving foundation models and building personalized AI systems. However, preferences are inherently diverse and complex, making it difficult for traditional reward models to capture their full range. While fine-grained preference data can help, collecting it is expensive and hard to scale. In this paper, we introduce Decomposed Reward Models (DRMs), a novel approach that extracts diverse human preferences from binary comparisons without requiring fine-grained annotations. Our key insight is to represent human preferences as vectors and analyze them using Principal Component Analysis (PCA). By constructing a dataset of embedding differences between preferred and rejected responses, DRMs identify orthogonal basis vectors that capture distinct aspects of preference. These decomposed rewards can be flexibly combined to align with different user needs, offering an interpretable and scalable alternative to traditional reward models. We demonstrate that DRMs effectively extract meaningful preference dimensions (e.g., helpfulness, safety, humor) and adapt to new users without additional training. Our results highlight DRMs as a powerful framework for personalized and interpretable LLM alignment."
  },
  {
    "title": "Understanding and Rectifying Safety Perception Distortion in VLMs",
    "url": "http://arxiv.org/abs/2502.13095v1",
    "arxiv_id": "2502.13095v1",
    "authors": [
      "Xiaohan Zou",
      "Jian Kang",
      "George Kesidis",
      "Lu Lin"
    ],
    "published": "2025-02-18T18:06:48+00:00",
    "summary": "Recent studies reveal that vision-language models (VLMs) become more susceptible to harmful requests and jailbreak attacks after integrating the vision modality, exhibiting greater vulnerability than their text-only LLM backbones. To uncover the root cause of this phenomenon, we conduct an in-depth analysis and identify a key issue: multimodal inputs introduce an modality-induced activation shift toward a \"safer\" direction compared to their text-only counterparts, leading VLMs to systematically overestimate the safety of harmful inputs. We refer to this issue as safety perception distortion. To mitigate such distortion, we propose Activation Shift Disentanglement and Calibration (ShiftDC), a training-free method that decomposes and calibrates the modality-induced activation shift to reduce the impact of modality on safety. By isolating and removing the safety-relevant component, ShiftDC restores the inherent safety alignment of the LLM backbone while preserving the vision-language capabilities of VLMs. Empirical results demonstrate that ShiftDC significantly enhances alignment performance on safety benchmarks without impairing model utility."
  },
  {
    "title": "Conditional Max-Sum for Asynchronous Multiagent Decision Making",
    "url": "http://arxiv.org/abs/2502.13194v1",
    "arxiv_id": "2502.13194v1",
    "authors": [
      "Dimitrios Troullinos",
      "Georgios Chalkiadakis",
      "Ioannis Papamichail",
      "Markos Papageorgiou"
    ],
    "published": "2025-02-18T17:16:27+00:00",
    "summary": "In this paper we present a novel approach for multiagent decision making in dynamic environments based on Factor Graphs and the Max-Sum algorithm, considering asynchronous variable reassignments and distributed message-passing among agents. Motivated by the challenging domain of lane-free traffic where automated vehicles can communicate and coordinate as agents, we propose a more realistic communication framework for Factor Graph formulations that satisfies the above-mentioned restrictions, along with Conditional Max-Sum: an extension of Max-Sum with a revised message-passing process that is better suited for asynchronous settings. The overall application in lane-free traffic can be viewed as a hybrid system where the Factor Graph formulation undertakes the strategic decision making of vehicles, that of desired lateral alignment in a coordinated manner; and acts on top of a rule-based method we devise that provides a structured representation of the lane-free environment for the factors, while also handling the underlying control of vehicles regarding core operations and safety. Our experimental evaluation showcases the capabilities of the proposed framework in problems with intense coordination needs when compared to a domain-specific baseline without communication, and an increased adeptness of Conditional Max-Sum with respect to the standard algorithm."
  },
  {
    "title": "Interactive Agents to Overcome Ambiguity in Software Engineering",
    "url": "http://arxiv.org/abs/2502.13069v1",
    "arxiv_id": "2502.13069v1",
    "authors": [
      "Sanidhya Vijayvargiya",
      "Xuhui Zhou",
      "Akhila Yerukola",
      "Maarten Sap",
      "Graham Neubig"
    ],
    "published": "2025-02-18T17:12:26+00:00",
    "summary": "AI agents are increasingly being deployed to automate tasks, often based on ambiguous and underspecified user instructions. Making unwarranted assumptions and failing to ask clarifying questions can lead to suboptimal outcomes, safety risks due to tool misuse, and wasted computational resources. In this work, we study the ability of LLM agents to handle ambiguous instructions in interactive code generation settings by evaluating proprietary and open-weight models on their performance across three key steps: (a) leveraging interactivity to improve performance in ambiguous scenarios, (b) detecting ambiguity, and (c) asking targeted questions. Our findings reveal that models struggle to distinguish between well-specified and underspecified instructions. However, when models interact for underspecified inputs, they effectively obtain vital information from the user, leading to significant improvements in performance and underscoring the value of effective interaction. Our study highlights critical gaps in how current state-of-the-art models handle ambiguity in complex software engineering tasks and structures the evaluation into distinct steps to enable targeted improvements."
  },
  {
    "title": "Enhancing Power Grid Inspections with Machine Learning",
    "url": "http://arxiv.org/abs/2502.13037v1",
    "arxiv_id": "2502.13037v1",
    "authors": [
      "Diogo Lavado",
      "Ricardo Santos",
      "Andre Coelho",
      "Joao Santos",
      "Alessandra Micheletti",
      "Claudia Soares"
    ],
    "published": "2025-02-18T16:49:47+00:00",
    "summary": "Ensuring the safety and reliability of power grids is critical as global energy demands continue to rise. Traditional inspection methods, such as manual observations or helicopter surveys, are resource-intensive and lack scalability. This paper explores the use of 3D computer vision to automate power grid inspections, utilizing the TS40K dataset -- a high-density, annotated collection of 3D LiDAR point clouds. By concentrating on 3D semantic segmentation, our approach addresses challenges like class imbalance and noisy data to enhance the detection of critical grid components such as power lines and towers. The benchmark results indicate significant performance improvements, with IoU scores reaching 95.53% for the detection of power lines using transformer-based models. Our findings illustrate the potential for integrating ML into grid maintenance workflows, increasing efficiency and enabling proactive risk management strategies."
  },
  {
    "title": "Fragility-aware Classification for Understanding Risk and Improving Generalization",
    "url": "http://arxiv.org/abs/2502.13024v1",
    "arxiv_id": "2502.13024v1",
    "authors": [
      "Chen Yang",
      "Zheng Cui",
      "Daniel Zhuoyu Long",
      "Jin Qi",
      "Ruohan Zhan"
    ],
    "published": "2025-02-18T16:44:03+00:00",
    "summary": "Classification models play a critical role in data-driven decision-making applications such as medical diagnosis, user profiling, recommendation systems, and default detection. Traditional performance metrics, such as accuracy, focus on overall error rates but fail to account for the confidence of incorrect predictions, thereby overlooking the risk of confident misjudgments. This risk is particularly significant in cost-sensitive and safety-critical domains like medical diagnosis and autonomous driving, where overconfident false predictions may cause severe consequences. To address this issue, we introduce the Fragility Index (FI), a novel metric that evaluates classification performance from a risk-averse perspective by explicitly capturing the tail risk of confident misjudgments. To enhance generalizability, we define FI within the robust satisficing (RS) framework, incorporating data uncertainty. We further develop a model training approach that optimizes FI while maintaining tractability for common loss functions. Specifically, we derive exact reformulations for cross-entropy loss, hinge-type loss, and Lipschitz loss, and extend the approach to deep learning models. Through synthetic experiments and real-world medical diagnosis tasks, we demonstrate that FI effectively identifies misjudgment risk and FI-based training improves model robustness and generalizability. Finally, we extend our framework to deep neural network training, further validating its effectiveness in enhancing deep learning models."
  },
  {
    "title": "Reasoning-to-Defend: Safety-Aware Reasoning Can Defend Large Language Models from Jailbreaking",
    "url": "http://arxiv.org/abs/2502.12970v1",
    "arxiv_id": "2502.12970v1",
    "authors": [
      "Junda Zhu",
      "Lingyong Yan",
      "Shuaiqiang Wang",
      "Dawei Yin",
      "Lei Sha"
    ],
    "published": "2025-02-18T15:48:46+00:00",
    "summary": "The reasoning abilities of Large Language Models (LLMs) have demonstrated remarkable advancement and exceptional performance across diverse domains. However, leveraging these reasoning capabilities to enhance LLM safety against adversarial attacks and jailbreak queries remains largely unexplored. To bridge this gap, we propose Reasoning-to-Defend (R2D), a novel training paradigm that integrates safety reflections of queries and responses into LLMs' generation process, unlocking a safety-aware reasoning mechanism. This approach enables self-evaluation at each reasoning step to create safety pivot tokens as indicators of the response's safety status. Furthermore, in order to improve the learning efficiency of pivot token prediction, we propose Contrastive Pivot Optimization(CPO), which enhances the model's ability to perceive the safety status of dialogues. Through this mechanism, LLMs dynamically adjust their response strategies during reasoning, significantly enhancing their defense capabilities against jailbreak attacks. Extensive experimental results demonstrate that R2D effectively mitigates various attacks and improves overall safety, highlighting the substantial potential of safety-aware reasoning in strengthening LLMs' robustness against jailbreaks."
  },
  {
    "title": "Trust Me, I'm Wrong: High-Certainty Hallucinations in LLMs",
    "url": "http://arxiv.org/abs/2502.12964v1",
    "arxiv_id": "2502.12964v1",
    "authors": [
      "Adi Simhi",
      "Itay Itzhak",
      "Fazl Barez",
      "Gabriel Stanovsky",
      "Yonatan Belinkov"
    ],
    "published": "2025-02-18T15:46:31+00:00",
    "summary": "Large Language Models (LLMs) often generate outputs that lack grounding in real-world facts, a phenomenon known as hallucinations. Prior research has associated hallucinations with model uncertainty, leveraging this relationship for hallucination detection and mitigation. In this paper, we challenge the underlying assumption that all hallucinations are associated with uncertainty. Using knowledge detection and uncertainty measurement methods, we demonstrate that models can hallucinate with high certainty even when they have the correct knowledge. We further show that high-certainty hallucinations are consistent across models and datasets, distinctive enough to be singled out, and challenge existing mitigation methods. Our findings reveal an overlooked aspect of hallucinations, emphasizing the need to understand their origins and improve mitigation strategies to enhance LLM safety. The code is available at https://github.com/technion-cs-nlp/Trust_me_Im_wrong ."
  },
  {
    "title": "LLMPopcorn: An Empirical Study of LLMs as Assistants for Popular Micro-video Generation",
    "url": "http://arxiv.org/abs/2502.12945v1",
    "arxiv_id": "2502.12945v1",
    "authors": [
      "Junchen Fu",
      "Xuri Ge",
      "Kaiwen Zheng",
      "Ioannis Arapakis",
      "Xin Xin",
      "Joemon M. Jose"
    ],
    "published": "2025-02-18T15:29:05+00:00",
    "summary": "Popular Micro-videos, dominant on platforms like TikTok and YouTube, hold significant commercial value. The rise of high-quality AI-generated content has spurred interest in AI-driven micro-video creation. However, despite the advanced capabilities of large language models (LLMs) like ChatGPT and DeepSeek in text generation and reasoning, their potential to assist the creation of popular micro-videos remains largely unexplored.   In this paper, we conduct an empirical study on LLM-assisted popular micro-video generation (LLMPopcorn). Specifically, we investigate the following research questions: (i) How can LLMs be effectively utilized to assist popular micro-video generation? (ii) To what extent can prompt-based enhancements optimize the LLM-generated content for higher popularity? (iii) How well do various LLMs and video generators perform in the popular micro-video generation task? By exploring these questions, we show that advanced LLMs like DeepSeek-V3 enable micro-video generation to achieve popularity comparable to human-created content. Prompt enhancements further boost popularity, and benchmarking highlights DeepSeek-V3 and DeepSeek-R1 among LLMs, while LTX-Video and HunyuanVideo lead in video generation. This pioneering work advances AI-assisted micro-video creation, uncovering new research opportunities. We will release the code and datasets to support future studies."
  },
  {
    "title": "LLMPopcorn: An Empirical Study of LLMs as Assistants for Popular Micro-video Generation",
    "url": "http://arxiv.org/abs/2502.12945v2",
    "arxiv_id": "2502.12945v2",
    "authors": [
      "Junchen Fu",
      "Xuri Ge",
      "Kaiwen Zheng",
      "Ioannis Arapakis",
      "Xin Xin",
      "Joemon M. Jose"
    ],
    "published": "2025-02-18T15:29:05+00:00",
    "summary": "Popular Micro-videos, dominant on platforms like TikTok and YouTube, hold significant commercial value. The rise of high-quality AI-generated content has spurred interest in AI-driven micro-video creation. However, despite the advanced capabilities of large language models (LLMs) like ChatGPT and DeepSeek in text generation and reasoning, their potential to assist the creation of popular micro-videos remains largely unexplored.   In this paper, we conduct an empirical study on LLM-assisted popular micro-video generation (LLMPopcorn). Specifically, we investigate the following research questions: (i) How can LLMs be effectively utilized to assist popular micro-video generation? (ii) To what extent can prompt-based enhancements optimize the LLM-generated content for higher popularity? (iii) How well do various LLMs and video generators perform in the popular micro-video generation task? By exploring these questions, we show that advanced LLMs like DeepSeek-V3 enable micro-video generation to achieve popularity comparable to human-created content. Prompt enhancements further boost popularity, and benchmarking highlights DeepSeek-V3 and DeepSeek-R1 among LLMs, while LTX-Video and HunyuanVideo lead in video generation. This pioneering work advances AI-assisted micro-video creation, uncovering new research opportunities. We will release the code and datasets to support future studies."
  },
  {
    "title": "None of the Others: a General Technique to Distinguish Reasoning from Memorization in Multiple-Choice LLM Evaluation Benchmarks",
    "url": "http://arxiv.org/abs/2502.12896v1",
    "arxiv_id": "2502.12896v1",
    "authors": [
      "Eva S\u00e1nchez Salido",
      "Julio Gonzalo",
      "Guillermo Marco"
    ],
    "published": "2025-02-18T14:32:44+00:00",
    "summary": "In LLM evaluations, reasoning is often distinguished from recall/memorization by performing numerical variations to math-oriented questions. Here we introduce a general variation method for multiple-choice questions that completely dissociates the correct answer from previously seen tokens or concepts, requiring LLMs to understand and reason (rather than memorizing) in order to answer correctly. Using this method, we evaluate state-of-the-art proprietary and open-source LLMs on two datasets available in English and Spanish: the public MMLU benchmark and the private UNED-Access 2024 dataset. Results show that all models experience remarkable accuracy drops under our proposed variation, with an average loss of 57% on MMLU and 50% on UNED-Access 2024, ranging from 10% to 93% across models. Notably, the most accurate model in our experimentation (OpenAI-o3-mini) is not the most robust (DeepSeek-R1-70B), suggesting that the best models in standard evaluations may not be the ones with better reasoning capabilities. Also, we see larger accuracy drops in public (vs private) datasets and questions posed in their original language (vs a manual translation), which are signs of contamination and also point to a relevant role of recall/memorization in current LLMs' answers."
  },
  {
    "title": "H-CoT: Hijacking the Chain-of-Thought Safety Reasoning Mechanism to Jailbreak Large Reasoning Models, Including OpenAI o1/o3, DeepSeek-R1, and Gemini 2.0 Flash Thinking",
    "url": "http://arxiv.org/abs/2502.12893v1",
    "arxiv_id": "2502.12893v1",
    "authors": [
      "Martin Kuo",
      "Jianyi Zhang",
      "Aolin Ding",
      "Qinsi Wang",
      "Louis DiValentin",
      "Yujia Bao",
      "Wei Wei",
      "Da-Cheng Juan",
      "Hai Li",
      "Yiran Chen"
    ],
    "published": "2025-02-18T14:29:12+00:00",
    "summary": "Large Reasoning Models (LRMs) have recently extended their powerful reasoning capabilities to safety checks-using chain-of-thought reasoning to decide whether a request should be answered. While this new approach offers a promising route for balancing model utility and safety, its robustness remains underexplored. To address this gap, we introduce Malicious-Educator, a benchmark that disguises extremely dangerous or malicious requests beneath seemingly legitimate educational prompts. Our experiments reveal severe security flaws in popular commercial-grade LRMs, including OpenAI o1/o3, DeepSeek-R1, and Gemini 2.0 Flash Thinking. For instance, although OpenAI's o1 model initially maintains a high refusal rate of about 98%, subsequent model updates significantly compromise its safety; and attackers can easily extract criminal strategies from DeepSeek-R1 and Gemini 2.0 Flash Thinking without any additional tricks. To further highlight these vulnerabilities, we propose Hijacking Chain-of-Thought (H-CoT), a universal and transferable attack method that leverages the model's own displayed intermediate reasoning to jailbreak its safety reasoning mechanism. Under H-CoT, refusal rates sharply decline-dropping from 98% to below 2%-and, in some instances, even transform initially cautious tones into ones that are willing to provide harmful content. We hope these findings underscore the urgent need for more robust safety mechanisms to preserve the benefits of advanced reasoning capabilities without compromising ethical standards."
  },
  {
    "title": "A Survey of Sim-to-Real Methods in RL: Progress, Prospects and Challenges with Foundation Models",
    "url": "http://arxiv.org/abs/2502.13187v1",
    "arxiv_id": "2502.13187v1",
    "authors": [
      "Longchao Da",
      "Justin Turnau",
      "Thirulogasankar Pranav Kutralingam",
      "Alvaro Velasquez",
      "Paulo Shakarian",
      "Hua Wei"
    ],
    "published": "2025-02-18T12:57:29+00:00",
    "summary": "Deep Reinforcement Learning (RL) has been explored and verified to be effective in solving decision-making tasks in various domains, such as robotics, transportation, recommender systems, etc. It learns from the interaction with environments and updates the policy using the collected experience. However, due to the limited real-world data and unbearable consequences of taking detrimental actions, the learning of RL policy is mainly restricted within the simulators. This practice guarantees safety in learning but introduces an inevitable sim-to-real gap in terms of deployment, thus causing degraded performance and risks in execution. There are attempts to solve the sim-to-real problems from different domains with various techniques, especially in the era with emerging techniques such as large foundations or language models that have cast light on the sim-to-real. This survey paper, to the best of our knowledge, is the first taxonomy that formally frames the sim-to-real techniques from key elements of the Markov Decision Process (State, Action, Transition, and Reward). Based on the framework, we cover comprehensive literature from the classic to the most advanced methods including the sim-to-real techniques empowered by foundation models, and we also discuss the specialties that are worth attention in different domains of sim-to-real problems. Then we summarize the formal evaluation process of sim-to-real performance with accessible code or benchmarks. The challenges and opportunities are also presented to encourage future exploration of this direction. We are actively maintaining a to include the most up-to-date sim-to-real research outcomes to help the researchers in their work."
  },
  {
    "title": "Reasoning and the Trusting Behavior of DeepSeek and GPT: An Experiment Revealing Hidden Fault Lines in Large Language Models",
    "url": "http://arxiv.org/abs/2502.12825v1",
    "arxiv_id": "2502.12825v1",
    "authors": [
      "Rubing Lu",
      "Jo\u00e3o Sedoc",
      "Arun Sundararajan"
    ],
    "published": "2025-02-18T12:46:18+00:00",
    "summary": "When encountering increasingly frequent performance improvements or cost reductions from a new large language model (LLM), developers of applications leveraging LLMs must decide whether to take advantage of these improvements or stay with older tried-and-tested models. Low perceived switching frictions can lead to choices that do not consider more subtle behavior changes that the transition may induce. Our experiments use a popular game-theoretic behavioral economics model of trust to show stark differences in the trusting behavior of OpenAI's and DeepSeek's models. We highlight a collapse in the economic trust behavior of the o1-mini and o3-mini models as they reconcile profit-maximizing and risk-seeking with future returns from trust, and contrast it with DeepSeek's more sophisticated and profitable trusting behavior that stems from an ability to incorporate deeper concepts like forward planning and theory-of-mind. As LLMs form the basis for high-stakes commercial systems, our results highlight the perils of relying on LLM performance benchmarks that are too narrowly defined and suggest that careful analysis of their hidden fault lines should be part of any organization's AI strategy."
  },
  {
    "title": "Reasoning and the Trusting Behavior of DeepSeek and GPT: An Experiment Revealing Hidden Fault Lines in Large Language Models",
    "url": "http://arxiv.org/abs/2502.12825v2",
    "arxiv_id": "2502.12825v2",
    "authors": [
      "Rubing Li",
      "Jo\u00e3o Sedoc",
      "Arun Sundararajan"
    ],
    "published": "2025-02-18T12:46:18+00:00",
    "summary": "When encountering increasingly frequent performance improvements or cost reductions from a new large language model (LLM), developers of applications leveraging LLMs must decide whether to take advantage of these improvements or stay with older tried-and-tested models. Low perceived switching frictions can lead to choices that do not consider more subtle behavior changes that the transition may induce. Our experiments use a popular game-theoretic behavioral economics model of trust to show stark differences in the trusting behavior of OpenAI's and DeepSeek's models. We highlight a collapse in the economic trust behavior of the o1-mini and o3-mini models as they reconcile profit-maximizing and risk-seeking with future returns from trust, and contrast it with DeepSeek's more sophisticated and profitable trusting behavior that stems from an ability to incorporate deeper concepts like forward planning and theory-of-mind. As LLMs form the basis for high-stakes commercial systems, our results highlight the perils of relying on LLM performance benchmarks that are too narrowly defined and suggest that careful analysis of their hidden fault lines should be part of any organization's AI strategy."
  },
  {
    "title": "Simulating User Diversity in Task-Oriented Dialogue Systems using Large Language Models",
    "url": "http://arxiv.org/abs/2502.12813v1",
    "arxiv_id": "2502.12813v1",
    "authors": [
      "Adnan Ahmad",
      "Stefan Hillmann",
      "Sebastian M\u00f6ller"
    ],
    "published": "2025-02-18T12:20:16+00:00",
    "summary": "In this study, we explore the application of Large Language Models (LLMs) for generating synthetic users and simulating user conversations with a task-oriented dialogue system and present detailed results and their analysis. We propose a comprehensive novel approach to user simulation technique that uses LLMs to create diverse user profiles, set goals, engage in multi-turn dialogues, and evaluate the conversation success. We employ two proprietary LLMs, namely GPT-4o and GPT-o1 (Achiam et al., 2023), to generate a heterogeneous base of user profiles, characterized by varied demographics, multiple user goals, different conversational styles, initial knowledge levels, interests, and conversational objectives. We perform a detailed analysis of the user profiles generated by LLMs to assess the diversity, consistency, and potential biases inherent in these LLM-generated user simulations. We find that GPT-o1 generates more heterogeneous user distribution across most user attributes, while GPT-4o generates more skewed user attributes. The generated set of user profiles are then utilized to simulate dialogue sessions by interacting with a task-oriented dialogue system."
  },
  {
    "title": "Expanding the Classical V-Model for the Development of Complex Systems Incorporating AI",
    "url": "http://arxiv.org/abs/2502.13184v1",
    "arxiv_id": "2502.13184v1",
    "authors": [
      "Lars Ullrich",
      "Michael Buchholz",
      "Klaus Dietmayer",
      "Knut Graichen"
    ],
    "published": "2025-02-18T11:01:37+00:00",
    "summary": "Research in the field of automated vehicles, or more generally cognitive cyber-physical systems that operate in the real world, is leading to increasingly complex systems. Among other things, artificial intelligence enables an ever-increasing degree of autonomy. In this context, the V-model, which has served for decades as a process reference model of the system development lifecycle is reaching its limits. To the contrary, innovative processes and frameworks have been developed that take into account the characteristics of emerging autonomous systems. To bridge the gap and merge the different methodologies, we present an extension of the V-model for iterative data-based development processes that harmonizes and formalizes the existing methods towards a generic framework. The iterative approach allows for seamless integration of continuous system refinement. While the data-based approach constitutes the consideration of data-based development processes and formalizes the use of synthetic and real world data. In this way, formalizing the process of development, verification, validation, and continuous integration contributes to ensuring the safety of emerging complex systems that incorporate AI."
  },
  {
    "title": "IPSR Model: Misinformation Intervention through Prebunking in Social Networks",
    "url": "http://arxiv.org/abs/2502.12740v1",
    "arxiv_id": "2502.12740v1",
    "authors": [
      "Robert Rai",
      "Rajesh Sharma",
      "Chandrakala Meena"
    ],
    "published": "2025-02-18T10:56:30+00:00",
    "summary": "In the present digital world, the rapid spread of misinformation is not just an annoyance but a real threat to public safety, and our collective decision-making. Prebunking, a type of psychological immunization, can educate people about misinformation and lay a foundation of cognitive resilience that makes them more robust against future misinformation. We use a compartmental modeling approach inspired by vaccination models from epidemiology to model the effectiveness of prebunking misinformation. Populations are classified into different compartments based on the exposure to prebunking and the propagation of misinformation through online social networks. Specific rates dictate the transitions between such states, similar to how people traverse between susceptible, infected, and recovered compartments in classical epidemiological models. This model integrates different levels of prebunking potency, the fraction of the population prebunked initially, and the forgetting rate effects. To the best of our knowledge this is the first work which study the extent of prebunking interventions to reduce the scale of misinformation, much as vaccinations curtail the spread of infectious diseases."
  },
  {
    "title": "myEye2Wheeler: A Two-Wheeler Indian Driver Real-World Eye-Tracking Dataset",
    "url": "http://arxiv.org/abs/2502.12723v1",
    "arxiv_id": "2502.12723v1",
    "authors": [
      "Bhaiya Vaibhaw Kumar",
      "Deepti Rawat",
      "Tanvi Kandalla",
      "Aarnav Nagariya",
      "Kavita Vemuri"
    ],
    "published": "2025-02-18T10:39:00+00:00",
    "summary": "This paper presents the myEye2Wheeler dataset, a unique resource of real-world gaze behaviour of two-wheeler drivers navigating complex Indian traffic. Most datasets are from four-wheeler drivers on well-planned roads and homogeneous traffic. Our dataset offers a critical lens into the unique visual attention patterns and insights into the decision-making of Indian two-wheeler drivers. The analysis demonstrates that existing saliency models, like TASED-Net, perform less effectively on the myEye-2Wheeler dataset compared to when applied on the European 4-wheeler eye tracking datasets (DR(Eye)VE), highlighting the need for models specifically tailored to the traffic conditions. By introducing the dataset, we not only fill a significant gap in two-wheeler driver behaviour research in India but also emphasise the critical need for developing context-specific saliency models. The larger aim is to improve road safety for two-wheeler users and lane-planning to support a cost-effective mode of transport."
  },
  {
    "title": "SATA: Safe and Adaptive Torque-Based Locomotion Policies Inspired by Animal Learning",
    "url": "http://arxiv.org/abs/2502.12674v1",
    "arxiv_id": "2502.12674v1",
    "authors": [
      "Peizhuo Li",
      "Hongyi Li",
      "Ge Sun",
      "Jin Cheng",
      "Xinrong Yang",
      "Guillaume Bellegarda",
      "Milad Shafiee",
      "Yuhong Cao",
      "Auke Ijspeert",
      "Guillaume Sartoretti"
    ],
    "published": "2025-02-18T09:25:37+00:00",
    "summary": "Despite recent advances in learning-based controllers for legged robots, deployments in human-centric environments remain limited by safety concerns. Most of these approaches use position-based control, where policies output target joint angles that must be processed by a low-level controller (e.g., PD or impedance controllers) to compute joint torques. Although impressive results have been achieved in controlled real-world scenarios, these methods often struggle with compliance and adaptability when encountering environments or disturbances unseen during training, potentially resulting in extreme or unsafe behaviors. Inspired by how animals achieve smooth and adaptive movements by controlling muscle extension and contraction, torque-based policies offer a promising alternative by enabling precise and direct control of the actuators in torque space. In principle, this approach facilitates more effective interactions with the environment, resulting in safer and more adaptable behaviors. However, challenges such as a highly nonlinear state space and inefficient exploration during training have hindered their broader adoption. To address these limitations, we propose SATA, a bio-inspired framework that mimics key biomechanical principles and adaptive learning mechanisms observed in animal locomotion. Our approach effectively addresses the inherent challenges of learning torque-based policies by significantly improving early-stage exploration, leading to high-performance final policies. Remarkably, our method achieves zero-shot sim-to-real transfer. Our experimental results indicate that SATA demonstrates remarkable compliance and safety, even in challenging environments such as soft/slippery terrain or narrow passages, and under significant external disturbances, highlighting its potential for practical deployments in human-centric and safety-critical scenarios."
  },
  {
    "title": "The Hidden Risks of Large Reasoning Models: A Safety Assessment of R1",
    "url": "http://arxiv.org/abs/2502.12659v1",
    "arxiv_id": "2502.12659v1",
    "authors": [
      "Kaiwen Zhou",
      "Chengzhi Liu",
      "Xuandong Zhao",
      "Shreedhar Jangam",
      "Jayanth Srinivasa",
      "Gaowen Liu",
      "Dawn Song",
      "Xin Eric Wang"
    ],
    "published": "2025-02-18T09:06:07+00:00",
    "summary": "The rapid development of large reasoning models, such as OpenAI-o3 and DeepSeek-R1, has led to significant improvements in complex reasoning over non-reasoning large language models~(LLMs). However, their enhanced capabilities, combined with the open-source access of models like DeepSeek-R1, raise serious safety concerns, particularly regarding their potential for misuse. In this work, we present a comprehensive safety assessment of these reasoning models, leveraging established safety benchmarks to evaluate their compliance with safety regulations. Furthermore, we investigate their susceptibility to adversarial attacks, such as jailbreaking and prompt injection, to assess their robustness in real-world applications. Through our multi-faceted analysis, we uncover four key findings: (1) There is a significant safety gap between the open-source R1 models and the o3-mini model, on both safety benchmark and attack, suggesting more safety effort on R1 is needed. (2) The distilled reasoning model shows poorer safety performance compared to its safety-aligned base models. (3) The stronger the model's reasoning ability, the greater the potential harm it may cause when answering unsafe questions. (4) The thinking process in R1 models pose greater safety concerns than their final answers. Our study provides insights into the security implications of reasoning models and highlights the need for further advancements in R1 models' safety to close the gap."
  },
  {
    "title": "A Graph-Enhanced Deep-Reinforcement Learning Framework for the Aircraft Landing Problem",
    "url": "http://arxiv.org/abs/2502.12617v1",
    "arxiv_id": "2502.12617v1",
    "authors": [
      "Vatsal Maru"
    ],
    "published": "2025-02-18T08:02:17+00:00",
    "summary": "The Aircraft Landing Problem (ALP) is one of the challenging problems in aircraft transportation and management. The challenge is to schedule the arriving aircraft in a sequence so that the cost and delays are optimized. There are various solution approaches to solving this problem, most of which are based on operations research algorithms and meta-heuristics. Although traditional methods perform better on one or the other factors, there remains a problem of solving real-time rescheduling and computational scalability altogether. This paper presents a novel deep reinforcement learning (DRL) framework that combines graph neural networks with actor-critic architectures to address the ALP. This paper introduces three key contributions: A graph-based state representation that efficiently captures temporal and spatial relationships between aircraft, a specialized actor-critic architecture designed to handle multiple competing objectives in landing scheduling, and a runway balance strategy that ensures efficient resource utilization while maintaining safety constraints. The results show that the trained algorithm can be tested on different problem sets and the results are competitive to operation research algorithms. The experimental results on standard benchmark data sets demonstrate a 99.95 reduction in computational time compared to Mixed Integer Programming (MIP) and 38 higher runway throughput over First Come First Serve (FCFS) approaches. Therefore, the proposed solution is competitive to traditional approaches and achieves substantial advancements. Notably, it does not require retraining, making it particularly suitable for industrial deployment. The frameworks capability to generate solutions within 1 second enables real-time rescheduling, addressing critical requirements of air traffic management."
  },
  {
    "title": "Learning-based Dynamic Robot-to-Human Handover",
    "url": "http://arxiv.org/abs/2502.12602v1",
    "arxiv_id": "2502.12602v1",
    "authors": [
      "Hyeonseong Kim",
      "Chanwoo Kim",
      "Matthew Pan",
      "Kyungjae Lee",
      "Sungjoon Choi"
    ],
    "published": "2025-02-18T07:26:07+00:00",
    "summary": "This paper presents a novel learning-based approach to dynamic robot-to-human handover, addressing the challenges of delivering objects to a moving receiver. We hypothesize that dynamic handover, where the robot adjusts to the receiver's movements, results in more efficient and comfortable interaction compared to static handover, where the receiver is assumed to be stationary. To validate this, we developed a nonparametric method for generating continuous handover motion, conditioned on the receiver's movements, and trained the model using a dataset of 1,000 human-to-human handover demonstrations. We integrated preference learning for improved handover effectiveness and applied impedance control to ensure user safety and adaptiveness. The approach was evaluated in both simulation and real-world settings, with user studies demonstrating that dynamic handover significantly reduces handover time and improves user comfort compared to static methods. Videos and demonstrations of our approach are available at https://zerotohero7886.github.io/dyn-r2h-handover ."
  },
  {
    "title": "DemonAgent: Dynamically Encrypted Multi-Backdoor Implantation Attack on LLM-based Agent",
    "url": "http://arxiv.org/abs/2502.12575v1",
    "arxiv_id": "2502.12575v1",
    "authors": [
      "Pengyu Zhu",
      "Zhenhong Zhou",
      "Yuanhe Zhang",
      "Shilinlu Yan",
      "Kun Wang",
      "Sen Su"
    ],
    "published": "2025-02-18T06:26:15+00:00",
    "summary": "As LLM-based agents become increasingly prevalent, backdoors can be implanted into agents through user queries or environment feedback, raising critical concerns regarding safety vulnerabilities. However, backdoor attacks are typically detectable by safety audits that analyze the reasoning process of agents. To this end, we propose a novel backdoor implantation strategy called \\textbf{Dynamically Encrypted Multi-Backdoor Implantation Attack}. Specifically, we introduce dynamic encryption, which maps the backdoor into benign content, effectively circumventing safety audits. To enhance stealthiness, we further decompose the backdoor into multiple sub-backdoor fragments. Based on these advancements, backdoors are allowed to bypass safety audits significantly. Additionally, we present AgentBackdoorEval, a dataset designed for the comprehensive evaluation of agent backdoor attacks. Experimental results across multiple datasets demonstrate that our method achieves an attack success rate nearing 100\\% while maintaining a detection rate of 0\\%, illustrating its effectiveness in evading safety audits. Our findings highlight the limitations of existing safety mechanisms in detecting advanced attacks, underscoring the urgent need for more robust defenses against backdoor threats. Code and data are available at https://github.com/whfeLingYu/DemonAgent."
  },
  {
    "title": "Exploring the Impact of Personality Traits on LLM Bias and Toxicity",
    "url": "http://arxiv.org/abs/2502.12566v1",
    "arxiv_id": "2502.12566v1",
    "authors": [
      "Shuo Wang",
      "Renhao Li",
      "Xi Chen",
      "Yulin Yuan",
      "Derek F. Wong",
      "Min Yang"
    ],
    "published": "2025-02-18T06:07:09+00:00",
    "summary": "With the different roles that AI is expected to play in human life, imbuing large language models (LLMs) with different personalities has attracted increasing research interests. While the \"personification\" enhances human experiences of interactivity and adaptability of LLMs, it gives rise to critical concerns about content safety, particularly regarding bias, sentiment and toxicity of LLM generation. This study explores how assigning different personality traits to LLMs affects the toxicity and biases of their outputs. Leveraging the widely accepted HEXACO personality framework developed in social psychology, we design experimentally sound prompts to test three LLMs' performance on three toxic and bias benchmarks. The findings demonstrate the sensitivity of all three models to HEXACO personality traits and, more importantly, a consistent variation in the biases, negative sentiment and toxicity of their output. In particular, adjusting the levels of several personality traits can effectively reduce bias and toxicity in model performance, similar to humans' correlations between personality traits and toxic behaviors. The findings highlight the additional need to examine content safety besides the efficiency of training or fine-tuning methods for LLM personification. They also suggest a potential for the adjustment of personalities to be a simple and low-cost method to conduct controlled text generation."
  },
  {
    "title": "Self Iterative Label Refinement via Robust Unlabeled Learning",
    "url": "http://arxiv.org/abs/2502.12565v1",
    "arxiv_id": "2502.12565v1",
    "authors": [
      "Hikaru Asano",
      "Tadashi Kozuno",
      "Yukino Baba"
    ],
    "published": "2025-02-18T06:04:18+00:00",
    "summary": "Recent advances in large language models (LLMs) have yielded impressive performance on various tasks, yet they often depend on high-quality feedback that can be costly. Self-refinement methods attempt to leverage LLMs' internal evaluation mechanisms with minimal human supervision; however, these approaches frequently suffer from inherent biases and overconfidence, especially in domains where the models lack sufficient internal knowledge, resulting in performance degradation. As an initial step toward enhancing self-refinement for broader applications, we introduce an iterative refinement pipeline that employs the Unlabeled-Unlabeled learning framework to improve LLM-generated pseudo-labels for classification tasks. By exploiting two unlabeled datasets with differing positive class ratios, our approach iteratively denoises and refines the initial pseudo-labels, thereby mitigating the adverse effects of internal biases with minimal human supervision. Evaluations on diverse datasets, including low-resource language corpora, patent classifications, and protein structure categorizations, demonstrate that our method consistently outperforms both initial LLM's classification performance and the self-refinement approaches by cutting-edge models (e.g., GPT-4o and DeepSeek-R1)."
  },
  {
    "title": "SEA: Low-Resource Safety Alignment for Multimodal Large Language Models via Synthetic Embeddings",
    "url": "http://arxiv.org/abs/2502.12562v1",
    "arxiv_id": "2502.12562v1",
    "authors": [
      "Weikai Lu",
      "Hao Peng",
      "Huiping Zhuang",
      "Cen Chen",
      "Ziqian Zeng"
    ],
    "published": "2025-02-18T05:57:35+00:00",
    "summary": "Multimodal Large Language Models (MLLMs) have serious security vulnerabilities.While safety alignment using multimodal datasets consisting of text and data of additional modalities can effectively enhance MLLM's security, it is costly to construct these datasets. Existing low-resource security alignment methods, including textual alignment, have been found to struggle with the security risks posed by additional modalities. To address this, we propose Synthetic Embedding augmented safety Alignment (SEA), which optimizes embeddings of additional modality through gradient updates to expand textual datasets. This enables multimodal safety alignment training even when only textual data is available. Extensive experiments on image, video, and audio-based MLLMs demonstrate that SEA can synthesize a high-quality embedding on a single RTX3090 GPU within 24 seconds. SEA significantly improves the security of MLLMs when faced with threats from additional modalities. To assess the security risks introduced by video and audio, we also introduced a new benchmark called VA-SafetyBench. High attack success rates across multiple MLLMs validate its challenge. Our code and data will be available at https://github.com/ZeroNLP/SEA."
  },
  {
    "title": "From Maneuver to Mishap: A Systematic Literature Review on U-Turn Safety Risks",
    "url": "http://arxiv.org/abs/2502.12556v1",
    "arxiv_id": "2502.12556v1",
    "authors": [
      "Syed Aaqib Javed",
      "Anannya Ghosh Tusti",
      "Biplov Pandeym Subasish Das"
    ],
    "published": "2025-02-18T05:44:19+00:00",
    "summary": "Understanding the impacts of U-turn configurations on intersection safety and traffic operations is essential for developing effective strategies to enhance road safety and efficiency. Extensive research has been conducted to investigate the role of geometric designs, driver behavior, and advanced technologies in mitigating crash risks and improving traffic flow at U-turn facilities. By synthesizing this collective body of work through the guidelines of Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA), this paper provides a valuable resource for transportation professionals, policymakers, and researchers seeking evidence-based solutions. This systematic review draws on studies from diverse traffic environments and regional contexts, focusing on innovative design interventions, such as restricted crossing U-turns (RCUTs) and median U-turn intersections (MUTs), as well as integrated strategies leveraging technological advancements. By presenting a comprehensive analysis of U-turn-related challenges and opportunities, this review contributes to advancing transportation safety research and guiding the development of adaptive strategies tailored to varied traffic conditions and evolving technologies."
  },
  {
    "title": "From Maneuver to Mishap: A Systematic Literature Review on U-Turn Safety Risks",
    "url": "http://arxiv.org/abs/2502.12556v2",
    "arxiv_id": "2502.12556v2",
    "authors": [
      "Syed Aaqib Javed",
      "Anannya Ghosh Tusti",
      "Biplov Pandey",
      "Subasish Das"
    ],
    "published": "2025-02-18T05:44:19+00:00",
    "summary": "Understanding the impacts of U-turn configurations on intersection safety and traffic operations is essential for developing effective strategies to enhance road safety and efficiency. Extensive research has been conducted to investigate the role of geometric designs, driver behavior, and advanced technologies in mitigating crash risks and improving traffic flow at U-turn facilities. By synthesizing this collective body of work through the guidelines of Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA), this paper provides a valuable resource for transportation professionals, policymakers, and researchers seeking evidence-based solutions. This systematic review draws on studies from diverse traffic environments and regional contexts, focusing on innovative design interventions, such as restricted crossing U-turns (RCUTs) and median U-turn intersections (MUTs), as well as integrated strategies leveraging technological advancements. By presenting a comprehensive analysis of U-turn-related challenges and opportunities, this review contributes to advancing transportation safety research and guiding the development of adaptive strategies tailored to varied traffic conditions and evolving technologies."
  },
  {
    "title": "LLM Safety for Children",
    "url": "http://arxiv.org/abs/2502.12552v1",
    "arxiv_id": "2502.12552v1",
    "authors": [
      "Prasanjit Rath",
      "Hari Shrawgi",
      "Parag Agrawal",
      "Sandipan Dandapat"
    ],
    "published": "2025-02-18T05:26:27+00:00",
    "summary": "This paper analyzes the safety of Large Language Models (LLMs) in interactions with children below age of 18 years. Despite the transformative applications of LLMs in various aspects of children's lives such as education and therapy, there remains a significant gap in understanding and mitigating potential content harms specific to this demographic. The study acknowledges the diverse nature of children often overlooked by standard safety evaluations and proposes a comprehensive approach to evaluating LLM safety specifically for children. We list down potential risks that children may encounter when using LLM powered applications. Additionally we develop Child User Models that reflect the varied personalities and interests of children informed by literature in child care and psychology. These user models aim to bridge the existing gap in child safety literature across various fields. We utilize Child User Models to evaluate the safety of six state of the art LLMs. Our observations reveal significant safety gaps in LLMs particularly in categories harmful to children but not adults"
  },
  {
    "title": "Inference-Time Computations for LLM Reasoning and Planning: A Benchmark and Insights",
    "url": "http://arxiv.org/abs/2502.12521v1",
    "arxiv_id": "2502.12521v1",
    "authors": [
      "Shubham Parashar",
      "Blake Olson",
      "Sambhav Khurana",
      "Eric Li",
      "Hongyi Ling",
      "James Caverlee",
      "Shuiwang Ji"
    ],
    "published": "2025-02-18T04:11:29+00:00",
    "summary": "We examine the reasoning and planning capabilities of large language models (LLMs) in solving complex tasks. Recent advances in inference-time techniques demonstrate the potential to enhance LLM reasoning without additional training by exploring intermediate steps during inference. Notably, OpenAI's o1 model shows promising performance through its novel use of multi-step reasoning and verification. Here, we explore how scaling inference-time techniques can improve reasoning and planning, focusing on understanding the tradeoff between computational cost and performance. To this end, we construct a comprehensive benchmark, known as Sys2Bench, and perform extensive experiments evaluating existing inference-time techniques on eleven diverse tasks across five categories, including arithmetic reasoning, logical reasoning, common sense reasoning, algorithmic reasoning, and planning. Our findings indicate that simply scaling inference-time computation has limitations, as no single inference-time technique consistently performs well across all reasoning and planning tasks."
  },
  {
    "title": "SAFEERASER: Enhancing Safety in Multimodal Large Language Models through Multimodal Machine Unlearning",
    "url": "http://arxiv.org/abs/2502.12520v1",
    "arxiv_id": "2502.12520v1",
    "authors": [
      "Junkai Chen",
      "Zhijie Deng",
      "Kening Zheng",
      "Yibo Yan",
      "Shuliang Liu",
      "PeiJun Wu",
      "Peijie Jiang",
      "Jia Liu",
      "Xuming Hu"
    ],
    "published": "2025-02-18T04:09:46+00:00",
    "summary": "As Multimodal Large Language Models (MLLMs) develop, their potential security issues have become increasingly prominent. Machine Unlearning (MU), as an effective strategy for forgetting specific knowledge in training data, has been widely used in privacy protection. However, MU for safety in MLLM has yet to be fully explored. To address this issue, we propose SAFEERASER, a safety unlearning benchmark for MLLMs, consisting of 3,000 images and 28.8K VQA pairs. We comprehensively evaluate unlearning methods from two perspectives: forget quality and model utility. Our findings show that existing MU methods struggle to maintain model performance while implementing the forget operation and often suffer from over-forgetting. Hence, we introduce Prompt Decouple (PD) Loss to alleviate over-forgetting through decouple prompt during unlearning process. To quantitatively measure over-forgetting mitigated by PD Loss, we propose a new metric called Safe Answer Refusal Rate (SARR). Experimental results demonstrate that combining PD Loss with existing unlearning methods can effectively prevent over-forgetting and achieve a decrease of 79.5% in the SARR metric of LLaVA-7B and LLaVA-13B, while maintaining forget quality and model utility. Our code and dataset will be released upon acceptance. Warning: This paper contains examples of harmful language and images, and reader discretion is recommended."
  },
  {
    "title": "Local Flaw Detection with Adaptive Pyramid Image Fusion Across Spatial Sampling Resolution for SWRs",
    "url": "http://arxiv.org/abs/2502.12512v1",
    "arxiv_id": "2502.12512v1",
    "authors": [
      "Siyu You",
      "Huayi Gou",
      "Leilei Yang",
      "Zhiliang Liu",
      "Mingjian Zuo"
    ],
    "published": "2025-02-18T03:55:29+00:00",
    "summary": "The inspection of local flaws (LFs) in Steel Wire Ropes (SWRs) is crucial for ensuring safety and reliability in various industries. Magnetic Flux Leakage (MFL) imaging is commonly used for non-destructive testing, but its effectiveness is often hindered by the combined effects of inspection speed and sampling rate. To address this issue, the impacts of inspection speed and sampling rate on image quality are studied, as variations in these factors can cause stripe noise, axial compression of defect features, and increased interference, complicating accurate detection. We define the relationship between inspection speed and sampling rate as spatial sampling resolution (SSR) and propose an adaptive SSR target-feature-oriented (AS-TFO) method. This method incorporates adaptive adjustment and pyramid image fusion techniques to enhance defect detection under different SSR scenarios. Experimental results show that under high SSR scenarios, the method achieves a precision of 92.54% and recall of 98.41%. It remains robust under low SSR scenarios with a precision of 94.87% and recall of 97.37%. The overall results show that the proposed method outperforms conventional approaches, achieving state-of-the-art performance. This improvement in detection accuracy and robustness is particularly valuable for handling complex inspection conditions, where inspection speed and sampling rate can vary significantly, making detection more robust and reliable in industrial settings."
  },
  {
    "title": "Towards Robust and Secure Embodied AI: A Survey on Vulnerabilities and Attacks",
    "url": "http://arxiv.org/abs/2502.13175v1",
    "arxiv_id": "2502.13175v1",
    "authors": [
      "Wenpeng Xing",
      "Minghao Li",
      "Mohan Li",
      "Meng Han"
    ],
    "published": "2025-02-18T03:38:07+00:00",
    "summary": "Embodied AI systems, including robots and autonomous vehicles, are increasingly integrated into real-world applications, where they encounter a range of vulnerabilities stemming from both environmental and system-level factors. These vulnerabilities manifest through sensor spoofing, adversarial attacks, and failures in task and motion planning, posing significant challenges to robustness and safety. Despite the growing body of research, existing reviews rarely focus specifically on the unique safety and security challenges of embodied AI systems. Most prior work either addresses general AI vulnerabilities or focuses on isolated aspects, lacking a dedicated and unified framework tailored to embodied AI. This survey fills this critical gap by: (1) categorizing vulnerabilities specific to embodied AI into exogenous (e.g., physical attacks, cybersecurity threats) and endogenous (e.g., sensor failures, software flaws) origins; (2) systematically analyzing adversarial attack paradigms unique to embodied AI, with a focus on their impact on perception, decision-making, and embodied interaction; (3) investigating attack vectors targeting large vision-language models (LVLMs) and large language models (LLMs) within embodied systems, such as jailbreak attacks and instruction misinterpretation; (4) evaluating robustness challenges in algorithms for embodied perception, decision-making, and task planning; and (5) proposing targeted strategies to enhance the safety and reliability of embodied AI systems. By integrating these dimensions, we provide a comprehensive framework for understanding the interplay between vulnerabilities and safety in embodied AI."
  },
  {
    "title": "SoK: Understanding Vulnerabilities in the Large Language Model Supply Chain",
    "url": "http://arxiv.org/abs/2502.12497v1",
    "arxiv_id": "2502.12497v1",
    "authors": [
      "Shenao Wang",
      "Yanjie Zhao",
      "Zhao Liu",
      "Quanchen Zou",
      "Haoyu Wang"
    ],
    "published": "2025-02-18T03:22:38+00:00",
    "summary": "Large Language Models (LLMs) transform artificial intelligence, driving advancements in natural language understanding, text generation, and autonomous systems. The increasing complexity of their development and deployment introduces significant security challenges, particularly within the LLM supply chain. However, existing research primarily focuses on content safety, such as adversarial attacks, jailbreaking, and backdoor attacks, while overlooking security vulnerabilities in the underlying software systems. To address this gap, this study systematically analyzes 529 vulnerabilities reported across 75 prominent projects spanning 13 lifecycle stages. The findings show that vulnerabilities are concentrated in the application (50.3%) and model (42.7%) layers, with improper resource control (45.7%) and improper neutralization (25.1%) identified as the leading root causes. Additionally, while 56.7% of the vulnerabilities have available fixes, 8% of these patches are ineffective, resulting in recurring vulnerabilities. This study underscores the challenges of securing the LLM ecosystem and provides actionable insights to guide future research and mitigation strategies."
  },
  {
    "title": "Safe at the Margins: A General Approach to Safety Alignment in Low-Resource English Languages -- A Singlish Case Study",
    "url": "http://arxiv.org/abs/2502.12485v1",
    "arxiv_id": "2502.12485v1",
    "authors": [
      "Isaac Lim",
      "Shaun Khoo",
      "Watson Chua",
      "Goh Jiayi",
      "Jessica Foo"
    ],
    "published": "2025-02-18T03:11:06+00:00",
    "summary": "To ensure safe usage, Large Language Models (LLMs) typically undergo alignment with human-defined values. However, this alignment often relies on primarily English data and is biased towards Western-centric values, limiting its effectiveness in low-resource language settings. In this paper, we describe our approach for aligning SEA-Lion-v2.1-Instruct (a Llama3-8B variant) to minimize toxicity in Singlish, an English creole specific to Singapore. We find that supervised fine-tuning and Kahneman-Tversky Optimization (KTO) on paired and unpaired preferences is more sample efficient and yields significantly better results than Direct Preference Optimization (DPO). Our analysis reveals that DPO implicitly enforces a weaker safety objective than KTO, and that SFT complements KTO by improving training stability. Finally, we introduce a simple but novel modification to KTO, KTO-S, which improves training stability through better gradient exploitation. Overall, we present a general approach for safety alignment conducive to low-resource English languages, successfully reducing toxicity by 99\\% on our Singlish benchmark, with gains generalizing to the broader TOXIGEN dataset while maintaining strong performance across standard LLM benchmarks."
  },
  {
    "title": "MCTS-Judge: Test-Time Scaling in LLM-as-a-Judge for Code Correctness Evaluation",
    "url": "http://arxiv.org/abs/2502.12468v1",
    "arxiv_id": "2502.12468v1",
    "authors": [
      "Yutong Wang",
      "Pengliang Ji",
      "Chaoqun Yang",
      "Kaixin Li",
      "Ming Hu",
      "Jiaoyang Li",
      "Guillaume Sartoretti"
    ],
    "published": "2025-02-18T02:55:48+00:00",
    "summary": "The LLM-as-a-Judge paradigm shows promise for evaluating generative content but lacks reliability in reasoning-intensive scenarios, such as programming. Inspired by recent advances in reasoning models and shifts in scaling laws, we pioneer bringing test-time computation into LLM-as-a-Judge, proposing MCTS-Judge, a resource-efficient, System-2 thinking framework for code correctness evaluation. MCTS-Judge leverages Monte Carlo Tree Search (MCTS) to decompose problems into simpler, multi-perspective evaluations. Through a node-selection strategy that combines self-assessment based on historical actions in the current trajectory and the Upper Confidence Bound for Trees based on prior rollouts, MCTS-Judge balances global optimization and refinement of the current trajectory. We further designed a high-precision, unit-test-level reward mechanism to encourage the Large Language Model (LLM) to perform line-by-line analysis. Extensive experiments on three benchmarks and five LLMs demonstrate the effectiveness of MCTS-Judge, which improves the base model's accuracy from 41% to 80%, surpassing the o1-series models with 3x fewer tokens. Further evaluations validate the superiority of its reasoning trajectory in logic, analytics, thoroughness, and overall quality, while revealing the test-time scaling law of the LLM-as-a-Judge paradigm."
  },
  {
    "title": "EquiBench: Benchmarking Code Reasoning Capabilities of Large Language Models via Equivalence Checking",
    "url": "http://arxiv.org/abs/2502.12466v1",
    "arxiv_id": "2502.12466v1",
    "authors": [
      "Anjiang Wei",
      "Jiannan Cao",
      "Ran Li",
      "Hongyu Chen",
      "Yuhui Zhang",
      "Ziheng Wang",
      "Yaofeng Sun",
      "Yuan Liu",
      "Thiago S. F. X. Teixeira",
      "Diyi Yang",
      "Ke Wang",
      "Alex Aiken"
    ],
    "published": "2025-02-18T02:54:25+00:00",
    "summary": "Equivalence checking, i.e., determining whether two programs produce identical outputs for all possible inputs, underpins a broad range of applications, including software refactoring, testing, and optimization. We present the task of equivalence checking as a new way to evaluate the code reasoning abilities of large language models (LLMs). We introduce EquiBench, a dataset of 2400 program pairs spanning four programming languages and six equivalence categories. These pairs are systematically generated through program analysis, compiler scheduling, and superoptimization, covering nontrivial structural transformations that demand deep semantic reasoning beyond simple syntactic variations. Our evaluation of 17 state-of-the-art LLMs shows that OpenAI o3-mini achieves the highest overall accuracy of 78.0%. In the most challenging categories, the best accuracies are 62.3% and 68.8%, only modestly above the 50% random baseline for binary classification, indicating significant room for improvement in current models' code reasoning capabilities."
  },
  {
    "title": "SafeRoute: Adaptive Model Selection for Efficient and Accurate Safety Guardrails in Large Language Models",
    "url": "http://arxiv.org/abs/2502.12464v1",
    "arxiv_id": "2502.12464v1",
    "authors": [
      "Seanie Lee",
      "Dong Bok Lee",
      "Dominik Wagner",
      "Minki Kang",
      "Haebin Seong",
      "Tobias Bocklet",
      "Juho Lee",
      "Sung Ju Hwang"
    ],
    "published": "2025-02-18T02:51:17+00:00",
    "summary": "Deploying large language models (LLMs) in real-world applications requires robust safety guard models to detect and block harmful user prompts. While large safety guard models achieve strong performance, their computational cost is substantial. To mitigate this, smaller distilled models are used, but they often underperform on \"hard\" examples where the larger model provides accurate predictions. We observe that many inputs can be reliably handled by the smaller model, while only a small fraction require the larger model's capacity. Motivated by this, we propose SafeRoute, a binary router that distinguishes hard examples from easy ones. Our method selectively applies the larger safety guard model to the data that the router considers hard, improving efficiency while maintaining accuracy compared to solely using the larger safety guard model. Experimental results on multiple benchmark datasets demonstrate that our adaptive model selection significantly enhances the trade-off between computational cost and safety performance, outperforming relevant baselines."
  },
  {
    "title": "Computational Safety for Generative AI: A Signal Processing Perspective",
    "url": "http://arxiv.org/abs/2502.12445v1",
    "arxiv_id": "2502.12445v1",
    "authors": [
      "Pin-Yu Chen"
    ],
    "published": "2025-02-18T02:26:50+00:00",
    "summary": "AI safety is a rapidly growing area of research that seeks to prevent the harm and misuse of frontier AI technology, particularly with respect to generative AI (GenAI) tools that are capable of creating realistic and high-quality content through text prompts. Examples of such tools include large language models (LLMs) and text-to-image (T2I) diffusion models. As the performance of various leading GenAI models approaches saturation due to similar training data sources and neural network architecture designs, the development of reliable safety guardrails has become a key differentiator for responsibility and sustainability. This paper presents a formalization of the concept of computational safety, which is a mathematical framework that enables the quantitative assessment, formulation, and study of safety challenges in GenAI through the lens of signal processing theory and methods. In particular, we explore two exemplary categories of computational safety challenges in GenAI that can be formulated as hypothesis testing problems. For the safety of model input, we show how sensitivity analysis and loss landscape analysis can be used to detect malicious prompts with jailbreak attempts. For the safety of model output, we elucidate how statistical signal processing and adversarial learning can be used to detect AI-generated content. Finally, we discuss key open research challenges, opportunities, and the essential role of signal processing in computational AI safety."
  },
  {
    "title": "Gradient Co-occurrence Analysis for Detecting Unsafe Prompts in Large Language Models",
    "url": "http://arxiv.org/abs/2502.12411v1",
    "arxiv_id": "2502.12411v1",
    "authors": [
      "Jingyuan Yang",
      "Bowen Yan",
      "Rongjun Li",
      "Ziyu Zhou",
      "Xin Chen",
      "Zhiyong Feng",
      "Wei Peng"
    ],
    "published": "2025-02-18T01:14:46+00:00",
    "summary": "Unsafe prompts pose significant safety risks to large language models (LLMs). Existing methods for detecting unsafe prompts rely on data-driven fine-tuning to train guardrail models, necessitating significant data and computational resources. In contrast, recent few-shot gradient-based methods emerge, requiring only few safe and unsafe reference prompts. A gradient-based approach identifies unsafe prompts by analyzing consistent patterns of the gradients of safety-critical parameters in LLMs. Although effective, its restriction to directional similarity (cosine similarity) introduces ``directional bias'', limiting its capability to identify unsafe prompts. To overcome this limitation, we introduce GradCoo, a novel gradient co-occurrence analysis method that expands the scope of safety-critical parameter identification to include unsigned gradient similarity, thereby reducing the impact of ``directional bias'' and enhancing the accuracy of unsafe prompt detection. Comprehensive experiments on the widely-used benchmark datasets ToxicChat and XStest demonstrate that our proposed method can achieve state-of-the-art (SOTA) performance compared to existing methods. Moreover, we confirm the generalizability of GradCoo in detecting unsafe prompts across a range of LLM base models with various sizes and origins."
  },
  {
    "title": "Reward-Safety Balance in Offline Safe RL via Diffusion Regularization",
    "url": "http://arxiv.org/abs/2502.12391v1",
    "arxiv_id": "2502.12391v1",
    "authors": [
      "Junyu Guo",
      "Zhi Zheng",
      "Donghao Ying",
      "Ming Jin",
      "Shangding Gu",
      "Costas Spanos",
      "Javad Lavaei"
    ],
    "published": "2025-02-18T00:00:03+00:00",
    "summary": "Constrained reinforcement learning (RL) seeks high-performance policies under safety constraints. We focus on an offline setting where the agent has only a fixed dataset -- common in realistic tasks to prevent unsafe exploration. To address this, we propose Diffusion-Regularized Constrained Offline Reinforcement Learning (DRCORL), which first uses a diffusion model to capture the behavioral policy from offline data and then extracts a simplified policy to enable efficient inference. We further apply gradient manipulation for safety adaptation, balancing the reward objective and constraint satisfaction. This approach leverages high-quality offline data while incorporating safety requirements. Empirical results show that DRCORL achieves reliable safety performance, fast inference, and strong reward outcomes across robot learning tasks. Compared to existing safe offline RL methods, it consistently meets cost limits and performs well with the same hyperparameters, indicating practical applicability in real-world scenarios."
  },
  {
    "title": "Locally-Deployed Chain-of-Thought (CoT) Reasoning Model in Chemical Engineering: Starting from 30 Experimental Data",
    "url": "http://arxiv.org/abs/2502.12383v1",
    "arxiv_id": "2502.12383v1",
    "authors": [
      "Tianhang Zhou",
      "Yingchun Niu",
      "Xingying Lan",
      "Chunming Xu"
    ],
    "published": "2025-02-17T23:43:48+00:00",
    "summary": "In the field of chemical engineering, traditional data-processing and prediction methods face significant challenges. Machine-learning and large-language models (LLMs) also have their respective limitations. This paper explores the application of the Chain-of-Thought (CoT) reasoning model in chemical engineering, starting from 30 experimental data points. By integrating traditional surrogate models like Gaussian processes and random forests with powerful LLMs such as DeepSeek-R1, a hierarchical architecture is proposed. Two CoT-building methods, Large Language Model-Chain of Thought (LLM-CoT) and Machine Learning-Large Language Model-Chain of Thought (ML-LLM-CoT), are studied. The LLM-CoT combines local models DeepSeek-r1:14b and Qwen2:7b with Ollama. The ML-LLM-CoT integrates a pre-trained Gaussian ML model with the LLM-based CoT framework. Our results show that during construction, ML-LLM-CoT is more efficient. It only has 2 points that require rethink and a total of 4 rethink times, while LLM-CoT has 5 points that need to be re-thought and 34 total rethink times. In predicting the solubility of 20 molecules with dissimilar structures, the number of molecules with a prediction deviation higher than 100\\% for the Gaussian model, LLM-CoT, and ML-LLM-CoT is 7, 6, and 4 respectively. These results indicate that ML-LLM-CoT performs better in controlling the number of high-deviation molecules, optimizing the average deviation, and achieving a higher success rate in solubility judgment, providing a more reliable method for chemical engineering and molecular property prediction. This study breaks through the limitations of traditional methods and offers new solutions for rapid property prediction and process optimization in chemical engineering."
  },
  {
    "title": "Soft Robotics for Search and Rescue: Advancements, Challenges, and Future Directions",
    "url": "http://arxiv.org/abs/2502.12373v1",
    "arxiv_id": "2502.12373v1",
    "authors": [
      "Abhishek Sebastian"
    ],
    "published": "2025-02-17T23:24:18+00:00",
    "summary": "Soft robotics has emerged as a transformative technology in Search and Rescue (SAR) operations, addressing challenges in navigating complex, hazardous environments that often limit traditional rigid robots. This paper critically examines advancements in soft robotic technologies tailored for SAR applications, focusing on their unique capabilities in adaptability, safety, and efficiency. By leveraging bio-inspired designs, flexible materials, and advanced locomotion mechanisms, such as crawling, rolling, and shape morphing, soft robots demonstrate exceptional potential in disaster scenarios. However, significant barriers persist, including material durability, power inefficiency, sensor integration, and control complexity. This comprehensive review highlights the current state of soft robotics in SAR, discusses simulation methodologies and hardware validations, and introduces performance metrics essential for their evaluation. By bridging the gap between theoretical advancements and practical deployment, this study underscores the potential of soft robotic systems to revolutionize SAR missions and advocates for continued interdisciplinary innovation to overcome existing limitations."
  },
  {
    "title": "Detecting Systematic Weaknesses in Vision Models along Predefined Human-Understandable Dimensions",
    "url": "http://arxiv.org/abs/2502.12360v1",
    "arxiv_id": "2502.12360v1",
    "authors": [
      "Sujan Sai Gannamaneni",
      "Rohil Prakash Rao",
      "Michael Mock",
      "Maram Akila",
      "Stefan Wrobel"
    ],
    "published": "2025-02-17T22:50:45+00:00",
    "summary": "Studying systematic weaknesses of DNNs has gained prominence in the last few years with the rising focus on building safe AI systems. Slice discovery methods (SDMs) are prominent algorithmic approaches for finding such systematic weaknesses. They identify top-k semantically coherent slices/subsets of data where a DNN-under-test has low performance. For being directly useful, e.g., as evidences in a safety argumentation, slices should be aligned with human-understandable (safety-relevant) dimensions, which, for example, are defined by safety and domain experts as parts of the operational design domain (ODD). While straightforward for structured data, the lack of semantic metadata makes these investigations challenging for unstructured data. Therefore, we propose a complete workflow which combines contemporary foundation models with algorithms for combinatorial search that consider structured data and DNN errors for finding systematic weaknesses in images. In contrast to existing approaches, ours identifies weak slices that are in line with predefined human-understandable dimensions. As the workflow includes foundation models, its intermediate and final results may not always be exact. Therefore, we build into our workflow an approach to address the impact of noisy metadata. We evaluate our approach w.r.t. its quality on four popular computer vision datasets, including autonomous driving datasets like Cityscapes, BDD100k, and RailSem19, while using multiple state-of-the-art models as DNNs-under-test."
  },
  {
    "title": "Asymptotic safety, quantum gravity, and the swampland: a conceptual assessment",
    "url": "http://arxiv.org/abs/2502.12290v1",
    "arxiv_id": "2502.12290v1",
    "authors": [
      "Ivano Basile",
      "Benjamin Knorr",
      "Alessia Platania",
      "Marc Schiffer"
    ],
    "published": "2025-02-17T20:00:06+00:00",
    "summary": "We provide a conceptual assessment of some aspects of fundamental quantum field theories of gravity in light of foundational aspects of the swampland program. On the one hand, asymptotically safe quantum gravity may provide a simple and predictive framework, thanks to a finite number of relevant parameters. On the other hand, a (sub-)set of intertwined swampland conjectures on the consistency of quantum gravity can be argued to be universal via effective field theory considerations. We answer whether some foundational features of these frameworks are compatible. This involves revisiting and refining several arguments (and loopholes) concerning the relation between field-theoretic descriptions of gravity and general swampland ideas. We identify the thermodynamics of black holes, spacetime topology change, and holography as the core aspects of this relation. We draw lessons on the features that a field theoretic description of gravity must (not) have to be consistent with fundamental principles underlying the swampland program, and on the universality of the latter."
  },
  {
    "title": "Integrating Expert Knowledge into Logical Programs via LLMs",
    "url": "http://arxiv.org/abs/2502.12275v1",
    "arxiv_id": "2502.12275v1",
    "authors": [
      "Franciszek G\u00f3rski",
      "Oskar Wysocki",
      "Marco Valentino",
      "Andre Freitas"
    ],
    "published": "2025-02-17T19:18:23+00:00",
    "summary": "This paper introduces ExKLoP, a novel framework designed to evaluate how effectively Large Language Models (LLMs) integrate expert knowledge into logical reasoning systems. This capability is especially valuable in engineering, where expert knowledge-such as manufacturer-recommended operational ranges-can be directly embedded into automated monitoring systems. By mirroring expert verification steps, tasks like range checking and constraint validation help ensure system safety and reliability. Our approach systematically evaluates LLM-generated logical rules, assessing both syntactic fluency and logical correctness in these critical validation tasks. We also explore the models capacity for self-correction via an iterative feedback loop based on code execution outcomes. ExKLoP presents an extensible dataset comprising 130 engineering premises, 950 prompts, and corresponding validation points. It enables comprehensive benchmarking while allowing control over task complexity and scalability of experiments. We leverage the synthetic data creation methodology to conduct extensive empirical evaluation on a diverse set of LLMs including Llama3, Gemma, Mixtral, Mistral, and Qwen. Results reveal that while models generate nearly perfect syntactically correct code, they frequently exhibit logical errors in translating expert knowledge. Furthermore, iterative self-correction yields only marginal improvements (up to 3%). Overall, ExKLoP serves as a robust evaluation platform that streamlines the selection of effective models for self-correcting systems while clearly delineating the types of errors encountered. The complete implementation, along with all relevant data, is available at GitHub."
  },
  {
    "title": "NeuroStrata: Harnessing Neurosymbolic Paradigms for Improved Design, Testability, and Verifiability of Autonomous CPS",
    "url": "http://arxiv.org/abs/2502.12267v1",
    "arxiv_id": "2502.12267v1",
    "authors": [
      "Xi Zheng",
      "Ziyang Li",
      "Ivan Ruchkin",
      "Ruzica Piskac",
      "Miroslav Pajic"
    ],
    "published": "2025-02-17T19:07:41+00:00",
    "summary": "Autonomous cyber-physical systems (CPSs) leverage AI for perception, planning, and control but face trust and safety certification challenges due to inherent uncertainties. The neurosymbolic paradigm replaces stochastic layers with interpretable symbolic AI, enabling determinism. While promising, challenges like multisensor fusion, adaptability, and verification remain. This paper introduces NeuroStrata, a neurosymbolic framework to enhance the testing and verification of autonomous CPS. We outline its key components, present early results, and detail future plans."
  },
  {
    "title": "SmokeNet: Efficient Smoke Segmentation Leveraging Multiscale Convolutions and Multiview Attention Mechanisms",
    "url": "http://arxiv.org/abs/2502.12258v1",
    "arxiv_id": "2502.12258v1",
    "authors": [
      "Xuesong Liu",
      "Emmett J. Ientilucci"
    ],
    "published": "2025-02-17T19:01:27+00:00",
    "summary": "Efficient segmentation of smoke plumes is crucial for environmental monitoring and industrial safety, enabling the detection and mitigation of harmful emissions from activities like quarry blasts and wildfires. Accurate segmentation facilitates environmental impact assessments, timely interventions, and compliance with safety standards. However, existing models often face high computational demands and limited adaptability to diverse smoke appearances, restricting their deployment in resource-constrained environments. To address these issues, we introduce SmokeNet, a novel deep learning architecture that leverages multiscale convolutions and multiview linear attention mechanisms combined with layer-specific loss functions to handle the complex dynamics of diverse smoke plumes, ensuring efficient and accurate segmentation across varied environments. Additionally, we evaluate SmokeNet's performance and versatility using four datasets, including our quarry blast smoke dataset made available to the community. The results demonstrate that SmokeNet maintains a favorable balance between computational efficiency and segmentation accuracy, making it suitable for deployment in environmental monitoring and safety management systems. By contributing a new dataset and offering an efficient segmentation model, SmokeNet advances smoke segmentation capabilities in diverse and challenging environments."
  },
  {
    "title": "Can LLMs Simulate Social Media Engagement? A Study on Action-Guided Response Generation",
    "url": "http://arxiv.org/abs/2502.12073v1",
    "arxiv_id": "2502.12073v1",
    "authors": [
      "Zhongyi Qiu",
      "Hanjia Lyu",
      "Wei Xiong",
      "Jiebo Luo"
    ],
    "published": "2025-02-17T17:43:08+00:00",
    "summary": "Social media enables dynamic user engagement with trending topics, and recent research has explored the potential of large language models (LLMs) for response generation. While some studies investigate LLMs as agents for simulating user behavior on social media, their focus remains on practical viability and scalability rather than a deeper understanding of how well LLM aligns with human behavior. This paper analyzes LLMs' ability to simulate social media engagement through action guided response generation, where a model first predicts a user's most likely engagement action-retweet, quote, or rewrite-towards a trending post before generating a personalized response conditioned on the predicted action. We benchmark GPT-4o-mini, O1-mini, and DeepSeek-R1 in social media engagement simulation regarding a major societal event discussed on X. Our findings reveal that zero-shot LLMs underperform BERT in action prediction, while few-shot prompting initially degrades the prediction accuracy of LLMs with limited examples. However, in response generation, few-shot LLMs achieve stronger semantic alignment with ground truth posts."
  },
  {
    "title": "TokenSkip: Controllable Chain-of-Thought Compression in LLMs",
    "url": "http://arxiv.org/abs/2502.12067v1",
    "arxiv_id": "2502.12067v1",
    "authors": [
      "Heming Xia",
      "Yongqi Li",
      "Chak Tou Leong",
      "Wenjie Wang",
      "Wenjie Li"
    ],
    "published": "2025-02-17T17:37:26+00:00",
    "summary": "Chain-of-Thought (CoT) has been proven effective in enhancing the reasoning capabilities of large language models (LLMs). Recent advancements, such as OpenAI's o1 and DeepSeek-R1, suggest that scaling up the length of CoT sequences during inference could further boost LLM reasoning performance. However, due to the autoregressive nature of LLM decoding, longer CoT outputs lead to a linear increase in inference latency, adversely affecting user experience, particularly when the CoT exceeds 10,000 tokens. To address this limitation, we analyze the semantic importance of tokens within CoT outputs and reveal that their contributions to reasoning vary. Building on this insight, we propose TokenSkip, a simple yet effective approach that enables LLMs to selectively skip less important tokens, allowing for controllable CoT compression. Extensive experiments across various models and tasks demonstrate the effectiveness of TokenSkip in reducing CoT token usage while preserving strong reasoning performance. Notably, when applied to Qwen2.5-14B-Instruct, TokenSkip reduces reasoning tokens by 40% (from 313 to 181) on GSM8K, with less than a 0.4% performance drop."
  },
  {
    "title": "PhysReason: A Comprehensive Benchmark towards Physics-Based Reasoning",
    "url": "http://arxiv.org/abs/2502.12054v1",
    "arxiv_id": "2502.12054v1",
    "authors": [
      "Xinyu Zhang",
      "Yuxuan Dong",
      "Yanrui Wu",
      "Jiaxing Huang",
      "Chengyou Jia",
      "Basura Fernando",
      "Mike Zheng Shou",
      "Lingling Zhang",
      "Jun Liu"
    ],
    "published": "2025-02-17T17:24:14+00:00",
    "summary": "Large language models demonstrate remarkable capabilities across various domains, especially mathematics and logic reasoning. However, current evaluations overlook physics-based reasoning - a complex task requiring physics theorems and constraints. We present PhysReason, a 1,200-problem benchmark comprising knowledge-based (25%) and reasoning-based (75%) problems, where the latter are divided into three difficulty levels (easy, medium, hard). Notably, problems require an average of 8.1 solution steps, with hard requiring 15.6, reflecting the complexity of physics-based reasoning. We propose the Physics Solution Auto Scoring Framework, incorporating efficient answer-level and comprehensive step-level evaluations. Top-performing models like Deepseek-R1, Gemini-2.0-Flash-Thinking, and o3-mini-high achieve less than 60% on answer-level evaluation, with performance dropping from knowledge questions (75.11%) to hard problems (31.95%). Through step-level evaluation, we identified four key bottlenecks: Physics Theorem Application, Physics Process Understanding, Calculation, and Physics Condition Analysis. These findings position PhysReason as a novel and comprehensive benchmark for evaluating physics-based reasoning capabilities in large language models. Our code and data will be published at https:/dxzxy12138.github.io/PhysReason."
  },
  {
    "title": "SafeChain: Safety of Language Models with Long Chain-of-Thought Reasoning Capabilities",
    "url": "http://arxiv.org/abs/2502.12025v1",
    "arxiv_id": "2502.12025v1",
    "authors": [
      "Fengqing Jiang",
      "Zhangchen Xu",
      "Yuetai Li",
      "Luyao Niu",
      "Zhen Xiang",
      "Bo Li",
      "Bill Yuchen Lin",
      "Radha Poovendran"
    ],
    "published": "2025-02-17T16:57:56+00:00",
    "summary": "Emerging large reasoning models (LRMs), such as DeepSeek-R1 models, leverage long chain-of-thought (CoT) reasoning to generate structured intermediate steps, enhancing their reasoning capabilities. However, long CoT does not inherently guarantee safe outputs, potentially leading to harmful consequences such as the introduction of security vulnerabilities in code or the spread of misinformation. Current research on large language model (LLM) safety usually focuses on short-answer responses, overlooking the long CoT style outputs of LRMs. To bridge this gap, we conduct a systematic study of LRM safety. First, we investigate safety evaluators calibrated against human annotations. Using our newly developed metrics, we thoroughly assess the safety of 12 state-of-the-art LRMs on StrongReject and WildJailbreak datasets. Our results show that LRMs are not safe compared to their reasoning advance. Further, we perform a fine-grained analysis of the reasoning trace and final answer. We find that three decoding strategies-ZeroThink, LessThink, and MoreThink-can improve model safety without additional training. However, these strategies either use constrained reasoning traces or incur high inference costs. To better strengthen LRM safety, we introduce SafeChain, the first-of-its-kind safety training dataset in CoT style. We fine-tune two LRMs with SafeChain, showing that it not only enhances model safety but also preserves performance across 6 reasoning benchmarks."
  },
  {
    "title": "Atom of Thoughts for Markov LLM Test-Time Scaling",
    "url": "http://arxiv.org/abs/2502.12018v1",
    "arxiv_id": "2502.12018v1",
    "authors": [
      "Fengwei Teng",
      "Zhaoyang Yu",
      "Quan Shi",
      "Jiayi Zhang",
      "Chenglin Wu",
      "Yuyu Luo"
    ],
    "published": "2025-02-17T16:52:42+00:00",
    "summary": "Large Language Models (LLMs) achieve superior performance through training-time scaling, and test-time scaling further enhances their capabilities by conducting effective reasoning during inference. However, as the scale of reasoning increases, existing test-time scaling methods suffer from accumulated historical information, which not only wastes computational resources but also interferes with effective reasoning. To address this issue, we observe that complex reasoning progress is often achieved by solving a sequence of independent subquestions, each being self-contained and verifiable. These subquestions are essentially atomic questions, relying primarily on their current state rather than accumulated history, similar to the memoryless transitions in a Markov process. Based on this observation, we propose Atom of Thoughts (AoT), where each state transition in the reasoning process consists of decomposing the current question into a dependency-based directed acyclic graph and contracting its subquestions, forming a new atomic question state. This iterative decomposition-contraction process continues until reaching directly solvable atomic questions, naturally realizing Markov transitions between question states. Furthermore, these atomic questions can be seamlessly integrated into existing test-time scaling methods, enabling AoT to serve as a plug-in enhancement for improving reasoning capabilities. Experiments across six benchmarks demonstrate the effectiveness of AoT both as a standalone framework and a plug-in enhancement. Notably, on HotpotQA, when applied to gpt-4o-mini, AoT achieves an 80.6% F1 score, surpassing o3-mini by 3.4% and DeepSeek-R1 by 10.6%. The code will be available at https://github.com/qixucen/atom."
  },
  {
    "title": "LIMR: Less is More for RL Scaling",
    "url": "http://arxiv.org/abs/2502.11886v1",
    "arxiv_id": "2502.11886v1",
    "authors": [
      "Xuefeng Li",
      "Haoyang Zou",
      "Pengfei Liu"
    ],
    "published": "2025-02-17T15:13:29+00:00",
    "summary": "In this paper, we ask: what truly determines the effectiveness of RL training data for enhancing language models' reasoning capabilities? While recent advances like o1, Deepseek R1, and Kimi1.5 demonstrate RL's potential, the lack of transparency about training data requirements has hindered systematic progress. Starting directly from base models without distillation, we challenge the assumption that scaling up RL training data inherently improves performance. we demonstrate that a strategically selected subset of just 1,389 samples can outperform the full 8,523-sample dataset. We introduce Learning Impact Measurement (LIM), an automated method to evaluate and prioritize training samples based on their alignment with model learning trajectories, enabling efficient resource utilization and scalable implementation. Our method achieves comparable or even superior performance using only 1,389 samples versus the full 8,523 samples dataset. Notably, while recent data-efficient approaches (e.g., LIMO and s1) show promise with 32B-scale models, we find it significantly underperforms at 7B-scale through supervised fine-tuning (SFT). In contrast, our RL-based LIMR achieves 16.7% higher accuracy on AIME24 and outperforms LIMO and s1 by 13.0% and 22.2% on MATH500. These results fundamentally reshape our understanding of RL scaling in LLMs, demonstrating that precise sample selection, rather than data scale, may be the key to unlocking enhanced reasoning capabilities. For reproducible research and future innovation, we are open-sourcing LIMR, including implementation of LIM, training and evaluation code, curated datasets, and trained models at https://github.com/GAIR-NLP/LIMR."
  },
  {
    "title": "Hypothesis-Driven Theory-of-Mind Reasoning for Large Language Models",
    "url": "http://arxiv.org/abs/2502.11881v1",
    "arxiv_id": "2502.11881v1",
    "authors": [
      "Hyunwoo Kim",
      "Melanie Sclar",
      "Tan Zhi-Xuan",
      "Lance Ying",
      "Sydney Levine",
      "Yang Liu",
      "Joshua B. Tenenbaum",
      "Yejin Choi"
    ],
    "published": "2025-02-17T15:08:50+00:00",
    "summary": "Existing LLM reasoning methods have shown impressive capabilities across various tasks, such as solving math and coding problems. However, applying these methods to scenarios without ground-truth answers or rule-based verification methods - such as tracking the mental states of an agent - remains challenging. Inspired by the sequential Monte Carlo algorithm, we introduce thought-tracing, an inference-time reasoning algorithm designed to trace the mental states of specific agents by generating hypotheses and weighting them based on observations without relying on ground-truth solutions to questions in datasets. Our algorithm is modeled after the Bayesian theory-of-mind framework, using LLMs to approximate probabilistic inference over agents' evolving mental states based on their perceptions and actions. We evaluate thought-tracing on diverse theory-of-mind benchmarks, demonstrating significant performance improvements compared to baseline LLMs. Our experiments also reveal interesting behaviors of the recent reasoning models - e.g., o1 and R1 - on theory-of-mind, highlighting the difference of social reasoning compared to other domains."
  },
  {
    "title": "StructTransform: A Scalable Attack Surface for Safety-Aligned Large Language Models",
    "url": "http://arxiv.org/abs/2502.11853v1",
    "arxiv_id": "2502.11853v1",
    "authors": [
      "Shehel Yoosuf",
      "Temoor Ali",
      "Ahmed Lekssays",
      "Mashael AlSabah",
      "Issa Khalil"
    ],
    "published": "2025-02-17T14:46:38+00:00",
    "summary": "In this work, we present a series of structure transformation attacks on LLM alignment, where we encode natural language intent using diverse syntax spaces, ranging from simple structure formats and basic query languages (e.g. SQL) to new novel spaces and syntaxes created entirely by LLMs. Our extensive evaluation shows that our simplest attacks can achieve close to 90% success rate, even on strict LLMs (such as Claude 3.5 Sonnet) using SOTA alignment mechanisms. We improve the attack performance further by using an adaptive scheme that combines structure transformations along with existing \\textit{content transformations}, resulting in over 96% ASR with 0% refusals.   To generalize our attacks, we explore numerous structure formats, including syntaxes purely generated by LLMs. Our results indicate that such novel syntaxes are easy to generate and result in a high ASR, suggesting that defending against our attacks is not a straightforward process. Finally, we develop a benchmark and evaluate existing safety-alignment defenses against it, showing that most of them fail with 100% ASR. Our results show that existing safety alignment mostly relies on token-level patterns without recognizing harmful concepts, highlighting and motivating the need for serious research efforts in this direction. As a case study, we demonstrate how attackers can use our attack to easily generate a sample malware, and a corpus of fraudulent SMS messages, which perform well in bypassing detection."
  },
  {
    "title": "BaxBench: Can LLMs Generate Correct and Secure Backends?",
    "url": "http://arxiv.org/abs/2502.11844v1",
    "arxiv_id": "2502.11844v1",
    "authors": [
      "Mark Vero",
      "Niels M\u00fcndler",
      "Victor Chibotaru",
      "Veselin Raychev",
      "Maximilian Baader",
      "Nikola Jovanovi\u0107",
      "Jingxuan He",
      "Martin Vechev"
    ],
    "published": "2025-02-17T14:37:47+00:00",
    "summary": "The automatic generation of programs has long been a fundamental challenge in computer science. Recent benchmarks have shown that large language models (LLMs) can effectively generate code at the function level, make code edits, and solve algorithmic coding tasks. However, to achieve full automation, LLMs should be able to generate production-quality, self-contained application modules. To evaluate the capabilities of LLMs in solving this challenge, we introduce BaxBench, a novel evaluation benchmark consisting of 392 tasks for the generation of backend applications. We focus on backends for three critical reasons: (i) they are practically relevant, building the core components of most modern web and cloud software, (ii) they are difficult to get right, requiring multiple functions and files to achieve the desired functionality, and (iii) they are security-critical, as they are exposed to untrusted third-parties, making secure solutions that prevent deployment-time attacks an imperative. BaxBench validates the functionality of the generated applications with comprehensive test cases, and assesses their security exposure by executing end-to-end exploits. Our experiments reveal key limitations of current LLMs in both functionality and security: (i) even the best model, OpenAI o1, achieves a mere 60% on code correctness; (ii) on average, we could successfully execute security exploits on more than half of the correct programs generated by each LLM; and (iii) in less popular backend frameworks, models further struggle to generate correct and secure applications. Progress on BaxBench signifies important steps towards autonomous and secure software development with LLMs."
  },
  {
    "title": "video-SALMONN-o1: Reasoning-enhanced Audio-visual Large Language Model",
    "url": "http://arxiv.org/abs/2502.11775v1",
    "arxiv_id": "2502.11775v1",
    "authors": [
      "Guangzhi Sun",
      "Yudong Yang",
      "Jimin Zhuang",
      "Changli Tang",
      "Yixuan Li",
      "Wei Li",
      "Zejun MA",
      "Chao Zhang"
    ],
    "published": "2025-02-17T13:07:40+00:00",
    "summary": "While recent advancements in reasoning optimization have significantly enhanced the capabilities of large language models (LLMs), existing efforts to improve reasoning have been limited to solving mathematical problems and focusing on visual graphical inputs, neglecting broader applications in general video understanding.This paper proposes video-SALMONN-o1, the first open-source reasoning-enhanced audio-visual LLM designed for general video understanding tasks. To enhance its reasoning abilities, we develop a reasoning-intensive dataset featuring challenging audio-visual questions with step-by-step solutions. We also propose process direct preference optimization (pDPO), which leverages contrastive step selection to achieve efficient step-level reward modelling tailored for multimodal inputs. Additionally, we introduce RivaBench, the first reasoning-intensive video understanding benchmark, featuring over 4,000 high-quality, expert-curated question-answer pairs across scenarios such as standup comedy, academic presentations, and synthetic video detection. video-SALMONN-o1 achieves 3-8% accuracy improvements over the LLaVA-OneVision baseline across different video reasoning benchmarks. Besides, pDPO achieves 6-8% improvements compared to the supervised fine-tuning model on RivaBench. Enhanced reasoning enables video-SALMONN-o1 zero-shot synthetic video detection capabilities."
  },
  {
    "title": "SQL-o1: A Self-Reward Heuristic Dynamic Search Method for Text-to-SQL",
    "url": "http://arxiv.org/abs/2502.11741v1",
    "arxiv_id": "2502.11741v1",
    "authors": [
      "Shuai Lyu",
      "Haoran Luo",
      "Zhonghong Ou",
      "Yifan Zhu",
      "Xiaoran Shang",
      "Yang Qin",
      "Meina Song"
    ],
    "published": "2025-02-17T12:28:11+00:00",
    "summary": "The Text-to-SQL(Text2SQL) task aims to convert natural language queries into executable SQL queries. Thanks to the application of large language models (LLMs), significant progress has been made in this field. However, challenges such as model scalability, limited generation space, and coherence issues in SQL generation still persist. To address these issues, we propose SQL-o1, a Self-Reward-based heuristic search method designed to enhance the reasoning ability of LLMs in SQL query generation. SQL-o1 combines Monte Carlo Tree Search (MCTS) for heuristic process-level search and constructs a Schema-Aware dataset to help the model better understand database schemas. Extensive experiments on the Bird and Spider datasets demonstrate that SQL-o1 improves execution accuracy by 10.8\\% on the complex Bird dataset compared to the latest baseline methods, even outperforming GPT-4-based approaches. Additionally, SQL-o1 excels in few-shot learning scenarios and shows strong cross-model transferability. Our code is publicly available at:https://github.com/ShuaiLyu0110/SQL-o1."
  },
  {
    "title": "Connecting Earth and Moon via the L1 Lagrangian point",
    "url": "http://arxiv.org/abs/2502.11694v1",
    "arxiv_id": "2502.11694v1",
    "authors": [
      "A. K. de Almeida Jr",
      "V. M. de Oliveira",
      "T. Vaillant",
      "D. Maia",
      "A. C. M. Correia",
      "D. Barbosa",
      "L. T. B. Santos"
    ],
    "published": "2025-02-17T11:32:02+00:00",
    "summary": "The renewed global interest in lunar exploration requires new orbital strategies to ensure flight safety which can benefit extended lunar missions and service a plethora of planned instruments in the lunar orbit and surface. We investigate here the equivalent fuel consumption cost to transfer from (to) a given orbit and enter (leave) at any point of an invariant manifold associated with a Lyapunov orbit around the Earth-Moon $L_1$ Lagrangian point using bi-impulsive maneuvers. Whereas solving this type of transfer is generally computationally expensive, we simulate here tens of millions of transfers orbits, for different times of flight, Jacobi constants and spatial location on the manifold. We are able to reduce computational cost by taking advantage of the efficient procedure given by the Theory of Functional Connections for solving boundary value problems, represented with special constraints created to the purposes of this work. We develop here the methodology for constructing these transfers, and apply it to find a low-cost transfer from an orbit around the Earth to a stable manifold and another low-cost transfer from an unstable manifold to an orbit around the Moon. In the end, we obtain an innovative Earth-to-Moon transfer that involves a gravity assist maneuver with the Moon and allows a long stationed stage at the Lyapunov orbit around $L_1$ which can be used for designing multi-purpose missions for extended periods of time with low fuel costs. This is paramount to optimize new exploration concepts."
  },
  {
    "title": "RIDE: Enhancing Large Language Model Alignment through Restyled In-Context Learning Demonstration Exemplars",
    "url": "http://arxiv.org/abs/2502.11681v1",
    "arxiv_id": "2502.11681v1",
    "authors": [
      "Yuncheng Hua",
      "Lizhen Qu",
      "Zhuang Li",
      "Hao Xue",
      "Flora D. Salim",
      "Gholamreza Haffari"
    ],
    "published": "2025-02-17T11:16:19+00:00",
    "summary": "Alignment tuning is crucial for ensuring large language models (LLMs) behave ethically and helpfully. Current alignment approaches require high-quality annotations and significant training resources. This paper proposes a low-cost, tuning-free method using in-context learning (ICL) to enhance LLM alignment. Through an analysis of high-quality ICL demos, we identified style as a key factor influencing LLM alignment capabilities and explicitly restyled ICL exemplars based on this stylistic framework. Additionally, we combined the restyled demos to achieve a balance between the two conflicting aspects of LLM alignment--factuality and safety. We packaged the restyled examples as prompts to trigger few-shot learning, improving LLM alignment. Compared to the best baseline approach, with an average score of 5.00 as the maximum, our method achieves a maximum 0.10 increase on the Alpaca task (from 4.50 to 4.60), a 0.22 enhancement on the Just-eval benchmark (from 4.34 to 4.56), and a maximum improvement of 0.32 (from 3.53 to 3.85) on the MT-Bench dataset. We release the code and data at https://github.com/AnonymousCode-ComputerScience/RIDE."
  },
  {
    "title": "DELMAN: Dynamic Defense Against Large Language Model Jailbreaking with Model Editing",
    "url": "http://arxiv.org/abs/2502.11647v1",
    "arxiv_id": "2502.11647v1",
    "authors": [
      "Yi Wang",
      "Fenghua Weng",
      "Sibei Yang",
      "Zhan Qin",
      "Minlie Huang",
      "Wenjie Wang"
    ],
    "published": "2025-02-17T10:39:21+00:00",
    "summary": "Large Language Models (LLMs) are widely applied in decision making, but their deployment is threatened by jailbreak attacks, where adversarial users manipulate model behavior to bypass safety measures. Existing defense mechanisms, such as safety fine-tuning and model editing, either require extensive parameter modifications or lack precision, leading to performance degradation on general tasks, which is unsuitable to post-deployment safety alignment. To address these challenges, we propose DELMAN (Dynamic Editing for LLMs JAilbreak DefeNse), a novel approach leveraging direct model editing for precise, dynamic protection against jailbreak attacks. DELMAN directly updates a minimal set of relevant parameters to neutralize harmful behaviors while preserving the model's utility. To avoid triggering a safe response in benign context, we incorporate KL-divergence regularization to ensure the updated model remains consistent with the original model when processing benign queries. Experimental results demonstrate that DELMAN outperforms baseline methods in mitigating jailbreak attacks while preserving the model's utility, and adapts seamlessly to new attack instances, providing a practical and efficient solution for post-deployment model protection."
  },
  {
    "title": "Enhancing Out-of-Distribution Detection in Medical Imaging with Normalizing Flows",
    "url": "http://arxiv.org/abs/2502.11638v1",
    "arxiv_id": "2502.11638v1",
    "authors": [
      "Dariush Lotfi",
      "Mohammad-Ali Nikouei Mahani",
      "Mohamad Koohi-Moghadam",
      "Kyongtae Ty Bae"
    ],
    "published": "2025-02-17T10:31:24+00:00",
    "summary": "Out-of-distribution (OOD) detection is crucial in AI-driven medical imaging to ensure reliability and safety by identifying inputs outside a model's training distribution. Existing methods often require retraining or modifications to pre-trained models, which is impractical for clinical applications. This study introduces a post-hoc normalizing flow-based approach that seamlessly integrates with pre-trained models. By leveraging normalizing flows, it estimates the likelihood of feature vectors extracted from pre-trained models, capturing semantically meaningful representations without relying on pixel-level statistics. The method was evaluated using the MedMNIST benchmark and a newly curated MedOOD dataset simulating clinically relevant distributional shifts. Performance was measured using standard OOD detection metrics (e.g., AUROC, FPR@95, AUPR_IN, AUPR_OUT), with statistical analyses comparing it against ten baseline methods. On MedMNIST, the proposed model achieved an AUROC of 93.80%, outperforming state-of-the-art methods. On MedOOD, it achieved an AUROC of 84.61%, demonstrating superior performance against other methods. Its post-hoc nature ensures compatibility with existing clinical workflows, addressing the limitations of previous approaches. The model and code to build OOD datasets are available at https://github.com/dlotfi/MedOODFlow."
  },
  {
    "title": "GraphThought: Graph Combinatorial Optimization with Thought Generation",
    "url": "http://arxiv.org/abs/2502.11607v1",
    "arxiv_id": "2502.11607v1",
    "authors": [
      "Zixiao Huang",
      "Lifeng Guo",
      "Junjie Sheng",
      "Haosheng Chen",
      "Wenhao Li",
      "Bo Jin",
      "Changhong Lu",
      "Xiangfeng Wang"
    ],
    "published": "2025-02-17T09:50:41+00:00",
    "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across various domains, especially in text processing and generative tasks. Recent advancements in the reasoning capabilities of state-of-the-art LLMs, such as OpenAI-o1, have significantly broadened their applicability, particularly in complex problem-solving and logical inference. However, most existing LLMs struggle with notable limitations in handling graph combinatorial optimization (GCO) problems. To bridge this gap, we formally define the Optimal Thoughts Design (OTD) problem, including its state and action thought space. We then introduce a novel framework, GraphThought, designed to generate high-quality thought datasets for GCO problems. Leveraging these datasets, we fine-tune the Llama-3-8B-Instruct model to develop Llama-GT. Notably, despite its compact 8B-parameter architecture, Llama-GT matches the performance of state-of-the-art LLMs on the GraphArena benchmark. Experimental results show that our approach outperforms both proprietary and open-source models, even rivaling specialized models like o1-mini. This work sets a new state-of-the-art benchmark while challenging the prevailing notion that model scale is the primary driver of reasoning capability."
  },
  {
    "title": "Runtime Enforcement of CPS against Signal Temporal Logic",
    "url": "http://arxiv.org/abs/2502.11584v1",
    "arxiv_id": "2502.11584v1",
    "authors": [
      "Han Su",
      "Saumya Shankar",
      "Srinivas Pinisetty",
      "Partha S. Roop",
      "Naijun Zhan"
    ],
    "published": "2025-02-17T09:16:58+00:00",
    "summary": "Cyber-Physical Systems (CPSs), especially those involving autonomy, need guarantees of their safety. Runtime Enforcement (RE) is a lightweight method to formally ensure that some specified properties are satisfied over the executions of the system. Hence, there is recent interest in the RE of CPS. However, existing methods are not designed to tackle specifications suitable for the hybrid dynamics of CPS. With this in mind, we develop runtime enforcement of CPS using properties defined in Signal Temporal Logic (STL).   In this work, we aim to construct a runtime enforcer for a given STL formula to minimally modify a signal to satisfy the formula. To achieve this, the STL formula to be enforced is first translated into a timed transducer, while the signal to be corrected is encoded as timed words. We provide timed transducers for the temporal operators \\emph{until} and \\emph{release} noting that other temporal operators can be expressed using these two. Our approach enables effective enforcement of STL properties for CPS. A case study is provided to illustrate the approach and generate empirical evidence of its suitability for CPS."
  },
  {
    "title": "Language Complexity Measurement as a Noisy Zero-Shot Proxy for Evaluating LLM Performance",
    "url": "http://arxiv.org/abs/2502.11578v1",
    "arxiv_id": "2502.11578v1",
    "authors": [
      "Birger Moell",
      "Johan Boye"
    ],
    "published": "2025-02-17T09:09:58+00:00",
    "summary": "Large Language Models (LLMs) have made significant strides in natural language generation but often face challenges in tasks requiring precise calculations and structural analysis. This paper investigates the performance of state-of-the-art LLMs on language complexity measurement tasks, through the computation of the LIX readability metric and Average Dependency Distance (ADD). Using Swedish high school and university-level essays, we evaluate the models' abilities to compute LIX scores and perform dependency parsing, comparing their results to established ground truths. Our findings reveal that while all models demonstrate some capacity for these tasks, ChatGPT-o1-mini performs most consistently, achieving the highest accuracy in both LIX computation and dependency parsing. Additionally, we observe a strong significant correlation -0.875 p 0.026 (N=6) between the models' accuracy in computing LIX and their overall performance on the Massive Multitask Language Understanding (MMLU) benchmark. These results suggest that language complexity measurement abilities can serve as a noisy zero-shot proxies for assessing the general capabilities of LLMs, providing a practical method for model evaluation without the need for extensive benchmarking datasets."
  },
  {
    "title": "Large Language Models and Mathematical Reasoning Failures",
    "url": "http://arxiv.org/abs/2502.11574v1",
    "arxiv_id": "2502.11574v1",
    "authors": [
      "Johan Boye",
      "Birger Moell"
    ],
    "published": "2025-02-17T09:07:32+00:00",
    "summary": "This paper investigates the mathematical reasoning capabilities of large language models (LLMs) using 50 newly constructed high-school-level word problems. Unlike prior studies that focus solely on answer correctness, we rigorously analyze both final answers and solution steps to identify reasoning failures. Evaluating eight state-of-the-art models - including Mixtral, Llama, Gemini, GPT-4o, and OpenAI's o1 variants - we find that while newer models (e.g., o3-mini, deepseek-r1) achieve higher accuracy, all models exhibit errors in spatial reasoning, strategic planning, and arithmetic, sometimes producing correct answers through flawed logic. Common failure modes include unwarranted assumptions, over-reliance on numerical patterns, and difficulty translating physical intuition into mathematical steps. Manual analysis reveals that models struggle with problems requiring multi-step deduction or real-world knowledge, despite possessing broad mathematical knowledge. Our results underscore the importance of evaluating reasoning processes, not just answers, and caution against overestimating LLMs' problem-solving proficiency. The study highlights persistent gaps in LLMs' generalization abilities, emphasizing the need for targeted improvements in structured reasoning and constraint handling."
  },
  {
    "title": "Equilibrate RLHF: Towards Balancing Helpfulness-Safety Trade-off in Large Language Models",
    "url": "http://arxiv.org/abs/2502.11555v1",
    "arxiv_id": "2502.11555v1",
    "authors": [
      "Yingshui Tan",
      "Yilei Jiang",
      "Yanshi Li",
      "Jiaheng Liu",
      "Xingyuan Bu",
      "Wenbo Su",
      "Xiangyu Yue",
      "Xiaoyong Zhu",
      "Bo Zheng"
    ],
    "published": "2025-02-17T08:40:30+00:00",
    "summary": "Fine-tuning large language models (LLMs) based on human preferences, commonly achieved through reinforcement learning from human feedback (RLHF), has been effective in improving their performance. However, maintaining LLM safety throughout the fine-tuning process remains a significant challenge, as resolving conflicts between safety and helpfulness can be non-trivial. Typically, the safety alignment of LLM is trained on data with safety-related categories. However, our experiments find that naively increasing the scale of safety training data usually leads the LLMs to an ``overly safe'' state rather than a ``truly safe'' state, boosting the refusal rate through extensive safety-aligned data without genuinely understanding the requirements for safe responses. Such an approach can inadvertently diminish the models' helpfulness. To understand the phenomenon, we first investigate the role of safety data by categorizing them into three different groups, and observe that each group behaves differently as training data scales up. To boost the balance between safety and helpfulness, we propose an Equilibrate RLHF framework including a Fine-grained Data-centric (FDC) approach that achieves better safety alignment even with fewer training data, and an Adaptive Message-wise Alignment (AMA) approach, which selectively highlight the key segments through a gradient masking strategy. Extensive experimental results demonstrate that our approach significantly enhances the safety alignment of LLMs while balancing safety and helpfulness."
  },
  {
    "title": "Evaluating o1-Like LLMs: Unlocking Reasoning for Translation through Comprehensive Analysis",
    "url": "http://arxiv.org/abs/2502.11544v1",
    "arxiv_id": "2502.11544v1",
    "authors": [
      "Andong Chen",
      "Yuchen Song",
      "Wenxin Zhu",
      "Kehai Chen",
      "Muyun Yang",
      "Tiejun Zhao",
      "Min zhang"
    ],
    "published": "2025-02-17T08:23:46+00:00",
    "summary": "The o1-Like LLMs are transforming AI by simulating human cognitive processes, but their performance in multilingual machine translation (MMT) remains underexplored. This study examines: (1) how o1-Like LLMs perform in MMT tasks and (2) what factors influence their translation quality. We evaluate multiple o1-Like LLMs and compare them with traditional models like ChatGPT and GPT-4o. Results show that o1-Like LLMs establish new multilingual translation benchmarks, with DeepSeek-R1 surpassing GPT-4o in contextless tasks. They demonstrate strengths in historical and cultural translation but exhibit a tendency for rambling issues in Chinese-centric outputs. Further analysis reveals three key insights: (1) High inference costs and slower processing speeds make complex translation tasks more resource-intensive. (2) Translation quality improves with model size, enhancing commonsense reasoning and cultural translation. (3) The temperature parameter significantly impacts output quality-lower temperatures yield more stable and accurate translations, while higher temperatures reduce coherence and precision."
  },
  {
    "title": "AURORA:Automated Training Framework of Universal Process Reward Models via Ensemble Prompting and Reverse Verification",
    "url": "http://arxiv.org/abs/2502.11520v1",
    "arxiv_id": "2502.11520v1",
    "authors": [
      "Xiaoyu Tan",
      "Tianchu Yao",
      "Chao Qu",
      "Bin Li",
      "Minghao Yang",
      "Dakuan Lu",
      "Haozhe Wang",
      "Xihe Qiu",
      "Wei Chu",
      "Yinghui Xu",
      "Yuan Qi"
    ],
    "published": "2025-02-17T07:41:27+00:00",
    "summary": "The reasoning capabilities of advanced large language models (LLMs) like o1 have revolutionized artificial intelligence applications. Nevertheless, evaluating and optimizing complex reasoning processes remain significant challenges due to diverse policy distributions and the inherent limitations of human effort and accuracy. In this paper, we present AURORA, a novel automated framework for training universal process reward models (PRMs) using ensemble prompting and reverse verification. The framework employs a two-phase approach: First, it uses diverse prompting strategies and ensemble methods to perform automated annotation and evaluation of processes, ensuring robust assessments for reward learning. Second, it leverages practical reference answers for reverse verification, enhancing the model's ability to validate outputs and improving training accuracy. To assess the framework's performance, we extend beyond the existing ProcessBench benchmark by introducing UniversalBench, which evaluates reward predictions across full trajectories under diverse policy distribtion with long Chain-of-Thought (CoT) outputs. Experimental results demonstrate that AURORA enhances process evaluation accuracy, improves PRMs' accuracy for diverse policy distributions and long-CoT responses. The project will be open-sourced at https://auroraprm.github.io/. The Universal-PRM-7B is available at https://huggingface.co/infly/Universal-PRM-7B."
  },
  {
    "title": "Adversary-Aware DPO: Enhancing Safety Alignment in Vision Language Models via Adversarial Training",
    "url": "http://arxiv.org/abs/2502.11455v1",
    "arxiv_id": "2502.11455v1",
    "authors": [
      "Fenghua Weng",
      "Jian Lou",
      "Jun Feng",
      "Minlie Huang",
      "Wenjie Wang"
    ],
    "published": "2025-02-17T05:28:47+00:00",
    "summary": "Safety alignment is critical in pre-training large language models (LLMs) to generate responses aligned with human values and refuse harmful queries. Unlike LLM, the current safety alignment of VLMs is often achieved with post-hoc safety fine-tuning. However, these methods are less effective to white-box attacks. To address this, we propose $\\textit{Adversary-aware DPO (ADPO)}$, a novel training framework that explicitly considers adversarial. $\\textit{Adversary-aware DPO (ADPO)}$ integrates adversarial training into DPO to enhance the safety alignment of VLMs under worst-case adversarial perturbations. $\\textit{ADPO}$ introduces two key components: (1) an adversarial-trained reference model that generates human-preferred responses under worst-case perturbations, and (2) an adversarial-aware DPO loss that generates winner-loser pairs accounting for adversarial distortions. By combining these innovations, $\\textit{ADPO}$ ensures that VLMs remain robust and reliable even in the presence of sophisticated jailbreak attacks. Extensive experiments demonstrate that $\\textit{ADPO}$ outperforms baselines in the safety alignment and general utility of VLMs."
  },
  {
    "title": "AGrail: A Lifelong Agent Guardrail with Effective and Adaptive Safety Detection",
    "url": "http://arxiv.org/abs/2502.11448v2",
    "arxiv_id": "2502.11448v2",
    "authors": [
      "Weidi Luo",
      "Shenghong Dai",
      "Xiaogeng Liu",
      "Suman Banerjee",
      "Huan Sun",
      "Muhao Chen",
      "Chaowei Xiao"
    ],
    "published": "2025-02-17T05:12:33+00:00",
    "summary": "The rapid advancements in Large Language Models (LLMs) have enabled their deployment as autonomous agents for handling complex tasks in dynamic environments. These LLMs demonstrate strong problem-solving capabilities and adaptability to multifaceted scenarios. However, their use as agents also introduces significant risks, including task-specific risks, which are identified by the agent administrator based on the specific task requirements and constraints, and systemic risks, which stem from vulnerabilities in their design or interactions, potentially compromising confidentiality, integrity, or availability (CIA) of information and triggering security risks. Existing defense agencies fail to adaptively and effectively mitigate these risks. In this paper, we propose AGrail, a lifelong agent guardrail to enhance LLM agent safety, which features adaptive safety check generation, effective safety check optimization, and tool compatibility and flexibility. Extensive experiments demonstrate that AGrail not only achieves strong performance against task-specific and system risks but also exhibits transferability across different LLM agents' tasks."
  },
  {
    "title": "What's in a Query: Polarity-Aware Distribution-Based Fair Ranking",
    "url": "http://arxiv.org/abs/2502.11429v1",
    "arxiv_id": "2502.11429v1",
    "authors": [
      "Aparna Balagopalan",
      "Kai Wang",
      "Olawale Salaudeen",
      "Asia Biega",
      "Marzyeh Ghassemi"
    ],
    "published": "2025-02-17T04:38:36+00:00",
    "summary": "Machine learning-driven rankings, where individuals (or items) are ranked in response to a query, mediate search exposure or attention in a variety of safety-critical settings. Thus, it is important to ensure that such rankings are fair. Under the goal of equal opportunity, attention allocated to an individual on a ranking interface should be proportional to their relevance across search queries. In this work, we examine amortized fair ranking -- where relevance and attention are cumulated over a sequence of user queries to make fair ranking more feasible in practice. Unlike prior methods that operate on expected amortized attention for each individual, we define new divergence-based measures for attention distribution-based fairness in ranking (DistFaiR), characterizing unfairness as the divergence between the distribution of attention and relevance corresponding to an individual over time. This allows us to propose new definitions of unfairness, which are more reliable at test time. Second, we prove that group fairness is upper-bounded by individual fairness under this definition for a useful class of divergence measures, and experimentally show that maximizing individual fairness through an integer linear programming-based optimization is often beneficial to group fairness. Lastly, we find that prior research in amortized fair ranking ignores critical information about queries, potentially leading to a fairwashing risk in practice by making rankings appear more fair than they actually are."
  },
  {
    "title": "Detecting and Filtering Unsafe Training Data via Data Attribution",
    "url": "http://arxiv.org/abs/2502.11411v1",
    "arxiv_id": "2502.11411v1",
    "authors": [
      "Yijun Pan",
      "Taiwei Shi",
      "Jieyu Zhao",
      "Jiaqi W. Ma"
    ],
    "published": "2025-02-17T03:50:58+00:00",
    "summary": "Large language models (LLMs) are vulnerable to unsafe training data that even small amounts of unsafe data can lead to harmful model behaviors. Detecting and filtering such unsafe training data is essential for trustworthy model development. Current state-of-the-art (SOTA) approaches typically rely on training moderation classifiers which requires significant computational overhead and are limited to predefined taxonomies, making them less adaptable to evolving safety concerns. Moreover, these classifiers lack insight into the training process, limiting their effectiveness in filtering unsafe data. To address these limitations, we propose DABUF, leveraging data attribution to detect and filter unsafe training data by attributing harmful model outputs to influential training data points. DABUF enables flexible identification of various unsafe data types without predefined taxonomies. However, in practice, model outputs can be complex with combined safe linguistic features and unsafe content, leading to reduced attribution accuracy. In such cases, DABUF will integrate moderation classifiers to identify a minimal subset of unsafe training data for targeted attribution (such as jailbreak). When model outputs are relatively straightforward, DABUF uses model outputs directly as the attribution targets. We evaluate the performance on two different tasks: in filtering jailbreaking training data and in identifying and mitigating gender bias. DABUF outperforms SOTA approaches by up to 7.5\\% in detection AUPRC in jailbreaking scenarios, and 44.1\\% in detecting gender bias. Moreover, retraining on DABUF-filtered data leads to higher model safety across experiments, underscoring its versatility in addressing a broad spectrum of unsafe data issues."
  },
  {
    "title": "CCJA: Context-Coherent Jailbreak Attack for Aligned Large Language Models",
    "url": "http://arxiv.org/abs/2502.11379v1",
    "arxiv_id": "2502.11379v1",
    "authors": [
      "Guanghao Zhou",
      "Panjia Qiu",
      "Mingyuan Fan",
      "Cen Chen",
      "Mingyuan Chu",
      "Xin Zhang",
      "Jun Zhou"
    ],
    "published": "2025-02-17T02:49:26+00:00",
    "summary": "Despite explicit alignment efforts for large language models (LLMs), they can still be exploited to trigger unintended behaviors, a phenomenon known as \"jailbreaking.\" Current jailbreak attack methods mainly focus on discrete prompt manipulations targeting closed-source LLMs, relying on manually crafted prompt templates and persuasion rules. However, as the capabilities of open-source LLMs improve, ensuring their safety becomes increasingly crucial. In such an environment, the accessibility of model parameters and gradient information by potential attackers exacerbates the severity of jailbreak threats. To address this research gap, we propose a novel \\underline{C}ontext-\\underline{C}oherent \\underline{J}ailbreak \\underline{A}ttack (CCJA). We define jailbreak attacks as an optimization problem within the embedding space of masked language models. Through combinatorial optimization, we effectively balance the jailbreak attack success rate with semantic coherence. Extensive evaluations show that our method not only maintains semantic consistency but also surpasses state-of-the-art baselines in attack effectiveness. Additionally, by integrating semantically coherent jailbreak prompts generated by our method into widely used black-box methodologies, we observe a notable enhancement in their success rates when targeting closed-source commercial LLMs. This highlights the security threat posed by open-source LLMs to commercial counterparts. We will open-source our code if the paper is accepted."
  },
  {
    "title": "HI-GVF: Shared Control based on Human-Influenced Guiding Vector Fields for Human-multi-robot Cooperation",
    "url": "http://arxiv.org/abs/2502.11370v1",
    "arxiv_id": "2502.11370v1",
    "authors": [
      "Pengming Zhu",
      "Zongtan Zhou",
      "Weijia Yao",
      "Wei Dai",
      "Zhiwen Zeng",
      "Huimin Lu"
    ],
    "published": "2025-02-17T02:33:09+00:00",
    "summary": "Human-multi-robot shared control leverages human decision-making and robotic autonomy to enhance human-robot collaboration. While widely studied, existing systems often adopt a leader-follower model, limiting robot autonomy to some extent. Besides, a human is required to directly participate in the motion control of robots through teleoperation, which significantly burdens the operator. To alleviate these two issues, we propose a layered shared control computing framework using human-influenced guiding vector fields (HI-GVF) for human-robot collaboration. HI-GVF guides the multi-robot system along a desired path specified by the human. Then, an intention field is designed to merge the human and robot intentions, accelerating the propagation of the human intention within the multi-robot system. Moreover, we give the stability analysis of the proposed model and use collision avoidance based on safety barrier certificates to fine-tune the velocity. Eventually, considering the firefighting task as an example scenario, we conduct simulations and experiments using multiple human-robot interfaces (brain-computer interface, myoelectric wristband, eye-tracking), and the results demonstrate that our proposed approach boosts the effectiveness and performance of the task."
  },
  {
    "title": "Sparse Autoencoder Features for Classifications and Transferability",
    "url": "http://arxiv.org/abs/2502.11367v1",
    "arxiv_id": "2502.11367v1",
    "authors": [
      "Jack Gallifant",
      "Shan Chen",
      "Kuleen Sasse",
      "Hugo Aerts",
      "Thomas Hartvigsen",
      "Danielle S. Bitterman"
    ],
    "published": "2025-02-17T02:30:45+00:00",
    "summary": "Sparse Autoencoders (SAEs) provide potentials for uncovering structured, human-interpretable representations in Large Language Models (LLMs), making them a crucial tool for transparent and controllable AI systems. We systematically analyze SAE for interpretable feature extraction from LLMs in safety-critical classification tasks. Our framework evaluates (1) model-layer selection and scaling properties, (2) SAE architectural configurations, including width and pooling strategies, and (3) the effect of binarizing continuous SAE activations. SAE-derived features achieve macro F1 > 0.8, outperforming hidden-state and BoW baselines while demonstrating cross-model transfer from Gemma 2 2B to 9B-IT models. These features generalize in a zero-shot manner to cross-lingual toxicity detection and visual classification tasks. Our analysis highlights the significant impact of pooling strategies and binarization thresholds, showing that binarization offers an efficient alternative to traditional feature selection while maintaining or improving performance. These findings establish new best practices for SAE-based interpretability and enable scalable, transparent deployment of LLMs in real-world applications. Full repo: https://github.com/shan23chen/MOSAIC."
  },
  {
    "title": "VLDBench: Vision Language Models Disinformation Detection Benchmark",
    "url": "http://arxiv.org/abs/2502.11361v1",
    "arxiv_id": "2502.11361v1",
    "authors": [
      "Shaina Raza",
      "Ashmal Vayani",
      "Aditya Jain",
      "Aravind Narayanan",
      "Vahid Reza Khazaie",
      "Syed Raza Bashir",
      "Elham Dolatabadi",
      "Gias Uddin",
      "Christos Emmanouilidis",
      "Rizwan Qureshi",
      "Mubarak Shah"
    ],
    "published": "2025-02-17T02:18:47+00:00",
    "summary": "The rapid rise of AI-generated content has made detecting disinformation increasingly challenging. In particular, multimodal disinformation, i.e., online posts-articles that contain images and texts with fabricated information are specially designed to deceive. While existing AI safety benchmarks primarily address bias and toxicity, multimodal disinformation detection remains largely underexplored. To address this challenge, we present the Vision-Language Disinformation Detection Benchmark VLDBench, the first comprehensive benchmark for detecting disinformation across both unimodal (text-only) and multimodal (text and image) content, comprising 31,000} news article-image pairs, spanning 13 distinct categories, for robust evaluation. VLDBench features a rigorous semi-automated data curation pipeline, with 22 domain experts dedicating 300 plus hours} to annotation, achieving a strong inter-annotator agreement (Cohen kappa = 0.78). We extensively evaluate state-of-the-art Large Language Models (LLMs) and Vision-Language Models (VLMs), demonstrating that integrating textual and visual cues in multimodal news posts improves disinformation detection accuracy by 5 - 35 % compared to unimodal models. Developed in alignment with AI governance frameworks such as the EU AI Act, NIST guidelines, and the MIT AI Risk Repository 2024, VLDBench is expected to become a benchmark for detecting disinformation in online multi-modal contents. Our code and data will be publicly available."
  },
  {
    "title": "A Framework for Learning Scoring Rules in Autonomous Driving Planning Systems",
    "url": "http://arxiv.org/abs/2502.11352v1",
    "arxiv_id": "2502.11352v1",
    "authors": [
      "Zikang Xiong",
      "Joe Kurian Eappen",
      "Suresh Jagannathan"
    ],
    "published": "2025-02-17T02:06:57+00:00",
    "summary": "In autonomous driving systems, motion planning is commonly implemented as a two-stage process: first, a trajectory proposer generates multiple candidate trajectories, then a scoring mechanism selects the most suitable trajectory for execution. For this critical selection stage, rule-based scoring mechanisms are particularly appealing as they can explicitly encode driving preferences, safety constraints, and traffic regulations in a formalized, human-understandable format. However, manually crafting these scoring rules presents significant challenges: the rules often contain complex interdependencies, require careful parameter tuning, and may not fully capture the nuances present in real-world driving data. This work introduces FLoRA, a novel framework that bridges this gap by learning interpretable scoring rules represented in temporal logic. Our method features a learnable logic structure that captures nuanced relationships across diverse driving scenarios, optimizing both rules and parameters directly from real-world driving demonstrations collected in NuPlan. Our approach effectively learns to evaluate driving behavior even though the training data only contains positive examples (successful driving demonstrations). Evaluations in closed-loop planning simulations demonstrate that our learned scoring rules outperform existing techniques, including expert-designed rules and neural network scoring models, while maintaining interpretability. This work introduces a data-driven approach to enhance the scoring mechanism in autonomous driving systems, designed as a plug-in module to seamlessly integrate with various trajectory proposers. Our video and code are available on xiong.zikang.me/FLoRA."
  },
  {
    "title": "Leveraging Multimodal-LLMs Assisted by Instance Segmentation for Intelligent Traffic Monitoring",
    "url": "http://arxiv.org/abs/2502.11304v1",
    "arxiv_id": "2502.11304v1",
    "authors": [
      "Murat Arda Onsu",
      "Poonam Lohan",
      "Burak Kantarci",
      "Aisha Syed",
      "Matthew Andrews",
      "Sean Kennedy"
    ],
    "published": "2025-02-16T23:03:26+00:00",
    "summary": "A robust and efficient traffic monitoring system is essential for smart cities and Intelligent Transportation Systems (ITS), using sensors and cameras to track vehicle movements, optimize traffic flow, reduce congestion, enhance road safety, and enable real-time adaptive traffic control. Traffic monitoring models must comprehensively understand dynamic urban conditions and provide an intuitive user interface for effective management. This research leverages the LLaVA visual grounding multimodal large language model (LLM) for traffic monitoring tasks on the real-time Quanser Interactive Lab simulation platform, covering scenarios like intersections, congestion, and collisions. Cameras placed at multiple urban locations collect real-time images from the simulation, which are fed into the LLaVA model with queries for analysis. An instance segmentation model integrated into the cameras highlights key elements such as vehicles and pedestrians, enhancing training and throughput. The system achieves 84.3% accuracy in recognizing vehicle locations and 76.4% in determining steering direction, outperforming traditional models."
  },
  {
    "title": "MC-BEVRO: Multi-Camera Bird Eye View Road Occupancy Detection for Traffic Monitoring",
    "url": "http://arxiv.org/abs/2502.11287v1",
    "arxiv_id": "2502.11287v1",
    "authors": [
      "Arpitsinh Vaghela",
      "Duo Lu",
      "Aayush Atul Verma",
      "Bharatesh Chakravarthi",
      "Hua Wei",
      "Yezhou Yang"
    ],
    "published": "2025-02-16T22:03:03+00:00",
    "summary": "Single camera 3D perception for traffic monitoring faces significant challenges due to occlusion and limited field of view. Moreover, fusing information from multiple cameras at the image feature level is difficult because of different view angles. Further, the necessity for practical implementation and compatibility with existing traffic infrastructure compounds these challenges. To address these issues, this paper introduces a novel Bird's-Eye-View road occupancy detection framework that leverages multiple roadside cameras to overcome the aforementioned limitations. To facilitate the framework's development and evaluation, a synthetic dataset featuring diverse scenes and varying camera configurations is generated using the CARLA simulator. A late fusion and three early fusion methods were implemented within the proposed framework, with performance further enhanced by integrating backgrounds. Extensive evaluations were conducted to analyze the impact of multi-camera inputs and varying BEV occupancy map sizes on model performance. Additionally, a real-world data collection pipeline was developed to assess the model's ability to generalize to real-world environments. The sim-to-real capabilities of the model were evaluated using zero-shot and few-shot fine-tuning, demonstrating its potential for practical application. This research aims to advance perception systems in traffic monitoring, contributing to improved traffic management, operational efficiency, and road safety."
  },
  {
    "title": "Soteria: Language-Specific Functional Parameter Steering for Multilingual Safety Alignment",
    "url": "http://arxiv.org/abs/2502.11244v1",
    "arxiv_id": "2502.11244v1",
    "authors": [
      "Somnath Banerjee",
      "Sayan Layek",
      "Pratyush Chatterjee",
      "Animesh Mukherjee",
      "Rima Hazra"
    ],
    "published": "2025-02-16T19:44:01+00:00",
    "summary": "Ensuring consistent safety across multiple languages remains a significant challenge for large language models (LLMs). We introduce Soteria, a lightweight yet powerful strategy that locates and minimally adjusts the \"functional heads\" most responsible for harmful content generation in each language. By altering only a fraction of parameters, Soteria drastically reduces policy violations without sacrificing overall model performance, even in low-resource settings. To rigorously evaluate our approach, we also present XThreatBench, a specialized multilingual dataset capturing fine-grained harmful behaviors drawn from real policy guidelines. Experiments with leading open-source LLMs (e.g., Llama, Qwen, Mistral) show that Soteria consistently improves safety metrics across high-, mid-, and low-resource languages. These findings highlight a promising path toward scalable, linguistically attuned, and ethically aligned LLMs worldwide."
  },
  {
    "title": "LLMs and Childhood Safety: Identifying Risks and Proposing a Protection Framework for Safe Child-LLM Interaction",
    "url": "http://arxiv.org/abs/2502.11242v1",
    "arxiv_id": "2502.11242v1",
    "authors": [
      "Junfeng Jiao",
      "Saleh Afroogh",
      "Kevin Chen",
      "Abhejay Murali",
      "David Atkinson",
      "Amit Dhurandhar"
    ],
    "published": "2025-02-16T19:39:48+00:00",
    "summary": "This study examines the growing use of Large Language Models (LLMs) in child-centered applications, highlighting safety and ethical concerns such as bias, harmful content, and cultural insensitivity. Despite their potential to enhance learning, there is a lack of standardized frameworks to mitigate these risks. Through a systematic literature review, we identify key parental and empirical concerns, including toxicity and ethical breaches in AI outputs. Moreover, to address these issues, this paper proposes a protection framework for safe Child-LLM interaction, incorporating metrics for content safety, behavioral ethics, and cultural sensitivity. The framework provides practical tools for evaluating LLM safety, offering guidance for developers, policymakers, and educators to ensure responsible AI deployment for children."
  },
  {
    "title": "Can't See the Forest for the Trees: Benchmarking Multimodal Safety Awareness for Multimodal LLMs",
    "url": "http://arxiv.org/abs/2502.11184v1",
    "arxiv_id": "2502.11184v1",
    "authors": [
      "Wenxuan Wang",
      "Xiaoyuan Liu",
      "Kuiyi Gao",
      "Jen-tse Huang",
      "Youliang Yuan",
      "Pinjia He",
      "Shuai Wang",
      "Zhaopeng Tu"
    ],
    "published": "2025-02-16T16:12:40+00:00",
    "summary": "Multimodal Large Language Models (MLLMs) have expanded the capabilities of traditional language models by enabling interaction through both text and images. However, ensuring the safety of these models remains a significant challenge, particularly in accurately identifying whether multimodal content is safe or unsafe-a capability we term safety awareness. In this paper, we introduce MMSafeAware, the first comprehensive multimodal safety awareness benchmark designed to evaluate MLLMs across 29 safety scenarios with 1500 carefully curated image-prompt pairs. MMSafeAware includes both unsafe and over-safety subsets to assess models abilities to correctly identify unsafe content and avoid over-sensitivity that can hinder helpfulness. Evaluating nine widely used MLLMs using MMSafeAware reveals that current models are not sufficiently safe and often overly sensitive; for example, GPT-4V misclassifies 36.1% of unsafe inputs as safe and 59.9% of benign inputs as unsafe. We further explore three methods to improve safety awareness-prompting-based approaches, visual contrastive decoding, and vision-centric reasoning fine-tuning-but find that none achieve satisfactory performance. Our findings highlight the profound challenges in developing MLLMs with robust safety awareness, underscoring the need for further research in this area. All the code and data will be publicly available to facilitate future research."
  },
  {
    "title": "Quantifying the Capability Boundary of DeepSeek Models: An Application-Driven Performance Analysis",
    "url": "http://arxiv.org/abs/2502.11164v1",
    "arxiv_id": "2502.11164v1",
    "authors": [
      "Shiguo Lian",
      "Kaikai Zhao",
      "Xuejiao Lei",
      "Ning Wang",
      "Zhenhong Long",
      "Peijun Yang",
      "Minjie Hua",
      "Chaoyang Ma",
      "Wen Liu",
      "Kai Wang",
      "Zhaoxiang Liu"
    ],
    "published": "2025-02-16T15:29:58+00:00",
    "summary": "DeepSeek-R1, known for its low training cost and exceptional reasoning capabilities, has achieved state-of-the-art performance on various benchmarks. However, detailed evaluations from the perspective of real-world applications are lacking, making it challenging for users to select the most suitable DeepSeek models for their specific needs. To address this gap, we evaluate the DeepSeek-V3, DeepSeek-R1, DeepSeek-R1-Distill-Qwen series, and DeepSeek-R1-Distill-Llama series on A-Eval, an application-driven benchmark. By comparing original instruction-tuned models with their distilled counterparts, we analyze how reasoning enhancements impact performance across diverse practical tasks. Our results show that reasoning-enhanced models, while generally powerful, do not universally outperform across all tasks, with performance gains varying significantly across tasks and models. To further assist users in model selection, we quantify the capability boundary of DeepSeek models through performance tier classifications and intuitive line charts. Specific examples provide actionable insights to help users select and deploy the most cost-effective DeepSeek models, ensuring optimal performance and resource efficiency in real-world applications."
  },
  {
    "title": "Safety Evaluation of DeepSeek Models in Chinese Contexts",
    "url": "http://arxiv.org/abs/2502.11137v1",
    "arxiv_id": "2502.11137v1",
    "authors": [
      "Wenjing Zhang",
      "Xuejiao Lei",
      "Zhaoxiang Liu",
      "Ning Wang",
      "Zhenhong Long",
      "Peijun Yang",
      "Jiaojiao Zhao",
      "Minjie Hua",
      "Chaoyang Ma",
      "Kai Wang",
      "Shiguo Lian"
    ],
    "published": "2025-02-16T14:05:54+00:00",
    "summary": "Recently, the DeepSeek series of models, leveraging their exceptional reasoning capabilities and open-source strategy, is reshaping the global AI landscape. Despite these advantages, they exhibit significant safety deficiencies. Research conducted by Robust Intelligence, a subsidiary of Cisco, in collaboration with the University of Pennsylvania, revealed that DeepSeek-R1 has a 100\\% attack success rate when processing harmful prompts. Additionally, multiple safety companies and research institutions have confirmed critical safety vulnerabilities in this model. As models demonstrating robust performance in Chinese and English, DeepSeek models require equally crucial safety assessments in both language contexts. However, current research has predominantly focused on safety evaluations in English environments, leaving a gap in comprehensive assessments of their safety performance in Chinese contexts. In response to this gap, this study introduces CHiSafetyBench, a Chinese-specific safety evaluation benchmark. This benchmark systematically evaluates the safety of DeepSeek-R1 and DeepSeek-V3 in Chinese contexts, revealing their performance across safety categories. The experimental results quantify the deficiencies of these two models in Chinese contexts, providing key insights for subsequent improvements."
  },
  {
    "title": "Ramp Up NTT in Record Time using GPU-Accelerated Algorithms and LLM-based Code Generation",
    "url": "http://arxiv.org/abs/2502.11110v1",
    "arxiv_id": "2502.11110v1",
    "authors": [
      "Yu Cui",
      "Hang Fu",
      "Licheng Wang",
      "Haibin Zhang"
    ],
    "published": "2025-02-16T12:53:23+00:00",
    "summary": "Homomorphic encryption (HE) is a core building block in privacy-preserving machine learning (PPML), but HE is also widely known as its efficiency bottleneck. Therefore, many GPU-accelerated cryptographic schemes have been proposed to improve the performance of HE. However, these methods often require complex modifications tailored to specific algorithms and are tightly coupled with specific GPU and operating systems. It is interesting to ask how to generally offer more practical GPU-accelerated cryptographic algorithm implementations. Given the powerful code generation capabilities of large language models (LLMs), we aim to explore their potential to automatically generate practical GPU-friendly algorithm code using CPU-friendly code. In this paper, we focus on number theoretic transform (NTT) -- the core mechanism of HE. We first develop and optimize a GPU-friendly NTT (GNTT) family that exploits PyTorch's fast matrix computation and precomputation, achieving an approximately 62x speedup -- a significant boost over existing ones. Then we explore GPU-friendly code generation using various LLMs, including DeepSeek-R1, OpenAI o1 and o3-mini. We discover many interesting findings throughout the process. For instance, somewhat surprisingly, our experiments demonstrate that DeepSeek-R1 significantly outperforms OpenAI o3-mini and o1, but still cannot beat our optimized protocol. The findings provide valuable insights for turbocharging PPML and enhancing code generation capabilities of LLMs. Codes are available at: https://github.com/LMPC-Lab/GenGPUCrypto."
  },
  {
    "title": "Knowledge Graph-Driven Retrieval-Augmented Generation: Integrating Deepseek-R1 with Weaviate for Advanced Chatbot Applications",
    "url": "http://arxiv.org/abs/2502.11108v1",
    "arxiv_id": "2502.11108v1",
    "authors": [
      "Alexandru Lecu",
      "Adrian Groza",
      "Lezan Hawizy"
    ],
    "published": "2025-02-16T12:52:28+00:00",
    "summary": "Large language models (LLMs) have significantly advanced the field of natural language generation. However, they frequently generate unverified outputs, which compromises their reliability in critical applications. In this study, we propose an innovative framework that combines structured biomedical knowledge with LLMs through a retrieval-augmented generation technique. Our system develops a thorough knowledge graph by identifying and refining causal relationships and named entities from medical abstracts related to age-related macular degeneration (AMD). Using a vector-based retrieval process and a locally deployed language model, our framework produces responses that are both contextually relevant and verifiable, with direct references to clinical evidence. Experimental results show that this method notably decreases hallucinations, enhances factual precision, and improves the clarity of generated responses, providing a robust solution for advanced biomedical chatbot applications."
  },
  {
    "title": "Talk Structurally, Act Hierarchically: A Collaborative Framework for LLM Multi-Agent Systems",
    "url": "http://arxiv.org/abs/2502.11098v1",
    "arxiv_id": "2502.11098v1",
    "authors": [
      "Zhao Wang",
      "Sota Moriyama",
      "Wei-Yao Wang",
      "Briti Gangopadhyay",
      "Shingo Takamatsu"
    ],
    "published": "2025-02-16T12:26:58+00:00",
    "summary": "Recent advancements in LLM-based multi-agent (LLM-MA) systems have shown promise, yet significant challenges remain in managing communication and refinement when agents collaborate on complex tasks. In this paper, we propose \\textit{Talk Structurally, Act Hierarchically (TalkHier)}, a novel framework that introduces a structured communication protocol for context-rich exchanges and a hierarchical refinement system to address issues such as incorrect outputs, falsehoods, and biases. \\textit{TalkHier} surpasses various types of SoTA, including inference scaling model (OpenAI-o1), open-source multi-agent models (e.g., AgentVerse), and majority voting strategies on current LLM and single-agent baselines (e.g., ReAct, GPT4o), across diverse tasks, including open-domain question answering, domain-specific selective questioning, and practical advertisement text generation. These results highlight its potential to set a new standard for LLM-MA systems, paving the way for more effective, adaptable, and collaborative multi-agent frameworks. The code is available https://github.com/sony/talkhier."
  },
  {
    "title": "Mixture of Tunable Experts - Behavior Modification of DeepSeek-R1 at Inference Time",
    "url": "http://arxiv.org/abs/2502.11096v1",
    "arxiv_id": "2502.11096v1",
    "authors": [
      "Robert Dahlke",
      "Henrik Klagges",
      "Dan Zecha",
      "Benjamin Merkel",
      "Sven Rohr",
      "Fabian Klemm"
    ],
    "published": "2025-02-16T12:24:39+00:00",
    "summary": "We present the Mixture-of-Tunable-Experts (MoTE), a method that extends the Mixture-of-Experts architecture of Large Language Models (LLMs). Without additional training, MoTE enables meaningful and focused behavior changes in LLMs on-the-fly during inference time.   By analyzing the digital LLM brain of DeepSeek-R1 using a technique we dub 'functional Token Resonance Imaging' (fTRI) - inspired by fMRI and using prompts designed to elicit specific behavior (e.g., 'What happened {time}{place}?') - we empirically identify distinctive experts associated with behaviors like refusal responses.   Using MoTE we are able to intervene and control such specific behavior. We switched off the top 10 most refusal-relevant experts (0.07% of R1's 14,848 routed experts), achieving a 52% refusal reduction on sensitive reference prompts without performance degradation on MT-Bench. Random expert deactivation resulted in smaller behavioral shifts with increased noise, whereas forced expert activation led to significantly higher refusal rates.   Our approach shares similarities with sparse autoencoders (SAEs) in terms of explainability and steerability. Unlike SAEs, MoTE does not require large training efforts, as within MoEs with a vast number of experts, specialization already emerged naturally during pretraining.   Our findings suggest that significant functional mechanisms in Mixture-of-Experts architectures can at least partially be localized in a small number of specific experts, rather than being distributed throughout the model's weights. Expert subgroups can be tuned to trigger significant behavior variations, providing insights into the inner workings of LLMs."
  },
  {
    "title": "SafeDialBench: A Fine-Grained Safety Benchmark for Large Language Models in Multi-Turn Dialogues with Diverse Jailbreak Attacks",
    "url": "http://arxiv.org/abs/2502.11090v2",
    "arxiv_id": "2502.11090v2",
    "authors": [
      "Hongye Cao",
      "Yanming Wang",
      "Sijia Jing",
      "Ziyue Peng",
      "Zhixin Bai",
      "Zhe Cao",
      "Meng Fang",
      "Fan Feng",
      "Boyan Wang",
      "Jiaheng Liu",
      "Tianpei Yang",
      "Jing Huo",
      "Yang Gao",
      "Fanyu Meng",
      "Xi Yang",
      "Chao Deng",
      "Junlan Feng"
    ],
    "published": "2025-02-16T12:08:08+00:00",
    "summary": "With the rapid advancement of Large Language Models (LLMs), the safety of LLMs has been a critical concern requiring precise assessment. Current benchmarks primarily concentrate on single-turn dialogues or a single jailbreak attack method to assess the safety. Additionally, these benchmarks have not taken into account the LLM's capability of identifying and handling unsafe information in detail. To address these issues, we propose a fine-grained benchmark SafeDialBench for evaluating the safety of LLMs across various jailbreak attacks in multi-turn dialogues. Specifically, we design a two-tier hierarchical safety taxonomy that considers 6 safety dimensions and generates more than 4000 multi-turn dialogues in both Chinese and English under 22 dialogue scenarios. We employ 7 jailbreak attack strategies, such as reference attack and purpose reverse, to enhance the dataset quality for dialogue generation. Notably, we construct an innovative assessment framework of LLMs, measuring capabilities in detecting, and handling unsafe information and maintaining consistency when facing jailbreak attacks. Experimental results across 17 LLMs reveal that Yi-34B-Chat and GLM4-9B-Chat demonstrate superior safety performance, while Llama3.1-8B-Instruct and o3-mini exhibit safety vulnerabilities."
  },
  {
    "title": "Rewrite to Jailbreak: Discover Learnable and Transferable Implicit Harmfulness Instruction",
    "url": "http://arxiv.org/abs/2502.11084v1",
    "arxiv_id": "2502.11084v1",
    "authors": [
      "Yuting Huang",
      "Chengyuan Liu",
      "Yifeng Feng",
      "Chao Wu",
      "Fei Wu",
      "Kun Kuang"
    ],
    "published": "2025-02-16T11:43:39+00:00",
    "summary": "As Large Language Models (LLMs) are widely applied in various domains, the safety of LLMs is increasingly attracting attention to avoid their powerful capabilities being misused. Existing jailbreak methods create a forced instruction-following scenario, or search adversarial prompts with prefix or suffix tokens to achieve a specific representation manually or automatically. However, they suffer from low efficiency and explicit jailbreak patterns, far from the real deployment of mass attacks to LLMs. In this paper, we point out that simply rewriting the original instruction can achieve a jailbreak, and we find that this rewriting approach is learnable and transferable. We propose the Rewrite to Jailbreak (R2J) approach, a transferable black-box jailbreak method to attack LLMs by iteratively exploring the weakness of the LLMs and automatically improving the attacking strategy. The jailbreak is more efficient and hard to identify since no additional features are introduced. Extensive experiments and analysis demonstrate the effectiveness of R2J, and we find that the jailbreak is also transferable to multiple datasets and various types of models with only a few queries. We hope our work motivates further investigation of LLM safety."
  },
  {
    "title": "ClimateLLM: Efficient Weather Forecasting via Frequency-Aware Large Language Models",
    "url": "http://arxiv.org/abs/2502.11059v1",
    "arxiv_id": "2502.11059v1",
    "authors": [
      "Shixuan Li",
      "Wei Yang",
      "Peiyu Zhang",
      "Xiongye Xiao",
      "Defu Cao",
      "Yuehan Qin",
      "Xiaole Zhang",
      "Yue Zhao",
      "Paul Bogdan"
    ],
    "published": "2025-02-16T09:57:50+00:00",
    "summary": "Weather forecasting is crucial for public safety, disaster prevention and mitigation, agricultural production, and energy management, with global relevance. Although deep learning has significantly advanced weather prediction, current methods face critical limitations: (i) they often struggle to capture both dynamic temporal dependencies and short-term abrupt changes, making extreme weather modeling difficult; (ii) they incur high computational costs due to extensive training and resource requirements; (iii) they have limited adaptability to multi-scale frequencies, leading to challenges when separating global trends from local fluctuations. To address these issues, we propose ClimateLLM, a foundation model for weather forecasting. It captures spatiotemporal dependencies via a cross-temporal and cross-spatial collaborative modeling framework that integrates Fourier-based frequency decomposition with Large Language Models (LLMs) to strengthen spatial and temporal modeling. Our framework uses a Mixture-of-Experts (MoE) mechanism that adaptively processes different frequency components, enabling efficient handling of both global signals and localized extreme events. In addition, we introduce a cross-temporal and cross-spatial dynamic prompting mechanism, allowing LLMs to incorporate meteorological patterns across multiple scales effectively. Extensive experiments on real-world datasets show that ClimateLLM outperforms state-of-the-art approaches in accuracy and efficiency, as a scalable solution for global weather forecasting."
  },
  {
    "title": "A Physics-Informed Machine Learning Framework for Safe and Optimal Control of Autonomous Systems",
    "url": "http://arxiv.org/abs/2502.11057v1",
    "arxiv_id": "2502.11057v1",
    "authors": [
      "Manan Tayal",
      "Aditya Singh",
      "Shishir Kolathaya",
      "Somil Bansal"
    ],
    "published": "2025-02-16T09:46:17+00:00",
    "summary": "As autonomous systems become more ubiquitous in daily life, ensuring high performance with guaranteed safety is crucial. However, safety and performance could be competing objectives, which makes their co-optimization difficult. Learning-based methods, such as Constrained Reinforcement Learning (CRL), achieve strong performance but lack formal safety guarantees due to safety being enforced as soft constraints, limiting their use in safety-critical settings. Conversely, formal methods such as Hamilton-Jacobi (HJ) Reachability Analysis and Control Barrier Functions (CBFs) provide rigorous safety assurances but often neglect performance, resulting in overly conservative controllers. To bridge this gap, we formulate the co-optimization of safety and performance as a state-constrained optimal control problem, where performance objectives are encoded via a cost function and safety requirements are imposed as state constraints. We demonstrate that the resultant value function satisfies a Hamilton-Jacobi-Bellman (HJB) equation, which we approximate efficiently using a novel physics-informed machine learning framework. In addition, we introduce a conformal prediction-based verification strategy to quantify the learning errors, recovering a high-confidence safety value function, along with a probabilistic error bound on performance degradation. Through several case studies, we demonstrate the efficacy of the proposed framework in enabling scalable learning of safe and performant controllers for complex, high-dimensional autonomous systems."
  },
  {
    "title": "Reasoning-Augmented Conversation for Multi-Turn Jailbreak Attacks on Large Language Models",
    "url": "http://arxiv.org/abs/2502.11054v2",
    "arxiv_id": "2502.11054v2",
    "authors": [
      "Zonghao Ying",
      "Deyue Zhang",
      "Zonglei Jing",
      "Yisong Xiao",
      "Quanchen Zou",
      "Aishan Liu",
      "Siyuan Liang",
      "Xiangzheng Zhang",
      "Xianglong Liu",
      "Dacheng Tao"
    ],
    "published": "2025-02-16T09:27:44+00:00",
    "summary": "Multi-turn jailbreak attacks simulate real-world human interactions by engaging large language models (LLMs) in iterative dialogues, exposing critical safety vulnerabilities. However, existing methods often struggle to balance semantic coherence with attack effectiveness, resulting in either benign semantic drift or ineffective detection evasion. To address this challenge, we propose Reasoning-Augmented Conversation, a novel multi-turn jailbreak framework that reformulates harmful queries into benign reasoning tasks and leverages LLMs' strong reasoning capabilities to compromise safety alignment. Specifically, we introduce an attack state machine framework to systematically model problem translation and iterative reasoning, ensuring coherent query generation across multiple turns. Building on this framework, we design gain-guided exploration, self-play, and rejection feedback modules to preserve attack semantics, enhance effectiveness, and sustain reasoning-driven attack progression. Extensive experiments on multiple LLMs demonstrate that RACE achieves state-of-the-art attack effectiveness in complex conversational scenarios, with attack success rates (ASRs) increasing by up to 96%. Notably, our approach achieves ASRs of 82% and 92% against leading commercial models, OpenAI o1 and DeepSeek R1, underscoring its potency. We release our code at https://github.com/NY1024/RACE to facilitate further research in this critical domain."
  },
  {
    "title": "Prompt Inject Detection with Generative Explanation as an Investigative Tool",
    "url": "http://arxiv.org/abs/2502.11006v1",
    "arxiv_id": "2502.11006v1",
    "authors": [
      "Jonathan Pan",
      "Swee Liang Wong",
      "Yidi Yuan",
      "Xin Wei Chia"
    ],
    "published": "2025-02-16T06:16:00+00:00",
    "summary": "Large Language Models (LLMs) are vulnerable to adversarial prompt based injects. These injects could jailbreak or exploit vulnerabilities within these models with explicit prompt requests leading to undesired responses. In the context of investigating prompt injects, the challenge is the sheer volume of input prompts involved that are likely to be largely benign. This investigative challenge is further complicated by the semantics and subjectivity of the input prompts involved in the LLM conversation with its user and the context of the environment to which the conversation is being carried out. Hence, the challenge for AI security investigators would be two-fold. The first is to identify adversarial prompt injects and then to assess whether the input prompt is contextually benign or adversarial. For the first step, this could be done using existing AI security solutions like guardrails to detect and protect the LLMs. Guardrails have been developed using a variety of approaches. A popular approach is to use signature based. Another popular approach to develop AI models to classify such prompts include the use of NLP based models like a language model. However, in the context of conducting an AI security investigation of prompt injects, these guardrails lack the ability to aid investigators in triaging or assessing the identified input prompts. In this applied research exploration, we explore the use of a text generation capabilities of LLM to detect prompt injects and generate explanation for its detections to aid AI security investigators in assessing and triaging of such prompt inject detections. The practical benefit of such a tool is to ease the task of conducting investigation into prompt injects."
  },
  {
    "title": "Fine-Tuning Hard-to-Simulate Objectives for Quadruped Locomotion: A Case Study on Total Power Saving",
    "url": "http://arxiv.org/abs/2502.10956v1",
    "arxiv_id": "2502.10956v1",
    "authors": [
      "Ruiqian Nai",
      "Jiacheng You",
      "Liu Cao",
      "Hanchen Cui",
      "Shiyuan Zhang",
      "Huazhe Xu",
      "Yang Gao"
    ],
    "published": "2025-02-16T02:22:50+00:00",
    "summary": "Legged locomotion is not just about mobility; it also encompasses crucial objectives such as energy efficiency, safety, and user experience, which are vital for real-world applications. However, key factors such as battery power consumption and stepping noise are often inaccurately modeled or missing in common simulators, leaving these aspects poorly optimized or unaddressed by current sim-to-real methods. Hand-designed proxies, such as mechanical power and foot contact forces, have been used to address these challenges but are often problem-specific and inaccurate.   In this paper, we propose a data-driven framework for fine-tuning locomotion policies, targeting these hard-to-simulate objectives. Our framework leverages real-world data to model these objectives and incorporates the learned model into simulation for policy improvement. We demonstrate the effectiveness of our framework on power saving for quadruped locomotion, achieving a significant 24-28\\% net reduction in total power consumption from the battery pack at various speeds. In essence, our approach offers a versatile solution for optimizing hard-to-simulate objectives in quadruped locomotion, providing an easy-to-adapt paradigm for continual improving with real-world knowledge. Project page https://hard-to-sim.github.io/."
  },
  {
    "title": "Fundamental Principles of Linguistic Structure are Not Represented by o3",
    "url": "http://arxiv.org/abs/2502.10934v1",
    "arxiv_id": "2502.10934v1",
    "authors": [
      "Elliot Murphy",
      "Evelina Leivada",
      "Vittoria Dentella",
      "Fritz Gunther",
      "Gary Marcus"
    ],
    "published": "2025-02-15T23:53:31+00:00",
    "summary": "A core component of a successful artificial general intelligence would be the rapid creation and manipulation of grounded compositional abstractions and the demonstration of expertise in the family of recursive hierarchical syntactic objects necessary for the creative use of human language. We evaluated the recently released o3 model (OpenAI; o3-mini-high) and discovered that while it succeeds on some basic linguistic tests relying on linear, surface statistics (e.g., the Strawberry Test), it fails to generalize basic phrase structure rules; it fails with comparative sentences involving semantically illegal cardinality comparisons ('Escher sentences'); its fails to correctly rate and explain acceptability dynamics; and it fails to distinguish between instructions to generate unacceptable semantic vs. unacceptable syntactic outputs. When tasked with generating simple violations of grammatical rules, it is seemingly incapable of representing multiple parses to evaluate against various possible semantic interpretations. In stark contrast to many recent claims that artificial language models are on the verge of replacing the field of linguistics, our results suggest not only that deep learning is hitting a wall with respect to compositionality (Marcus 2022), but that it is hitting [a [stubbornly [resilient wall]]] that cannot readily be surmounted to reach human-like compositional reasoning simply through more compute."
  }
]