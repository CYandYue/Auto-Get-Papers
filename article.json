[
  {
    "title": "FedEAT: A Robustness Optimization Framework for Federated LLMs",
    "url": "http://arxiv.org/abs/2502.11863v1",
    "authors": [
      "Yahao Pang",
      "Xingyuan Wu",
      "Xiaojin Zhang",
      "Wei Chen",
      "Hai Jin"
    ],
    "published": "2025-02-17T14:55:46+00:00",
    "summary": "Significant advancements have been made by Large Language Models (LLMs) in the domains of natural language understanding and automated content creatio..."
  },
  {
    "title": "Rethinking Audio-Visual Adversarial Vulnerability from Temporal and Modality Perspectives",
    "url": "http://arxiv.org/abs/2502.11858v1",
    "authors": [
      "Zeliang Zhang",
      "Susan Liang",
      "Daiki Shimada",
      "Chenliang Xu"
    ],
    "published": "2025-02-17T14:50:34+00:00",
    "summary": "While audio-visual learning equips models with a richer understanding of the real world by leveraging multiple sensory modalities, this integration al..."
  },
  {
    "title": "VLDBench: Vision Language Models Disinformation Detection Benchmark",
    "url": "http://arxiv.org/abs/2502.11361v1",
    "authors": [
      "Shaina Raza",
      "Ashmal Vayani",
      "Aditya Jain",
      "Aravind Narayanan",
      "Vahid Reza Khazaie",
      "Syed Raza Bashir",
      "Elham Dolatabadi",
      "Gias Uddin",
      "Christos Emmanouilidis",
      "Rizwan Qureshi",
      "Mubarak Shah"
    ],
    "published": "2025-02-17T02:18:47+00:00",
    "summary": "The rapid rise of AI-generated content has made detecting disinformation increasingly challenging. In particular, multimodal disinformation, i.e., onl..."
  },
  {
    "title": "G-Safeguard: A Topology-Guided Security Lens and Treatment on LLM-based Multi-agent Systems",
    "url": "http://arxiv.org/abs/2502.11127v1",
    "authors": [
      "Shilong Wang",
      "Guibin Zhang",
      "Miao Yu",
      "Guancheng Wan",
      "Fanci Meng",
      "Chongye Guo",
      "Kun Wang",
      "Yang Wang"
    ],
    "published": "2025-02-16T13:48:41+00:00",
    "summary": "Large Language Model (LLM)-based Multi-agent Systems (MAS) have demonstrated remarkable capabilities in various complex tasks, ranging from collaborat..."
  },
  {
    "title": "Fast Proxies for LLM Robustness Evaluation",
    "url": "http://arxiv.org/abs/2502.10487v1",
    "authors": [
      "Tim Beyer",
      "Jan Schuchardt",
      "Leo Schwinn",
      "Stephan G\u00fcnnemann"
    ],
    "published": "2025-02-14T11:15:27+00:00",
    "summary": "Evaluating the robustness of LLMs to adversarial attacks is crucial for safe deployment, yet current red-teaming methods are often prohibitively expen..."
  },
  {
    "title": "SyntheticPop: Attacking Speaker Verification Systems With Synthetic VoicePops",
    "url": "http://arxiv.org/abs/2502.09553v1",
    "authors": [
      "Eshaq Jamdar",
      "Amith Kamath Belman"
    ],
    "published": "2025-02-13T18:05:12+00:00",
    "summary": "Voice Authentication (VA), also known as Automatic Speaker Verification (ASV), is a widely adopted authentication method, particularly in automated sy..."
  },
  {
    "title": "Wasserstein distributional adversarial training for deep neural networks",
    "url": "http://arxiv.org/abs/2502.09352v1",
    "authors": [
      "Xingjian Bai",
      "Guangyi He",
      "Yifan Jiang",
      "Jan Obloj"
    ],
    "published": "2025-02-13T14:18:41+00:00",
    "summary": "Design of adversarial attacks for deep neural networks, as well as methods of adversarial training against them, are subject of intense research. In t..."
  },
  {
    "title": "AI Safety for Everyone",
    "url": "http://arxiv.org/abs/2502.09288v2",
    "authors": [
      "Balint Gyevnar",
      "Atoosa Kasirzadeh"
    ],
    "published": "2025-02-13T13:04:59+00:00",
    "summary": "Recent discussions and research in AI safety have increasingly emphasized the deep connection between AI safety and existential risk from advanced AI ..."
  },
  {
    "title": "LiSA: Leveraging Link Recommender to Attack Graph Neural Networks via Subgraph Injection",
    "url": "http://arxiv.org/abs/2502.09271v2",
    "authors": [
      "Wenlun Zhang",
      "Enyan Dai",
      "Kentaro Yoshioka"
    ],
    "published": "2025-02-13T12:33:39+00:00",
    "summary": "Graph Neural Networks (GNNs) have demonstrated remarkable proficiency in modeling data with graph structures, yet recent research reveals their suscep..."
  },
  {
    "title": "FLAME: Flexible LLM-Assisted Moderation Engine",
    "url": "http://arxiv.org/abs/2502.09175v1",
    "authors": [
      "Ivan Bakulin",
      "Ilia Kopanichuk",
      "Iaroslav Bespalov",
      "Nikita Radchenko",
      "Vladimir Shaposhnikov",
      "Dmitry Dylov",
      "Ivan Oseledets"
    ],
    "published": "2025-02-13T11:05:55+00:00",
    "summary": "The rapid advancement of Large Language Models (LLMs) has introduced significant challenges in moderating user-model interactions. While LLMs demonstr..."
  },
  {
    "title": "Pulling Back the Curtain: Unsupervised Adversarial Detection via Contrastive Auxiliary Networks",
    "url": "http://arxiv.org/abs/2502.09110v1",
    "authors": [
      "Eylon Mizrahi",
      "Raz Lapid",
      "Moshe Sipper"
    ],
    "published": "2025-02-13T09:40:26+00:00",
    "summary": "Deep learning models are widely employed in safety-critical applications yet remain susceptible to adversarial attacks -- imperceptible perturbations ..."
  },
  {
    "title": "ASVspoof 5: Design, Collection and Validation of Resources for Spoofing, Deepfake, and Adversarial Attack Detection Using Crowdsourced Speech",
    "url": "http://arxiv.org/abs/2502.08857v2",
    "authors": [
      "Xin Wang",
      "H\u00e9ctor Delgado",
      "Hemlata Tak",
      "Jee-weon Jung",
      "Hye-jin Shim",
      "Massimiliano Todisco",
      "Ivan Kukanov",
      "Xuechen Liu",
      "Md Sahidullah",
      "Tomi Kinnunen",
      "Nicholas Evans",
      "Kong Aik Lee",
      "Junichi Yamagishi",
      "Myeonghun Jeong",
      "Ge Zhu",
      "Yongyi Zang",
      "You Zhang",
      "Soumi Maiti",
      "Florian Lux",
      "Nicolas M\u00fcller",
      "Wangyou Zhang",
      "Chengzhe Sun",
      "Shuwei Hou",
      "Siwei Lyu",
      "S\u00e9bastien Le Maguer",
      "Cheng Gong",
      "Hanjie Guo",
      "Liping Chen",
      "Vishwanath Singh"
    ],
    "published": "2025-02-13T00:15:54+00:00",
    "summary": "ASVspoof 5 is the fifth edition in a series of challenges which promote the study of speech spoofing and deepfake attacks as well as the design of det..."
  },
  {
    "title": "AdvSwap: Covert Adversarial Perturbation with High Frequency Info-swapping for Autonomous Driving Perception",
    "url": "http://arxiv.org/abs/2502.08374v1",
    "authors": [
      "Yuanhao Huang",
      "Qinfan Zhang",
      "Jiandong Xing",
      "Mengyue Cheng",
      "Haiyang Yu",
      "Yilong Ren",
      "Xiao Xiong"
    ],
    "published": "2025-02-12T13:05:35+00:00",
    "summary": "Perception module of Autonomous vehicles (AVs) are increasingly susceptible to be attacked, which exploit vulnerabilities in neural networks through a..."
  },
  {
    "title": "MAA: Meticulous Adversarial Attack against Vision-Language Pre-trained Models",
    "url": "http://arxiv.org/abs/2502.08079v1",
    "authors": [
      "Peng-Fei Zhang",
      "Guangdong Bai",
      "Zi Huang"
    ],
    "published": "2025-02-12T02:53:27+00:00",
    "summary": "Current adversarial attacks for evaluating the robustness of vision-language pre-trained (VLP) models in multi-modal tasks suffer from limited transfe..."
  },
  {
    "title": "Quaternion-Hadamard Network: A Novel Defense Against Adversarial Attacks with a New Dataset",
    "url": "http://arxiv.org/abs/2502.10452v1",
    "authors": [
      "Vladimir Frants",
      "Sos Agaian"
    ],
    "published": "2025-02-12T00:13:40+00:00",
    "summary": "This paper addresses the vulnerability of deep-learning models designed for rain, snow, and haze removal. Despite enhancing image quality in adverse w..."
  },
  {
    "title": "Universal Adversarial Attack on Aligned Multimodal LLMs",
    "url": "http://arxiv.org/abs/2502.07987v2",
    "authors": [
      "Temurbek Rahmatullaev",
      "Polina Druzhinina",
      "Matvey Mikhalchuk",
      "Andrey Kuznetsov",
      "Anton Razzhigaev"
    ],
    "published": "2025-02-11T22:07:47+00:00",
    "summary": "We propose a universal adversarial attack on multimodal Large Language Models (LLMs) that leverages a single optimized image to override alignment saf..."
  },
  {
    "title": "Approximate Energetic Resilience of Nonlinear Systems under Partial Loss of Control Authority",
    "url": "http://arxiv.org/abs/2502.07603v1",
    "authors": [
      "Ram Padmanabhan",
      "Melkior Ornik"
    ],
    "published": "2025-02-11T14:52:26+00:00",
    "summary": "In this paper, we quantify the resilience of nonlinear dynamical systems by studying the increased energy used by all inputs of a system that suffers ..."
  },
  {
    "title": "SEMU: Singular Value Decomposition for Efficient Machine Unlearning",
    "url": "http://arxiv.org/abs/2502.07587v1",
    "authors": [
      "Marcin Sendera",
      "\u0141ukasz Struski",
      "Kamil Ksi\u0105\u017cek",
      "Kryspin Musiol",
      "Jacek Tabor",
      "Dawid Rymarczyk"
    ],
    "published": "2025-02-11T14:36:39+00:00",
    "summary": "While the capabilities of generative foundational models have advanced rapidly in recent years, methods to prevent harmful and unsafe behaviors remain..."
  },
  {
    "title": "RoMA: Robust Malware Attribution via Byte-level Adversarial Training with Global Perturbations and Adversarial Consistency Regularization",
    "url": "http://arxiv.org/abs/2502.07492v2",
    "authors": [
      "Yuxia Sun",
      "Huihong Chen",
      "Jingcai Guo",
      "Aoxiang Sun",
      "Zhetao Li",
      "Haolin Liu"
    ],
    "published": "2025-02-11T11:51:12+00:00",
    "summary": "Attributing APT (Advanced Persistent Threat) malware to their respective groups is crucial for threat intelligence and cybersecurity. However, APT adv..."
  },
  {
    "title": "LUNAR: LLM Unlearning via Neural Activation Redirection",
    "url": "http://arxiv.org/abs/2502.07218v1",
    "authors": [
      "William F. Shen",
      "Xinchi Qiu",
      "Meghdad Kurmanji",
      "Alex Iacob",
      "Lorenzo Sani",
      "Yihong Chen",
      "Nicola Cancedda",
      "Nicholas D. Lane"
    ],
    "published": "2025-02-11T03:23:22+00:00",
    "summary": "Large Language Models (LLMs) benefit from training on ever larger amounts of textual data, but as a result, they increasingly incur the risk of leakin..."
  },
  {
    "title": "Krum Federated Chain (KFC): Using blockchain to defend against adversarial attacks in Federated Learning",
    "url": "http://arxiv.org/abs/2502.06917v1",
    "authors": [
      "Mario Garc\u00eda-M\u00e1rquez",
      "Nuria Rodr\u00edguez-Barroso",
      "M. Victoria Luz\u00f3n",
      "Francisco Herrera"
    ],
    "published": "2025-02-10T15:15:50+00:00",
    "summary": "Federated Learning presents a nascent approach to machine learning, enabling collaborative model training across decentralized devices while safeguard..."
  },
  {
    "title": "Amnesia as a Catalyst for Enhancing Black Box Pixel Attacks in Image Classification and Object Detection",
    "url": "http://arxiv.org/abs/2502.07821v1",
    "authors": [
      "Dongsu Song",
      "Daehwa Ko",
      "Jay Hoon Jung"
    ],
    "published": "2025-02-10T11:49:41+00:00",
    "summary": "It is well known that query-based attacks tend to have relatively higher success rates in adversarial black-box attacks. While research on black-box a..."
  },
  {
    "title": "Jailbreaking to Jailbreak",
    "url": "http://arxiv.org/abs/2502.09638v1",
    "authors": [
      "Jeremy Kritz",
      "Vaughn Robinson",
      "Robert Vacareanu",
      "Bijan Varjavand",
      "Michael Choi",
      "Bobby Gogov",
      "Scale Red Team",
      "Summer Yue",
      "Willow E. Primack",
      "Zifan Wang"
    ],
    "published": "2025-02-09T20:49:16+00:00",
    "summary": "Refusal training on Large Language Models (LLMs) prevents harmful outputs, yet this defense remains vulnerable to both automated and human-crafted jai..."
  },
  {
    "title": "Optimization under Attack: Resilience, Vulnerability, and the Path to Collapse",
    "url": "http://arxiv.org/abs/2502.05954v1",
    "authors": [
      "Amal Aldawsari",
      "Evangelos Pournaras"
    ],
    "published": "2025-02-09T16:48:09+00:00",
    "summary": "Optimization is instrumental for improving operations of large-scale socio-technical infrastructures of Smart Cities, for instance, energy and traffic..."
  },
  {
    "title": "Protecting Intellectual Property of EEG-based Neural Networks with Watermarking",
    "url": "http://arxiv.org/abs/2502.05931v1",
    "authors": [
      "Ahmed Abdelaziz",
      "Ahmed Fathi",
      "Ahmed Fares"
    ],
    "published": "2025-02-09T15:21:45+00:00",
    "summary": "EEG-based neural networks, pivotal in medical diagnosis and brain-computer interfaces, face significant intellectual property (IP) risks due to their ..."
  },
  {
    "title": "Assessing confidence in frontier AI safety cases",
    "url": "http://arxiv.org/abs/2502.05791v1",
    "authors": [
      "Stephen Barrett",
      "Philip Fox",
      "Joshua Krook",
      "Tuneer Mondal",
      "Simon Mylius",
      "Alejandro Tlaie"
    ],
    "published": "2025-02-09T06:35:11+00:00",
    "summary": "Powerful new frontier AI technologies are bringing many benefits to society but at the same time bring new risks. AI developers and regulators are the..."
  },
  {
    "title": "Effective Black-Box Multi-Faceted Attacks Breach Vision Large Language Model Guardrails",
    "url": "http://arxiv.org/abs/2502.05772v1",
    "authors": [
      "Yijun Yang",
      "Lichao Wang",
      "Xiao Yang",
      "Lanqing Hong",
      "Jun Zhu"
    ],
    "published": "2025-02-09T04:21:27+00:00",
    "summary": "Vision Large Language Models (VLLMs) integrate visual data processing, expanding their real-world applications, but also increasing the risk of genera..."
  },
  {
    "title": "Rigid Body Adversarial Attacks",
    "url": "http://arxiv.org/abs/2502.05669v1",
    "authors": [
      "Aravind Ramakrishnan",
      "David I. W. Levin",
      "Alec Jacobson"
    ],
    "published": "2025-02-08T19:12:27+00:00",
    "summary": "Due to their performance and simplicity, rigid body simulators are often used in applications where the objects of interest can considered very stiff...."
  },
  {
    "title": "Democratic Training Against Universal Adversarial Perturbations",
    "url": "http://arxiv.org/abs/2502.05542v1",
    "authors": [
      "Bing Sun",
      "Jun Sun",
      "Wei Zhao"
    ],
    "published": "2025-02-08T12:15:32+00:00",
    "summary": "Despite their advances and success, real-world deep neural networks are known to be vulnerable to adversarial attacks. Universal adversarial perturbat..."
  },
  {
    "title": "Do Spikes Protect Privacy? Investigating Black-Box Model Inversion Attacks in Spiking Neural Networks",
    "url": "http://arxiv.org/abs/2502.05509v1",
    "authors": [
      "Hamed Poursiami",
      "Ayana Moshruba",
      "Maryam Parsa"
    ],
    "published": "2025-02-08T10:02:27+00:00",
    "summary": "As machine learning models become integral to security-sensitive applications, concerns over data leakage from adversarial attacks continue to rise. M..."
  },
  {
    "title": "Towards Trustworthy Retrieval Augmented Generation for Large Language Models: A Survey",
    "url": "http://arxiv.org/abs/2502.06872v1",
    "authors": [
      "Bo Ni",
      "Zheyuan Liu",
      "Leyao Wang",
      "Yongjia Lei",
      "Yuying Zhao",
      "Xueqi Cheng",
      "Qingkai Zeng",
      "Luna Dong",
      "Yinglong Xia",
      "Krishnaram Kenthapadi",
      "Ryan Rossi",
      "Franck Dernoncourt",
      "Md Mehrab Tanjim",
      "Nesreen Ahmed",
      "Xiaorui Liu",
      "Wenqi Fan",
      "Erik Blasch",
      "Yu Wang",
      "Meng Jiang",
      "Tyler Derr"
    ],
    "published": "2025-02-08T06:50:47+00:00",
    "summary": "Retrieval-Augmented Generation (RAG) is an advanced technique designed to address the challenges of Artificial Intelligence-Generated Content (AIGC). ..."
  },
  {
    "title": "Forbidden Science: Dual-Use AI Challenge Benchmark and Scientific Refusal Tests",
    "url": "http://arxiv.org/abs/2502.06867v1",
    "authors": [
      "David Noever",
      "Forrest McKee"
    ],
    "published": "2025-02-08T04:27:33+00:00",
    "summary": "The development of robust safety benchmarks for large language models requires open, reproducible datasets that can measure both appropriate refusal o..."
  },
  {
    "title": "The Odyssey of the Fittest: Can Agents Survive and Still Be Good?",
    "url": "http://arxiv.org/abs/2502.05442v1",
    "authors": [
      "Dylan Waldner",
      "Risto Miikkulainen"
    ],
    "published": "2025-02-08T04:17:28+00:00",
    "summary": "As AI models grow in power and generality, understanding how agents learn and make decisions in complex environments is critical to promoting ethical ..."
  },
  {
    "title": "Towards LLM Unlearning Resilient to Relearning Attacks: A Sharpness-Aware Minimization Perspective and Beyond",
    "url": "http://arxiv.org/abs/2502.05374v1",
    "authors": [
      "Chongyu Fan",
      "Jinghan Jia",
      "Yihua Zhang",
      "Anil Ramakrishna",
      "Mingyi Hong",
      "Sijia Liu"
    ],
    "published": "2025-02-07T23:03:55+00:00",
    "summary": "The LLM unlearning technique has recently been introduced to comply with data regulations and address the safety and ethical concerns of LLMs by remov..."
  },
  {
    "title": "Federated Learning for Anomaly Detection in Energy Consumption Data: Assessing the Vulnerability to Adversarial Attacks",
    "url": "http://arxiv.org/abs/2502.05041v1",
    "authors": [
      "Yohannis Kifle Telila",
      "Damitha Senevirathne",
      "Dumindu Tissera",
      "Apurva Narayan",
      "Miriam A. M. Capretz",
      "Katarina Grolinger"
    ],
    "published": "2025-02-07T16:08:20+00:00",
    "summary": "Anomaly detection is crucial in the energy sector to identify irregular patterns indicating equipment failures, energy theft, or other issues. Machine..."
  },
  {
    "title": "DMPA: Model Poisoning Attacks on Decentralized Federated Learning for Model Differences",
    "url": "http://arxiv.org/abs/2502.04771v1",
    "authors": [
      "Chao Feng",
      "Yunlong Li",
      "Yuanzhe Gao",
      "Alberto Huertas Celdr\u00e1n",
      "Jan von der Assen",
      "G\u00e9r\u00f4me Bovet",
      "Burkhard Stiller"
    ],
    "published": "2025-02-07T09:15:38+00:00",
    "summary": "Federated learning (FL) has garnered significant attention as a prominent privacy-preserving Machine Learning (ML) paradigm. Decentralized FL (DFL) es..."
  },
  {
    "title": "Mechanistic Understandings of Representation Vulnerabilities and Engineering Robust Vision Transformers",
    "url": "http://arxiv.org/abs/2502.04679v1",
    "authors": [
      "Chashi Mahiul Islam",
      "Samuel Jacob Chacko",
      "Mao Nishino",
      "Xiuwen Liu"
    ],
    "published": "2025-02-07T05:58:16+00:00",
    "summary": "While transformer-based models dominate NLP and vision applications, their underlying mechanisms to map the input space to the label space semanticall..."
  },
  {
    "title": "Confidence Elicitation: A New Attack Vector for Large Language Models",
    "url": "http://arxiv.org/abs/2502.04643v2",
    "authors": [
      "Brian Formento",
      "Chuan Sheng Foo",
      "See-Kiong Ng"
    ],
    "published": "2025-02-07T04:07:36+00:00",
    "summary": "A fundamental issue in deep learning has been adversarial robustness. As these systems have scaled, such issues have persisted. Currently, large langu..."
  },
  {
    "title": "BitAbuse: A Dataset of Visually Perturbed Texts for Defending Phishing Attacks",
    "url": "http://arxiv.org/abs/2502.05225v1",
    "authors": [
      "Hanyong Lee",
      "Chaelyn Lee",
      "Yongjae Lee",
      "Jaesung Lee"
    ],
    "published": "2025-02-06T05:04:04+00:00",
    "summary": "Phishing often targets victims through visually perturbed texts to bypass security systems. The noise contained in these texts functions as an adversa..."
  },
  {
    "title": "How vulnerable is my policy? Adversarial attacks on modern behavior cloning policies",
    "url": "http://arxiv.org/abs/2502.03698v1",
    "authors": [
      "Basavasagar Patil",
      "Akansha Kalra",
      "Guanhong Tao",
      "Daniel S. Brown"
    ],
    "published": "2025-02-06T01:17:39+00:00",
    "summary": "Learning from Demonstration (LfD) algorithms have shown promising results in robotic manipulation tasks, but their vulnerability to adversarial attack..."
  },
  {
    "title": "Optimizing Robustness and Accuracy in Mixture of Experts: A Dual-Model Approach",
    "url": "http://arxiv.org/abs/2502.06832v2",
    "authors": [
      "Xu Zhang",
      "Kaidi Xu",
      "Ziqing Hu",
      "Ren Wang"
    ],
    "published": "2025-02-05T20:45:52+00:00",
    "summary": "Mixture of Experts (MoE) have shown remarkable success in leveraging specialized expert networks for complex machine learning tasks. However, their su..."
  },
  {
    "title": "Enabling External Scrutiny of AI Systems with Privacy-Enhancing Technologies",
    "url": "http://arxiv.org/abs/2502.05219v1",
    "authors": [
      "Kendrea Beers",
      "Helen Toner"
    ],
    "published": "2025-02-05T15:31:11+00:00",
    "summary": "This article describes how technical infrastructure developed by the nonprofit OpenMined enables external scrutiny of AI systems without compromising ..."
  },
  {
    "title": "Large Language Model Adversarial Landscape Through the Lens of Attack Objectives",
    "url": "http://arxiv.org/abs/2502.02960v1",
    "authors": [
      "Nan Wang",
      "Kane Walter",
      "Yansong Gao",
      "Alsharif Abuadbba"
    ],
    "published": "2025-02-05T07:54:07+00:00",
    "summary": "Large Language Models (LLMs) represent a transformative leap in artificial intelligence, enabling the comprehension, generation, and nuanced interacti..."
  },
  {
    "title": "Real-Time Privacy Risk Measurement with Privacy Tokens for Gradient Leakage",
    "url": "http://arxiv.org/abs/2502.02913v4",
    "authors": [
      "Jiayang Meng",
      "Tao Huang",
      "Hong Chen",
      "Xin Shi",
      "Qingyu Huang",
      "Chen Hou"
    ],
    "published": "2025-02-05T06:20:20+00:00",
    "summary": "The widespread deployment of deep learning models in privacy-sensitive domains has amplified concerns regarding privacy risks, particularly those stem..."
  },
  {
    "title": "Wolfpack Adversarial Attack for Robust Multi-Agent Reinforcement Learning",
    "url": "http://arxiv.org/abs/2502.02844v2",
    "authors": [
      "Sunwoo Lee",
      "Jaebak Hwang",
      "Yonghyeon Jo",
      "Seungyul Han"
    ],
    "published": "2025-02-05T02:59:23+00:00",
    "summary": "Traditional robust methods in multi-agent reinforcement learning (MARL) often struggle against coordinated adversarial attacks in cooperative scenario..."
  },
  {
    "title": "MARAGE: Transferable Multi-Model Adversarial Attack for Retrieval-Augmented Generation Data Extraction",
    "url": "http://arxiv.org/abs/2502.04360v1",
    "authors": [
      "Xiao Hu",
      "Eric Liu",
      "Weizhou Wang",
      "Xiangyu Guo",
      "David Lie"
    ],
    "published": "2025-02-05T00:17:01+00:00",
    "summary": "Retrieval-Augmented Generation (RAG) offers a solution to mitigate hallucinations in Large Language Models (LLMs) by grounding their outputs to knowle..."
  },
  {
    "title": "Uncertainty Quantification for Collaborative Object Detection Under Adversarial Attacks",
    "url": "http://arxiv.org/abs/2502.02537v1",
    "authors": [
      "Huiqun Huang",
      "Cong Chen",
      "Jean-Philippe Monteuuis",
      "Jonathan Petit",
      "Fei Miao"
    ],
    "published": "2025-02-04T18:03:32+00:00",
    "summary": "Collaborative Object Detection (COD) and collaborative perception can integrate data or features from various entities, and improve object detection a..."
  },
  {
    "title": "CoRPA: Adversarial Image Generation for Chest X-rays Using Concept Vector Perturbations and Generative Models",
    "url": "http://arxiv.org/abs/2502.05214v1",
    "authors": [
      "Amy Rafferty",
      "Rishi Ramaesh",
      "Ajitha Rajan"
    ],
    "published": "2025-02-04T17:14:31+00:00",
    "summary": "Deep learning models for medical image classification tasks are becoming widely implemented in AI-assisted diagnostic tools, aiming to enhance diagnos..."
  },
  {
    "title": "FRAUD-RLA: A new reinforcement learning adversarial attack against credit card fraud detection",
    "url": "http://arxiv.org/abs/2502.02290v1",
    "authors": [
      "Daniele Lunghi",
      "Yannick Molinghen",
      "Alkis Simitsis",
      "Tom Lenaerts",
      "Gianluca Bontempi"
    ],
    "published": "2025-02-04T12:59:35+00:00",
    "summary": "Adversarial attacks pose a significant threat to data-driven systems, and researchers have spent considerable resources studying them. Despite its eco..."
  },
  {
    "title": "Dual-Flow: Transferable Multi-Target, Instance-Agnostic Attacks via In-the-wild Cascading Flow Optimization",
    "url": "http://arxiv.org/abs/2502.02096v2",
    "authors": [
      "Yixiao Chen",
      "Shikun Sun",
      "Jianshu Li",
      "Ruoyu Li",
      "Zhe Li",
      "Junliang Xing"
    ],
    "published": "2025-02-04T08:25:58+00:00",
    "summary": "Adversarial attacks are widely used to evaluate model robustness, and in black-box scenarios, the transferability of these attacks becomes crucial. Ex..."
  }
]